In diesem Kapitel wird mit Hilfe von “Produkten” von Vektoren deren
Länge bzw der eingeschlossene Winkel gemessen: Geometrisch führt dies
zur Euklidischen Geometrie, algebraisch zu dem für viele Anwendungen
wichtigen Spektralsatz. Ein zweiter zentraler Satz in diesem Kapitel ist
der Satz von Sylvester über die Struktur von geeigneten “Produkten”.
Die Euklidische bzw affine Klassifikation der Quadriken spiegeln den ge-
ometrischen Gehalt dieser beiden Sätze wider.

TARGET DECK: Lineare Algebra 2
### 3.1 Billinearformen & Struktursätze

Betrachtet man Punkte $X, Y \in \mathbb{E}^2$ einer **Euklidischen Ebene** als durch kartesische Koordinatenvektoren bzw. **Polarkoordinaten** gegeben, $$ \overrightarrow{OX} = \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} r \cos \varphi \\ r \sin \varphi \end{pmatrix}, \quad \overrightarrow{OY} = \begin{pmatrix} u \\ v \end{pmatrix} = \begin{pmatrix} s \cos \psi \\ s \sin \psi \end{pmatrix} \in \mathbb{R}^2 \cong \mathbb{R}^{2 \times 1}, $$ so erhält man den **Abstand zum Ursprung** (Länge des Koordinatenvektors) als $$ x = \sqrt{x_1^2 + x_2^2} \quad \text{bzw.} \quad y = \sqrt{y_1^2 + y_2^2} $$ und den **Winkel zwischen** den beiden **Koordinatenvektoren** am Ursprung via $$ \cos(\varphi - \psi) = \cos \varphi \cos \psi + \sin \varphi \sin \psi = \frac{x_1 y_1 + x_2 y_2}{xy}. $$ Beide Formeln beinhalten das **Produkt eines Vektors mit sich selbst** bzw. zweier Vektoren: Später werden wir sehen, wie **Länge und Winkel** mit Hilfe von **Produkten definiert** werden können, nachdem wir geeignete **Produkte** und ihre **Eigenschaften** im Allgemeinen untersucht haben.

 **Produkte in der Algebra**
Im **Kontext der Algebren** (siehe **Abschnitt 2.4**, Def Algebra/Bem Fortsetzungssatz) wurde bereits eine **Multiplikation oder allgemeiner, bilineare Abbildung** $$ \cdot : V \times V \to V, \quad \beta : V \times V \to W $$ betrachtet; hier benötigen wir jedoch **skalare Werte „Produkte“**, die **gegebenenfalls** aber **keine „Multiplikation“ in diesem Sinne** (also **nicht** bilinear) sind.

---

#### Def 3.1(5):

Definition von Sesquillinearform #flashcard 

Seien $V$ ein $K$-Vektorraum und $K \ni x \mapsto \bar{x} \in K$ ein **(Körper-) Automorphismus**, d.h., eine **bijektive Abbildung** mit $$ x + y = \bar{x} + \bar{y}, \quad xy = \bar{x} \bar{y} \quad \text{für } x, y \in K. $$**Sesquilinearform** Eine Abbildung $\sigma : V \times V \to K$ heißt **Sesquilinearform** (bzgl. $\bar{\cdot}$), falls 
<!--ID: 1738257597491-->


1.  **(i)** $\forall v \in V: V \to K, \quad w \mapsto \sigma(v, w) \in K$ **linear** ist, d.h.,  $$\sigma(v, \cdot) \in V^*.$$
2. **(ii)** $\forall w \in V: V \to K, \quad v \mapsto \sigma(v, w) \in K$ **semilinear** ist, d.h.,$$\forall v, v' \in V, \alpha \in K: \quad \sigma(\alpha v + v', w) = \bar{\alpha} \sigma(v, w) + \sigma(v', w).$$
<!--ID: 1738156913313-->

---

#### Bsp&Def 3.1(8):

Definition von **Billinearform** #flashcard 

Die **Identität** $x \mapsto x$ ist ein **Körperautomorphismus** für jeden Körper $K$. Sesquilinearformen $\sigma: V \times V \to K$ bzgl. $\bar{\cdot} = \text{id}_K$ nennt man **Bilinearformen**.
<!--ID: 1738158863132-->

---

#### Bem 3.1 (9):

1. **Bilineare Sesquilinearform:** Ist $\bar{\cdot} = \text{id}_K$, so ist $\sigma$ **trivial**: $$\forall \lambda \in K, v \in V: \quad \sigma(\lambda v, w) = (\lambda v, w).$$ 
#### Bem 3.1 (10):

**Komplexe Konjugation als Körperautomorphismus:** Falls $K = \mathbb{C}$, liefert die **komplexe Konjugation** einen **Körperautomorphismus** ($\mathbb{C}$-Vektorautomorphismus, siehe Sect 1.3): $$\bar{z} = \overline{z} \quad \text{für alle } z \in \mathbb{C}.$$ Dies ist ein wichtiger Grund für die Einführung des Begriffs.

---

#### Bem 3.1(11):

In $\mathbb{Z}_p$ mit $p \in \mathbb{P}$ gibt es **nur** einen Körperautomorphismus: $$x \mapsto x^p.$$ In $\mathbb{R}$ gibt es **nur** die komplexe Konjugation $\bar{z} = \overline{z}$.

---

#### Bem&Def 3.1(14):

Definition von der **kanonischen Sesquilinearform** #flashcard 

**Kanonische Sesquilinearform in $\mathbb{K}^{n \times n}$:** Sei $\cdot : K \to K$ ein **Körperautomorphismus**. Jedes $S \in K^{n \times n}$ liefert dann die zu $S$ **assoziierte Sesquilinearform** $$\sigma_S : K^{n \times n} \times K^{n \times n} \to K, \quad (X, Y) \mapsto \sigma_S(X, Y) := X^t S Y.$$ Für $S = \mathbb{1}$ nennt man dies auch die **kanonische Sesquilinearform** auf $K^{n \times n}$.
<!--ID: 1738159064578-->


---

#### Satz 3.1(15):

Der Fortsetzungssatz für die Kanonische Sesquilinearform #flashcard 

Sind $V$ ein $K$-Vektorraum und $K \ni x \mapsto \bar{x} \in K$ ein **Körperautomorphismus** (z.B. **konjugiert**), dann existiert für jede **Basis von $V$** eine Familie in $K$, sodass eine **Sesquilinearform $\sigma$** existiert mit $$ \forall i, j \in I : \quad \sigma(b_i, b_j) = s_{ij}. $$
<!--ID: 1738159064594-->

---

#### Bew 3.1(16) von ([[#Satz 3.1(12)]]):

Beweis von Fortsetzungssatz für Kanonische Sesquillinearform #flashcard 

Wir imitieren den Beweis unseres ersten **Fortsetzungssatzes**. --- ## **3.1 Bilinearformen & Sesquilinearformen** ### **Eindeutigkeit** Sei $\sigma$ eine **Sesquilinearform** mit der gewünschten Eigenschaft; gilt $$ v = \sum_{i \in I} b_i x_i \quad \text{und} \quad w = \sum_{i \in I} b_j y_j, $$ so folgt $$ \sigma(v, w) = \sum_{i, j \in I} \bar{b}_i s_{ij} b_j. $$ **Existenz** Da jeder Vektor $v \in V$ **eindeutig** durch eine **Linearkombination** in der Basis $(b_i)_{i \in I}$ dargestellt wird, wird durch $$ \sigma : V \times V \to K, \quad (v, w) \mapsto \sigma(v, w) := \sum_{i, j \in I} \bar{b}_i s_{ij} b_j $$ eine **wohldefinierte** Abbildung erzeugt. **Offenbar ist $\sigma$ sesquilinear.**
<!--ID: 1738159208176-->

---

#### Bem 3.1(19):

Jede **Sesquilinearform** $\sigma : V \times V \to K$ liefert eine **semilineare Abbildung** $$ V \ni v \mapsto \sigma(v, \cdot) \in V^*. $$ Mit einem **"Fortsetzungssatz für semilineare Abbildungen"** hätte man auch den oben (Sect 2.4) für bilineare Abbildungen skizzierten Beweis imitieren können.

---

#### Buchhaltung 3.1(20):

Ist $\dim V < \infty$ und $B = (b_1, \dots, b_n)$ eine **Basis** von $V$, so kann man eine **Sesquilinearform** $\sigma$ **auch komplett durch die Matrix** $S$ ihrer Werte $s_{ij} = \sigma(b_i, b_j)$ **beschreiben** (vgl. Sect 1.4): $$ \begin{array}{c|cccc} \sigma & b_1 & b_2 & \dots & b_n \\ \hline b_1 & s_{11} & s_{12} & \dots & s_{1n} \\ b_2 & s_{21} & s_{22} & \dots & s_{2n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ b_n & s_{n1} & s_{n2} & \dots & s_{nn} \end{array} $$ Für **Vektoren** $$ v = \sum_{i=1}^{n} b_i x_i = BX, \quad w = \sum_{j=1}^{n} b_j y_j = BY $$ ist dann $$ \sigma(v, w) = \sum_{i,j=1}^{n} \bar{x}_i s_{ij} y_j = X^t S Y. $$

---

#### Def 3.1(22):

Definition der Darstellungs- oder Gramsche Matrix #flashcard 

Die **Darstellungs- oder Gramsche Matrix** einer **Sesquilinearform** $\sigma : V \times V \to K$ bezüglich einer Basis $B = (b_1, \dots, b_n)$ von $V$ ist die Matrix $$ \Gamma_B(\sigma) := \big( \sigma(b_i, b_j) \big)_{i,j=1,\dots,n}. $$
<!--ID: 1738159812751-->

---
#### Transformationsformel für Gramsche matrizen 3.1(23): 

Sei $V$ ein $n$-dimensionaler Vektorraum mit Basen $B$ und $B' = BP$ mit $P \in \text{Gl}(n)$, dann gilt für die zugehörigen Gramschen Matrizen einer Sesquilinearform $\sigma$: $$ \Gamma_{B'}(\sigma) = P^t \Gamma_B(\sigma) P. $$ #flashcard 

#### Bew 3.1(24):
Ein Basiswechsel $B' = BP$ mit $P = A_B^{B'}(\text{id}_V) \in \text{Gl}(n)$ liefert mit $$ b_i' = \sum_{k=1}^{n} b_k p_{ki} \quad \text{für die Matrix } P = (p_{ij}) $$ die Transformation der Gramschen Matrix: $$ s'_{ij} = \sigma(b_i', b_j') = \sum_{k,l=1}^{n} p_{ki} s_{kl} p_{lj}, $$ also die Behauptung.
<!--ID: 1738160264877-->

---

#### Bem 3.1(25):

Diese **Transformationsformel für Gramsche Matrizen** liefert einen weiteren **Äquivalenzbegriff für quadratische Matrizen** $S, S' \in K^{n \times n}$: $$ S' \sim S, \quad \text{falls} \quad \exists P \in \text{Gl}(n) : S' = P^t S P. $$ Die diversen Begriffe der Äquivalenz von Matrizen (vgl. **Sect 1.4 & 2.3**) haben dieselben Funktionen/Bedeutungen von **Matrizen in der Strukturtheorie**. 

Zusammenhang zwischen Sesquilinearformen und Matrizen

---

#### Bem 3.1(26):

Die Menge der **Sesquilinearformen** auf einem **$K$-Vektorraum** $V$ ist isomorph zu den **$(n \times n)$-Matrizen über $K$**. Ist $m = \dim V < \infty$ und $B$ eine **Basis** von $V$, so erhält man (**Fortsetzungssatz**) einen **Isomorphismus** $$ K^{n \times n} \cong \{ \sigma : V \times V \to K \text{ sesquilinear} \} \quad \sigma \mapsto \Gamma_B(\sigma) \in K^{n \times n}. $$

---

#### Def 3.1(27):

Definition wann ist eine Sesquilinearform auf bzg. eines Automorphismuses 
- Symetrisch
- Schiefsymetrisch #flashcard 

Eine **Sesquilinearform** $\sigma : V \times V \to K$ auf einem $K$-Vektorraum, bzgl. eines **Automorphismus** $\bar{\cdot}$ auf $K$, nennen wir 
<!--ID: 1738257597499-->


1. **symmetrisch**, falls $\forall v, w \in V$ gilt: $$\sigma(v, w) = \sigma(w, v).$$
2. **schiefsymmetrisch**, falls $\forall v, w \in V$ gilt: $$\sigma(v, w) = -\sigma(w, v).$$
Falls $K = \mathbb{C}$ und $\bar{\cdot}$ die **komplexe Konjugation** ist, nennt man eine **symmetrische Sesquilinearform** auch eine **Hermitesche Sesquilinearform**.
<!--ID: 1738160649327-->

---

#### Bem 3.1(31):

1. Ist $\bar{\cdot}$ nicht-trivial und **(schief-)symmetrisch**, so ist $\bar{\cdot}$ eine **Involution**, d.h., es gilt: $$\bar{\bar{x}} = \text{id}_K.$$ Nämlich: Man wähle $w, v \in V$ mit $\sigma(w, w) = 1$ und berechne $$ \forall x \in K: \quad \bar{x} = \overline{\sigma(wx, w)} = \pm \sigma(w, wx) = \sigma(w, w x) = x. $$ 2. Ist $\text{Char}(K) \neq 2$ und $\bar{\cdot}$ eine Involution, so kann jede **Sesquilinearform** in einen **symmetrischen** und einen **schiefsymmetrischen** Anteil zerlegt werden: $$ \sigma = \sigma_+ + \sigma_-, \quad \text{mit} \quad \sigma_{\pm}(v, w) = \frac{\sigma(v, w) \pm \sigma(w, v)}{2}. $$ 3. Ist $\text{Char}(K) = 2$, so ist eine **Bilinearform** $\sigma$ genau dann **schiefsymmetrisch**, wenn sie **alternierend** ist, d.h., wenn $$\forall v \in V: \quad \sigma(v, v) = 0.$$ Andernfalls ist jede **alternierende Sesquilinearform bilinear**.


#### Buchhaltung 3.1(33):

Ist $V$ ein $K$-Vektorraum mit $\dim V < \infty$, $\text{Char}(K) \neq 2$ und $\bar{\cdot}$ eine Involution, so ist eine **Sesquilinearform** $\sigma$ auf $V$ genau dann **schiefsymmetrisch**, wenn jede **Gramsche Matrix** $S = \Gamma_B(\sigma)$ dies ist: 

1. Falls **symmetrisch**, dann $$\sigma \sim 0 = 2 \Gamma_B(\sigma) = S = S^t \quad \Rightarrow \quad S = S^t.$$ 
2. Falls **schiefsymmetrisch**, dann $$\sigma \sim 0 = 2 \Gamma_B(\sigma) = S = S^t \quad \Rightarrow \quad S = -S.$$
