# Lineare Algebra und Geometrie

## 2.5 Minimalpolynom & Hauptraumzerlegung

### Mission 
Mit Hilfe des Minimalpolynoms lassen sich nun die ersten Struktursätze für Endomorphismen formulieren und beweisen: Das Minimalpolynom eines Endomorphismus gibt tiefere Einsicht darüber, wie ein Endomorphismus auf dem zugrundeliegenden Vektorraum operiert.

TARGET DECK: Lineare Algebra 2
### Zerlegungslemma 

Das Zerlegungslemma #flashcard 

Seien$f \in \text{End}(V)$und$p_1(t), p_2(t) \in K[t]$normiert und teilerfremd. Ist$p(t) = p_1(t) p_2(t)$ein Annullatorpolynom von$f$, so zerfällt$V$in$f$-invariante Unterräume:
$$
V = U_1 \oplus U_2, \quad \text{mit} \quad U_i := \ker p_i(f), \quad i = 1,2.
$$
Ist$p(t)$das Minimalpolynom von$f$, so sind$p_i(t)$die Minimalpolynome der Einschränkungen$g_i := f|_{U_i} \in \text{End}(U_i)$.
<!--ID: 1738334308879-->

---
### Beweis

Für $q(t) \in K[t]$ ist $\ker q(f) \subseteq V$ stets ein $f$-invarianter Unterraum, denn $$ \forall v \in \ker q(f) : q(f)(f(v)) = f(q(f)(v)) = 0. $$ Da $p(t)$ **Annullatorpolynom** von $f$ ist, $p(f) = 0$, erhalten wir $$ \{0\} = p_1(f)(p_2(f)(V)) \quad \Rightarrow \quad \begin{cases} p_1(f)(V) \subseteq \ker p_1(f), \\ p_2(f)(V) \subseteq \ker p_2(f). \end{cases} $$ Da $p_1(t)$ und $p_2(t)$ **teilerfremd** sind, gilt (Lemma von Bézout, Sect 2.4) $$ \exists q_1(t), q_2(t) \in K[t] : 1 = p_1(t) q_1(t) + p_2(t) q_2(t); $$ und damit $$ V \subseteq p_1(f)(V) + p_2(f)(V) \subseteq U_2 + U_1; $$ andererseits gilt für $v \in U_1 \cap U_2 = \ker p_1(f) \cap \ker p_2(f)$ $$ v = q_1(f)(p_1(f)(v)) + q_2(f)(p_2(f)(v)) = 0, $$ also ist die **Summe direkt**, und $$V = U_1 \oplus U_2.$$ wie behauptet. --- Nun sei $p(t) = \mu_f(t)$ **Minimalpolynom** von $f$, wir zeigen **$\mu_{g_1}(t) = p_1(t)$**. Nach Definition ist $$ p_1(f) = \mu_{g_1}(t) = 0 $$ also $p_1(t)$ **Annullatorpolynom** von $g_1 = f|_{U_1}$; insbesondere hat $g_1$ daher ein **Minimalpolynom** $\mu_{g_1}(t)$, und **$\mu_{g_1}(t) \mid p_1(t)$**. Andererseits liefert $$ p_2(f)(V) \subseteq U_1 \quad \Rightarrow \quad \mu_{g_1}(t) p_2(f) = 0, $$ dass **$\mu_{g_1}(t)$ Annullatorpolynom** von $f$ ist, mithin $$ p_1(t) p_2(t) = \mu_f(t) \quad \Rightarrow \quad \mu_{g_1}(t) (t) p_2(t) \quad \Rightarrow \quad p_1(t) \mid \mu_{g_1}(t). $$ Also **teilen sich $p_1(t)$ und $\mu_{g_1}(t)$ gegenseitig**, sind daher **gleich**.
<!--ID: 1738334425730-->

---

### Def Hauptraumzerlegung

Definiton der Hauptraumzerlegung #flashcard 

Ist das Minimalpolynom eines Endomorphismus$f \in \text{End}(V)$Produkt von Linearfaktorpotenzen:
$$
\mu_f (t) = (t - x_1)^{r_1} \cdots (t - x_m)^{r_m}, \quad \text{mit } x_i \neq x_j \text{ für } i \neq j,
$$
<!--ID: 1738335372205-->


so ist $V$ die direkte Summe der Haupträume zu den Eigenwerten $x_i$ von$f$:

$$
V = \bigoplus_{i=1}^{m} H_i, \quad \text{mit} \quad H_i := \ker ( (id_V x_{i} - f x_i)^{r_i}.
$$

#### Bemerkung 
Allgemeiner kann man den Hauptraum $H \leq V$ zu einem Eigenwert$x \in K$eines Endomorphismus$f \in \text{End}(V)$ definieren als:

$$
H := \bigcup_{r \in \mathbb{N}} \ker ( (id_V x - f)^r ) \leq V.
$$

---

### Bem & Def

Definition der Direkten Summe #flashcard 

Hier ist $V = \bigoplus_{i \in I} U_i$ die **direkte Summe** einer Familie $(U_i)_{i \in I}$ von Unterräumen $U_i \leq V$: **Die Summe der Familie ist direkt**, wenn $$ \sum_{i \in I} U_i = U_i, \quad \text{wenn} \quad \forall v \in \bigoplus_{i \in I} U_i : \sum_{i} v_i = 0. $$ Eine Summe von Unterräumen ist genau dann **direkt**, wenn die Zerlegung jedes Vektors der Summe in Vektoren der Unterräume **eindeutig** ist, $$ \forall u \in \sum_{i \in I} U_i, \quad U_i \cap \sum_{j \neq i} U_j = \{0\}. $$ Für **endlich viele Unterräume** $U_1, \dots, U_m$ ist die Summe **direkt**, wenn sie **iterativ direkt** ist, das heißt, wenn $$ V = U_1 \oplus \big((U_2 \oplus \dots) \oplus U_m\big). $$ Offenbar ist jede direkte Summe auch **iterativ direkt**; die Umkehrung folgt mit **Induktion**.
<!--ID: 1738335372225-->

---

#### Bew 

Beweis von der Direktnen Summe #flashcard 

Folgt direkt aus dem **Zerlegungslemma**, mit **Induktion über $m$**. 
<!--ID: 1738335486683-->


---

#### Bem

Ist $\dim V < \infty$ und **$\text{Char}(K) = 0$**, so liefert die **Hauptraumzerlegung** auch die **algebraischen Vielfachheiten** $k_i$ der **Eigenwerte** $x_i$, $$ k_i = \dim H_i. $$
Unter der Annahme, dass **$\chi_f(t) = (t - x_1)^{k_1} \dots (t - x_m)^{k_m}$** zerfällt, folgt auch $$ \mu_f(t) = (t - x_1)^{h_1} \dots (t - x_m)^{h_m}; $$ die **Hauptraumzerlegung** liefert $f$-invariante Unterräume $H_i$, auf denen $f_i = f|_{H_i}$ charakteristische Polynome $$ \chi_{f_i}(t) = (t - x_i)^{k_i}, $$ mit **$k_i = \dim H_i$** haben.

---

#### Kor&Def

Zerfällt **$\mu_f(t)$** in **(paarweise verschiedene) Linearfaktoren**, so ist $f \in \text{End}(V)$ **diagonalisierbar**, d.h., $V$ besitzt eine **Basis $B$ aus Eigenvektoren von $f$**. #flashcard 

---
### **Beweis**([[#Kor&Def]])

Mit der **Hauptraumzerlegung** zerfällt $V$ in **Eigenräume von $f$**: $$ V = \bigoplus_{i=1}^{m} H_i = \bigoplus_{i=1}^{m} \ker(d_i x_i - f). $$ Da die **Summe direkt** ist, liefert die **Vereinigung von Basen $B_i$** der Eigenräume $H_i$ eine **Basis $B$ von $V$**.
<!--ID: 1738336899952-->

---
#### Bem

Eigenvektoren $v_1, \dots, v_m \in V$ zu **paarweise verschiedenen Eigenwerten** $x_1, \dots, x_m \in K$ eines $f \in \text{End}(V)$ sind **stets linear unabhängig**: Setzt man $w_i := v_i y_i$, mit $y_1, \dots, y_m \in K$, und $w = \sum_{i=1}^{m} w_i$, so liefert wiederholte Anwendung von $f$ auf $f(w)$ die Matrixgleichung: $$ (f - x_1) w = (w_1, \dots, w_m) X $$ mit der **Vandermonde-Matrix** $$ X = (x_i^{j-1})_{i,j=1,\dots,m}, $$ die **invertierbar** ist, weil $$ \det X = \prod_{i < j} (x_i - x_j) \neq 0. $$ - Also impliziert $0 = w = \sum_{i=1}^{m} w_i$ die Gleichung $$ (w_1, \dots, w_m) = (f - x_1) w, \quad X^{-1} = (0, \dots, 0), $$ d.h. **$w_i = 0$ für alle $i = 1, \dots, m$** und damit sind die Eigenvektoren **linear unabhängig**. 

---

### **Bemerkung**

Ist $\dim V < \infty$ und $\text{Char}(K) = 0$, so ist $f \in \text{End}(V)$ genau **dann diagonalisierbar**, wenn **$\chi_f(t)$ in Linearfaktoren zerfällt** und die algebraischen und geometrischen Vielfachheiten aller Eigenwerte **übereinstimmen**: $$ \chi_f(t) = \prod_{i=1}^{m} (t - x_i)^{k_i}, \quad \forall i = 1, \dots, m : k_i = g_i. $$ - **Folgerung:** Ein Endomorphismus $f \in \text{End}(V)$ mit $\dim V = m$ und **paarweise verschiedenen Eigenwerten** ist **stets diagonalisierbar**. 

---

### **Buchhaltung**

- Ist also $\dim V < \infty$ und $$\mu_f(t) = (t - x_1)^{r_1} \dots (t - x_m)^{r_m},$$ mit den **paarweise verschiedenen Eigenwerten** $x_i$, so hat $f$ eine **Darstellungsmatrix in Block-Diagonalgestalt**: $$ \Delta_B(f) = \text{diag}(X_1, \dots, X_m), \quad X_i = \text{diag}(x_i, \dots, x_i) \in K^{k_i \times k_i}, $$ wobei die **algebraischen Vielfachheiten** $k_i = \dim H_i$ der Eigenwerte $x_i$ sind. - Ist $f$ **diagonalisierbar**, dann $r_1 = \dots = r_m = 1$, sodass $\Delta_B(f)$ **diagonale Gestalt** hat: $$ X_i = x_i \cdot \mathbb{1}. $$ 
- **Umkehrung:** Ist $f$ **offenbar diagonalisierbar**, wenn eine Basis $B$ existiert, sodass $\Delta_B(f)$ eine **Diagonalmatrix** ist, d.h., **$r_i = 1$ für alle $i$**. 

- Man sagt, $f$ ist **triangulierbar**, wenn eine Basis $B$ existiert, sodass die Darstellungsmatrix **$\Delta_B(f)$ eine obere Dreiecksmatrix** ist. 

- Dies ist genau der Fall, wenn **$\chi_f(t)$ in Linearfaktoren zerfällt**, das Minimalpolynom in Linearfaktoren zerfällt und **$f$ mindestens eine Kette von Eigenvektoren besitzt**. 

- **Bemerkung:** Auch nennt man eine Matrix $X \in K^{n \times n}$ **diagonalisierbar** oder **triangulierbar**, **falls $f_X \in \text{End}(K^{n \times n})$ diagonalisierbar bzw. triangulisierbar ist**. 

- **Fazit:** Dies ist genau der Fall, wenn **$Q \in \text{Gl}(n)$ existiert**, sodass $$ Q^{-1} X Q $$ eine **Diagonalmatrix bzw. obere Dreiecksmatrix** ist.