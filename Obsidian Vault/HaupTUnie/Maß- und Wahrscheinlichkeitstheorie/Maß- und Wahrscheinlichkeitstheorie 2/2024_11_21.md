# 4.4.1 Eigenschaften der bedingten Erwartung

Die bedingte Erwartung erfüllt eine Reihe von Eigenschaften, die nachfolgend aufgelistet sind. Die meisten davon sind analog zu Eigenschaften, die wir bereits für die übliche Erwartung bewiesen haben. Ich werde lediglich drei dieser Eigenschaften beweisen (farblich markiert), die spezifisch für die bedingte Erwartung sind und die Sie bisher noch nicht gesehen haben. 

## Satz 4.17: Eigenschaften der bedingten Erwartung 

Seien $X, Y$ $L^1$-Zufallsvariablen auf einem Wahrscheinlichkeitsraum $(\Omega, \mathcal{A}, P)$ und $\mathcal{F}$ eine Unter-$\sigma$-Algebra von $\mathcal{A}$. Dann gilt: 

1. **Linearität** $$E(aX + Y \mid \mathcal{F}) = aE(X \mid \mathcal{F}) + E(Y \mid \mathcal{F}), \quad \forall a \in \mathbb{R}.$$
2. **Monotonie** Falls $X \leq Y$ fast sicher, dann gilt: $$E(X \mid \mathcal{F}) \leq E(Y \mid \mathcal{F}).$$
 3. **Multiplikationseigenschaft** Falls das Produkt $XY \in L^1$ und $Y$ $\mathcal{F}$-messbar ist, dann gilt: $$E(XY \mid \mathcal{F}) = Y \cdot E(X \mid \mathcal{F}).$$ Insbesondere gilt: $$E(Y \mid \mathcal{F}) = E(Y \mid Y) = Y, \quad \text{falls } Y \text{ $\mathcal{F}$-messbar ist.}$$
 4. **Turmeigenschaft** Falls $\mathcal{G} \subseteq \mathcal{F}$ eine Unter-$\sigma$-Algebra von $\mathcal{F}$ ist, dann gilt: $$E[E(X \mid \mathcal{F}) \mid \mathcal{G}] = E[E(X \mid \mathcal{G}) \mid \mathcal{F}] = E(X \mid \mathcal{G}).$$
 5. **Dreiecksungleichung** $$\lvert E(X \mid \mathcal{F}) \rvert \leq E(\lvert X \rvert \mid \mathcal{F}).$$
 6. **Unabhängigkeit** Falls $X$ und $\mathcal{F}$ unabhängig sind, gilt: $$E(X \mid \mathcal{F}) = E(X).$$ Insbesondere gilt dies, wenn $\mathcal{F}$ trivial ist, d.h., wenn für jedes $A \in \mathcal{F}$ $P(A)$ entweder $0$ oder $1$ ist. 

 7. **Dominierte Konvergenz** Falls $Y > 0$ ($Y \in L^1$) und $\{X_n\}_{n \geq 1}$ eine Folge von Zufallsvariablen ist mit $X_n \to X$ fast sicher und $\lvert X_n \rvert \leq Y$ fast sicher für alle $n$, dann gilt: $$\lim_{n \to \infty} E(X_n \mid \mathcal{F}) = E(X \mid \mathcal{F}),$$ fast sicher und in $L^1(P)$. 

 8. **Jensensche Ungleichung** Falls die integrierbare Zufallsvariable $X$ Werte in einem Intervall $I \subseteq \mathbb{R}$ annimmt und $\phi: I \to \mathbb{R}$ eine konvexe Funktion ist, dann gilt: $$\phi(E(X \mid \mathcal{F})) \leq E(\phi(X) \mid \mathcal{F}),$$ fast sicher. Da die bedingte Erwartung nur bis auf Nullmengen eindeutig definiert ist, gelten alle Gleichungen und Ungleichungen in obigen Aussagen **mit Wahrscheinlichkeit 1**. 

--- 
## Bemerkung 4.18:

Einige Beobachtungen zu diesen Eigenschaften 
1. **Zu Eigenschaft c)** Diese Eigenschaft besagt vereinfacht, dass, wenn die Zufallsvariable, von der wir die bedingte Erwartung berechnen wollen, einen Faktor enthält, der $\mathcal{F}$-messbar ist, dieser „aus der Erwartung herausgezogen“ werden kann. 
2. **Zu Eigenschaft d)** Diese Eigenschaft, auch „Turmeigenschaft der bedingten Erwartung“ genannt, besagt, dass wenn wir zwei Unter-$\sigma$-Algebren $\mathcal{F}$ und $\mathcal{G}$ von $\mathcal{A}$ haben, und eine feiner als die andere ist (z.B. $\mathcal{F}$ enthält $\mathcal{G}$), dann die Reihenfolge, in der wir die bedingte Erwartung berechnen, keine Rolle spielt. Das Ergebnis ist die Erwartung, die nur auf der kleineren $\sigma$-Algebra ($\mathcal{G}$) basiert.

3. Anwendung der bedingten Jensen-Ungleichung Eine Anwendung der bedingten Jensen-Ungleichung zeigt, dass, wenn $X \in L^2(\Omega, \mathcal{A}, P)$, auch $E(X \mid \mathcal{F}) \in L^2(\Omega, \mathcal{F}, P)$. (4.13) Tatsächlich folgt mit $\phi: \mathbb{R} \to \mathbb{R}, \phi(x) = x^2$ aus (4.12), dass: $$(E(X \mid \mathcal{F}))^2 \leq E(X^2 \mid \mathcal{F}).$$ Nimmt man die Erwartung der linken und rechten Seite, ergibt sich: $$E((E(X \mid \mathcal{F}))^2) \leq E(X^2) < \infty.$$ 
---
### Beweis der Eigenschaften c), d), f) 

**Beweis von c)** Betrachten wir zunächst den Fall, dass $X, Y$ positiv sind und $Y$ nur abzählbar viele Werte $y_n, n \geq 1$ annimmt. Es ist klar, dass $Y \cdot E(X \mid \mathcal{F})$ $\mathcal{F}$-messbar ist (weil beide Faktoren $\mathcal{F}$-messbar sind), daher genügt es zu zeigen: $$\int_A Y \cdot E(X \mid \mathcal{F}) \, dP = \int_A XY \, dP \quad \text{für alle } A \in \mathcal{F}.$$ Wir schreiben (unter Verwendung der monotonen Konvergenz, um die Reihenfolge von Summation und Integration in Schritt 2 und 4 zu tauschen): $$ \begin{aligned} \int_A Y \cdot E(X \mid \mathcal{F}) \, dP &= E(1_A Y \cdot E(X \mid \mathcal{F})) \\ &= \sum_{n \geq 1} E(1_A 1_{Y = y_n} E(X \mid \mathcal{F})) \\ &= \sum_{n \geq 1} E(1_A 1_{Y = y_n} X) \\ &= E(1_A XY), \end{aligned} $$ wobei wir im dritten Schritt benutzt haben, dass $A \cap \{Y = y_n\} \in \mathcal{F}$, weil $Y$ $\mathcal{F}$-messbar ist. Dies zeigt Eigenschaft c) im Spezialfall $X, Y > 0$ und $Y$ diskret. Falls $X, Y > 0$, aber $Y$ nicht diskret ist, wählen wir eine Folge $Y_n$ positiver diskreter Zufallsvariablen, die fast sicher gegen $Y$ konvergiert (z.B. $Y_n = 2^{-n} \lfloor 2^n Y \rfloor$). Wiederholen Sie das Argument für $Y_n$ anstelle von $Y$ und verwenden Sie die monotone Konvergenz, um den Limes $n \to \infty$ zu nehmen und $Y_n$ durch $Y$ zu ersetzen. Falls $X, Y$ nicht positiv sind, teilen wir sie wie üblich in positive und negative Teile auf: $$X = X^+ - X^-, \quad Y = Y^+ - Y^-.$$ Dann wenden wir die Linearität (Eigenschaft a)) an. 


**Beweis von d)** Die Gleichung $E[E(X \mid \mathcal{G}) \mid \mathcal{F}] = E(X \mid \mathcal{G})$ folgt aus c), da $E(X \mid \mathcal{G})$ $\mathcal{G}$-messbar ist und daher auch $\mathcal{F}$-messbar (weil $\mathcal{G} \subseteq \mathcal{F}$). Um die andere Gleichung zu beweisen, sei $A \in \mathcal{G}$, also insbesondere $A \in \mathcal{F}$. Dann gilt die folgende Gleichungskette: $$ \begin{aligned} E(1_A E[E(X \mid \mathcal{F}) \mid \mathcal{G}]) &= E(1_A E(X \mid \mathcal{F})) && \text{weil } A \in \mathcal{G}, \\ &= E(1_A X) && \text{weil } A \in \mathcal{F}, \\ &= E(1_A E(X \mid \mathcal{G})) && \text{weil } A \in \mathcal{G}. \end{aligned} $$ Nun haben wir zwei $\mathcal{G}$-messbare Funktionen ($f = E(X \mid \mathcal{G})$ und $g = E[E(X \mid \mathcal{F}) \mid \mathcal{G}]$), deren Integrale für alle $A \in \mathcal{G}$ übereinstimmen. Daher gilt $f = g$ fast sicher (man betrachte die Mengen $A = \{f > g\}$ und $A' = \{f < g\}$) #
$\square$. 

**Beweis von f)**

Offensichtlich ist $E(X)$ $\mathcal{F}$-messbar, da es eine Konstante ist. Für $A \in \mathcal{F}$ gilt:  
$$
E(1_A X) = \text{(Unabhängigkeit)} = E(X) P(A) = E(1_A E(X)).
$$

---

### 4.4.2 Die intuitive Bedeutung

Die Definition von $E(X \mid \mathcal{F})$ ist möglicherweise nicht sehr anschaulich, und der Existenzbeweis (Satz 4.14) trägt dazu wenig bei.

---

## Satz 4.19: Bedingte Erwartung als orthogonale Projektion

Sei $\mathcal{F} \subseteq \mathcal{A}$ eine $\sigma$-Algebra und $X$ eine $L^2(\Omega, \mathcal{A}, P)$-Zufallsvariable. Jede $\mathcal{F}$-messbare und quadratintegrierbare Zufallsvariable $Y$ erfüllt:  
$$
E[(X - Y)^2] \geq E[(X - E(X \mid \mathcal{F}))^2], \tag{4.14}
$$
wobei Gleichheit nur gilt, wenn $Y = E(X \mid \mathcal{F})$ (fast sicher).

---

### Beweis([[#Satz 4.19 Bedingte Erwartung als orthogonale Projektion]])

Aus Satz 4.17 (Punkt h) und Bemerkung 4.18 (Punkt 3) wissen wir, dass $E(X \mid \mathcal{F})$ quadratintegrierbar ist, wenn $X$ quadratintegrierbar ist.  

Durch Entwicklung des Quadrats erhalten wir:
$$
\begin{aligned}
E[(X - Y)^2] - E[(X - E(X \mid \mathcal{F}))^2] 
&= E(X^2 + Y^2 - 2XY - X^2 + 2X E(X \mid \mathcal{F}) - (E(X \mid \mathcal{F}))^2) \\
&= E(Y^2 - 2Y E(X \mid \mathcal{F}) + (E(X \mid \mathcal{F}))^2) \\
&= E[(Y - E(X \mid \mathcal{F}))^2] \geq 0.
\end{aligned}
$$

- Im ersten Schritt wurde das Quadrat entwickelt.  
- Im zweiten Schritt wurde verwendet, dass:
  $$
  E(XY) = E[E(XY \mid \mathcal{F})] = E(Y E(X \mid \mathcal{F})),
  $$
  was aus der Turmeigenschaft folgt, da $Y$ $\mathcal{F}$-messbar ist.  
- Außerdem gilt:
  $$
  E(XE(X \mid \mathcal{F})) = E[E(XE(X \mid \mathcal{F}) \mid \mathcal{F})] = E(E(X \mid \mathcal{F})^2),
  $$
  ebenfalls durch die Turmeigenschaft, da $E(X \mid \mathcal{F})$ $\mathcal{F}$-messbar ist.

---

#### Bemerkung 4.20: Geometrische Interpretation

Um die Bedeutung zu verstehen, denken wir an den euklidischen Raum (z.B. $\mathbb{R}^3$):  
- Sei $P$ eine Ebene, die den Ursprung enthält, und $x$ ein Punkt außerhalb von $P$.  
- Es gibt genau einen Punkt $y^* \in P$, der den minimalen Abstand zu $x$ hat. Dieser Punkt $y^*$ ist die orthogonale Projektion von $x$ auf $P$.  
- Für jeden anderen Punkt $y \in P$ gilt $||y - x|| \geq ||y^* - x||$.

Ähnlich ist $L^2(P)$ ein Vektorraum mit einem Skalarprodukt definiert durch $\langle f, g \rangle = E(fg)$. Dieses Skalarprodukt induziert eine Distanz:  
$$
||f - g||_{L^2} = \sqrt{\langle f - g, f - g \rangle}.
$$  
Die Menge $V_{\mathcal{F}}$ aller quadratintegrierbaren Zufallsvariablen $Y$, die $\mathcal{F}$-messbar sind, bildet einen linearen Unterraum von $L^2(P)$. Satz 4.19 besagt, dass $E(X \mid \mathcal{F})$ die orthogonale Projektion von $X$ auf $V_{\mathcal{F}}$ ist, d.h., der Punkt in $V_{\mathcal{F}}$, der den minimalen $L^2$-Abstand zu $X$ hat.

---

### Statistische Interpretation

Eine alternative Sichtweise des Satzes 4.19 ist folgende:  
- Unter allen $\mathcal{F}$-messbaren Zufallsvariablen ist $E(X \mid \mathcal{F})$ diejenige, die $X$ am nächsten kommt.  
- Angenommen, wir interessieren uns für eine zufällige experimentelle Größe $X$, haben aber nur teilweise Zugriff darauf. Konkret dürfen wir nur beobachten, ob Ereignisse in $\mathcal{F}$ eintreten.  

Falls $\mathcal{F} = \sigma(Y)$ bedeutet dies, dass wir nur die Zufallsvariable $Y$ messen können und auf dieser Basis eine Schätzung für $X$ vornehmen möchten. Dann ist $E(X \mid Y)$ die beste Schätzung für $X$ (im Sinne des mittleren quadratischen Fehlers). In der Statistik würde man sagen, dass $E(X \mid Y)$ der beste Schätzer von $X$ unter Kenntnis von $Y$ ist, bezogen auf den mittleren quadratischen Fehler.

---

#### Bemerkung 4.21: Geometrische Interpretation der Turmeigenschaft

Im üblichen euklidischen Raum $\mathbb{R}^3$:  
- Sei $x$ ein Punkt, $\ell$ eine Gerade durch den Ursprung, und $P$ eine Ebene, die $\ell$ enthält.  
- Die orthogonale Projektion $x_0$ von $x$ auf $P$ ist der Punkt auf $P$, der die Distanz zu $x$ minimiert.  
- Die orthogonale Projektion $x_{00}$ von $x_0$ auf $\ell$ ist der Punkt auf $\ell$, der die Distanz zu $x$ minimiert.  

Ähnlich beschreibt die Turmeigenschaft der bedingten Erwartung, wie verschachtelte Projektionen funktionieren:  
$$
E[E(X \mid \mathcal{F}) \mid \mathcal{G}] = E(X \mid \mathcal{G}),
$$
wenn $\mathcal{G} \subseteq \mathcal{F}$.

#### 4.4.2 Geometrische Interpretation der Turmeigenschaft

Wenn wir zunächst $x$ auf $P$ projizieren und dann auf $\ell$, erhalten wir $x_{00}$. Projizieren wir hingegen zuerst $x$ auf $\ell$ und dann auf $P$, erhalten wir ebenfalls $x_{00}$, da $\ell \subseteq P$. Die Analogie zur Turmeigenschaft wird klar, wenn wir $x$ als quadratintegrierbare Zufallsvariable $X$ betrachten, $\ell$ als die Menge der $\mathcal{G}$-messbaren quadratintegrierbaren Zufallsvariablen und $P$ als die Menge der $\mathcal{F}$-messbaren quadratintegrierbaren Zufallsvariablen, wobei $\mathcal{F} \subseteq \mathcal{G}$. --- # Beispiel 4.22: Schätzung einer Zufallsvariablen Wir betrachten eine Zufallsvariable $Z$, die die Summe zweier unabhängiger Zufallsvariablen $X$ und $Y$ ist, beide gleichverteilt auf $[0, 1]$. Beobachten wir den Wert von $Y$ und stellen fest, dass $Y = y$, dann ist unsere beste Schätzung für $Z$: $$ Z = E(X) + y. $$ Der mittlere quadratische Fehler dieser Schätzung ist: $$ E((X + y - (1/2 + y))^2) = \text{Var}(X), $$ da $\text{Var}(X)$ minimal ist, wenn $a = E(X)$. Eine alternative Schätzung $f(y)$ würde einen größeren mittleren quadratischen Fehler verursachen: $$ E((X + y - f(y))^2) > \text{Var}(X). $$ 

---
## 4.4.3 Marginalverteilung, 

bedingte Verteilung, bedingte Erwartung Gegeben ein $n$-dimensionaler Zufallsvektor $(X_1, \dots, X_n)$ mit einer Verteilung $\mu$, nennt man die Verteilung der einzelnen Zufallsvariablen $X_i$ die **$i$-te Marginalverteilung**. Falls $\mu$ eine Dichte $f$ bezüglich des $n$-dimensionalen Lebesgue-Maßes $\lambda^n$ besitzt, hat die Marginalverteilung $\mu_i$ von $X_i$ die Dichte: $$ f_i(x) = \int_{\mathbb{R}^{n-1}} f(y_1, \dots, y_{i-1}, x, y_{i+1}, \dots, y_n) \, \lambda^{n-1}(dy). \tag{4.15} $$ ### Beweisidee Die Wahrscheinlichkeit $P(X_i \leq x)$ ist gegeben durch: $$ P(X_i \leq x) = P(X_1 \leq 1, \dots, X_i \leq x, X_{i+1} \leq 1, \dots, X_n \leq 1), $$ was sich als Integral der Dichte $f$ über alle Variablen außer $X_i$ ausdrücken lässt: $$ P(X_i \leq x) = \int_{-\infty}^x f_i(y) \, \lambda(dy). $$ Da $f$ nicht-negativ und integrierbar ist, folgt aus dem Satz von Fubini, dass $f_i(x)$ für fast jedes $x$ endlich ist. --- ## Fall $n = 2$ Sei $\mu$ eine Wahrscheinlichkeitsverteilung auf $(\mathbb{R}^2, \mathcal{B}(\mathbb{R}^2))$ mit Dichte $f(x, y)$ bezüglich des zweidimensionalen Lebesgue-Maßes $\lambda^2 = \lambda \otimes \lambda$. Die **Marginalverteilungen** $\mu_1$ und $\mu_2$ von $X_1$ und $X_2$ (die Wahrscheinlichkeitsverteilungen auf $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$) haben die Dichten: $$ f_1(x) = \int_{\mathbb{R}} f(x, y) \, \lambda(dy), \quad f_2(y) = \int_{\mathbb{R}} f(x, y) \, \lambda(dx). \tag{4.16} $$ --- ### Bedingte Erwartung Sei $h: \mathbb{R} \to \mathbb{R}$ eine messbare Funktion und $h(X_1)$ integrierbar. Wir wollen $Y = E(h(X_1) \mid X_2)$ bestimmen. Definiere die Funktion $g: \mathbb{R} \to \mathbb{R}$ als: $$ g(y) = \begin{cases} \int_{\mathbb{R}} h(x) \frac{f(x, y)}{f_2(y)} \, \lambda(dx), & \text{falls } f_2(y) > 0, \\ 0, & \text{sonst}. \end{cases} \tag{4.17} $$ --- ### Bemerkung zu $f_2(y) = 0$ Unter der Verteilung $\mu$ hat die Menge $E = \{(x, y) : f_2(y) = 0\} \subseteq \mathbb{R}^2$ Maß null: $$ \mu(E) = \int_{\{y : f_2(y) = 0\}} f_2(y) \, \lambda(dy) = 0. $$ Dies folgt aus der Definition der Dichte und der Tatsache, dass $f_2(y)$ fast überall bezüglich $\lambda$ endlich ist.

### Bedingte Erwartung 

$E(h(X_1) \mid X_2)$ Die Zufallsvariable $Y = g(X_2)$ ist $\sigma(X_2)$-messbar. Die $\sigma(X_2)$-messbaren Mengen $A$ haben die Form: $$ A = \{(x, y) \in \mathbb{R}^2 : y \in B\}, $$ wobei $B \in \mathcal{B}(\mathbb{R})$ eine Borelmengen ist. Dann gilt: $$ \begin{aligned} E(h(X_1)1_A) &= E(h(X_1)1_A 1_{E'}^c) \\ &= \int_{\mathbb{R}^2} f(x, y) h(x) 1_B(y) 1_{f_2(y) > 0} \, \lambda(dx) \lambda(dy) \\ &= \int_{\mathbb{R}} g(y) 1_B(y) f_2(y) 1_{f_2(y) > 0} \, \lambda(dy) \\ &= \int_{\mathbb{R}} g(y) 1_B(y) f_2(y) \, \lambda(dy) \\ &= E(g(X_2)1_B) = E(Y 1_B). \end{aligned} $$ Dabei haben wir den Satz von Fubini angewendet, da $h(X_1)$ nach Voraussetzung integrierbar ist. Folglich gilt: $$ E(h(X_1) \mid X_2) = g(X_2), $$ wobei: $$ g(y) = \begin{cases} \int_{\mathbb{R}} h(x) \frac{f(x, y)}{f_2(y)} \, \lambda(dx), & \text{falls } f_2(y) > 0, \\ 0, & \text{sonst}. \end{cases} \tag{4.18} $$ 

--- 
#### Bemerkung 4.23: 

Verhalten bei $f_2(y) = 0$ Auf der Menge $\{y : f_2(y) = 0\}$ haben wir $g(y) = 0$ definiert, obwohl jeder andere Wert ebenfalls möglich wäre. Diese Menge hat Maß Null. --- ## Beispiel 4.24: Berechnung von $E(X_1^2 \mid X_2)$ Sei $\mu$ die Verteilung des Paars $(X_1, X_2)$ mit gemeinsamer Dichtefunktion: $$ f(x, y) = \begin{cases} x + \frac{3}{2}y^2, & \text{falls } (x, y) \in (0, 1)^2, \\ 0, & \text{sonst}. \end{cases} $$ Wir wollen $E(X_1^2 \mid X_2)$ berechnen. Nach Formel (4.17) bestimmen wir zunächst $f_2(y)$: $$ f_2(y) = \int_{(0, 1)} f(x, y) \, \lambda(dx) = \int_0^1 \left(x + \frac{3}{2}y^2\right) dx = \frac{3}{2}y^2 + \frac{1}{2}. $$ Nun berechnen wir $g(y)$: $$ \begin{aligned} g(y) &= \int_{(0, 1)} x^2 \frac{f(x, y)}{f_2(y)} \, \lambda(dx) \\ &= \int_0^1 x^2 \frac{x + \frac{3}{2}y^2}{\frac{1}{2} + \frac{3}{2}y^2} dx \\ &= \frac{1}{\frac{1}{2} + \frac{3}{2}y^2} \int_0^1 \left(x^3 + \frac{3}{2}y^2 x^2\right) dx \\ &= \frac{1}{1 + 3y^2} \left[\frac{x^4}{4} + \frac{3}{2}y^2 \frac{x^3}{3}\right]_0^1 \\ &= \frac{1}{1 + 3y^2} \left(\frac{1}{4} + \frac{1}{2}y^2\right). \end{aligned} $$ Also ist: $$ g(y) = \frac{\frac{1}{2} + y^2}{1 + 3y^2}. $$ Die bedingte Erwartung ist: $$ E(X_1^2 \mid X_2) = \frac{\frac{1}{2} + X_2^2}{1 + 3X_2^2}. $$