### 3.2 Symmetrische Sesquilinearformen & der Satz von Sylvester:

Symmetrische Sesqillinearformen bilden die **(algebraische) Grundlage** für die geometrischen Begriffe „Länge“ und „Winkel“. In diesem Abschnitt werden grundlegende Eigenschaften symmetrischer Sesquilinearformen diskutiert und insbesondere der zentrale **Struktur-Satz von Sylvester** bewiesen. Dabei ist bemerkenswert, dass es an diversen Stellen besser ist, zu **spezialisieren** – im Gegensatz zu anderen Situationen bringt Verallgemeinerung hier **keine größere Klarheit**, sondern eher **technische Schwierigkeiten**. **Polarisation** liefert dafür **erstes Evidenz**:

TARGET DECK: Lineare Algebra 2
#### Def&Lem 3.2(3) (Polarisation):

Definition von Polarisation und der Fal für $char(K) \neq 2$ #flashcard 

Die **induzierte quadratische Form** einer **symmetrischen Bilinearform** $\sigma : V \times V \to K$ ist die Abbildung $$ q : V \to K, \quad v \mapsto q(v) := \sigma(v, v). $$ Falls **$\text{Char}(K) \neq 2$**, so gilt damit: $$\forall v, w \in V : \quad \sigma(v, w) = \frac{q(v + w) - q(v) - q(w)}{2}.$$<!--ID: 1738240562521-->

---

#### Bew 3.2(4):

Beweis für die Darstellung der Polarisation im falle $char(K) \neq 2$ #flashcard 

Sei $v, w \in V$, so gilt $$q(v + w) = q(v) + 2\sigma(v, w) + q(w);$$ wegen $\text{Char}(K) \neq 2$ kann man **nach $\sigma(v, w)$ auflösen**.
<!--ID: 1738240713909-->

---

#### Bem 3.2(5):

**Allgemeine Charakteristik:** Im Allgemeinen liefert $q(v) = \sigma(v, v)$ **keine quadratische Form**, da $q(vx) = q(x) \bar{x} x^2$ gelten muss, **wenn** $\bar{x} \neq x$. 

---

#### Bem 3.2(6):

**Falls $\text{Char}(K) = 0$**, so kann man **alternativ** auch verwenden: $$\forall v, w \in V : \quad \sigma(v, w) = \frac{q(v + w) - q(v - w)}{4}.$$ 
**Hermitesche Sesquilinearformen:** 
Eine **ähnliche (kompliziertere) Form** gilt für **Hermitesche Sesquilinearformen**: Ist $\sigma : V\times V \to K$ eine **symmetrische Sesquilinearform** und $\text{Char}(K) \neq 2$, so kann man **beweisen**: $$\sigma = 0 \Leftrightarrow \forall v \in V : \sigma(v,v) = 0 $$Nähmlich : Ist $\sigma \neq 0$, so gibt es $v,w \in V$ mit $\sigma(v,w) = 1$ annehmen kann (ersetzen wir $w' = w\frac{1}{\sigma(v,w)}$);
- Ist nun $\sigma(v, v) \neq 0$ oder $\sigma(w, w) \neq 0$, so ist man fertig.
- Ist aber $\sigma(v, v) = \sigma(w, w) = 0$, so folgt $$\sigma(v + w, v + w) = 2 \neq 0.$$
---

### Mission 3.2(10): 

Als erste geometrische Interpretation des allgemeinen Setup
wird der Begriff der “Orthogonalität” diskutiert: Zwei Vektoren schliessen
einen “rechten Winkel” ein, wenn ihr Produkt verschwindet

---

#### Def 3.2(11):

Definition von:
- **Orthogonal**
- **Orthogonalraum**
- **Orthogonalsystem**
- **Orthogonalbasis** #flashcard 

Sei $\sigma$ eine **symmetrische Sesquilinearform** auf $V$; dann heißen - **Vektoren $v, w \in V$ orthogonal (bzgl. $\sigma$)**, falls $\sigma(v, w) = 0$: $$ v \perp w \quad \Leftrightarrow \quad \sigma(v, w) = 0. $$
- **$S^\perp := \bigcap_{s \in S} \ker \sigma(s, \cdot)$** der **Orthogonalraum** von $S \subseteq V$ $(S \neq \emptyset)$; 
-  **$(e_i)_{i \in I}$ ein Orthonormalsystem**, falls $$\forall i, j \in I : \quad \sigma(e_i, e_j) = \pm \delta_{ij}.$$
- **$(e_i)_{i \in I}$ eine Orthonormalbasis**, falls $(e_i)_{i \in I}$ **Orthonormalsystem und Basis** ist.
<!--ID: 1738242124037-->

---

#### Bem 3.2(16):

Wegen der **Symmetrie** von $\sigma$ ist die **Orthogonalitätsrelation** auch **symmetrisch**: $$ w \perp u \quad \Leftrightarrow \quad u \perp w. $$

---
#### Bem 3.2(17):

**Eigenschaften des Orthogonalraums:** Da $\forall v \in V : \sigma(v, \cdot) \in V^*$, ist der **Orthogonalraum** **wohledefiniert** und (als **Schnitt von Unterräumen**) ein **Unterraum**: $$ S^\perp \subseteq V. $$ Offenbar gilt: $$ \bar{S} \subseteq S \quad \Rightarrow \quad \bar{S}^\perp \supseteq S^\perp. $$ - **Folgerung:** Insbesondere folgt **$S^{\perp \perp} \supseteq S$**; andererseits ist auch $S^{\perp \perp} \subseteq [S]$, denn für $w \in S^\perp$ und $v = \sum_{s \in S} s x_s \in [S]$ ist $$ \sigma(v, w) = \sum_{s \in S} x_s \sigma(s, w) = 0. $$ - **Schlussfolgerung:** Auch **$[S]^\perp = S^\perp$** für jede **Teilmenge** $\emptyset \neq S \subseteq V$. Ähnlich zeigt man, dass für jede **Familie** $(U_i)_{i \in I}$ von **Unterräumen** $U_i \subseteq V$ gilt: $$ \left( \sum_{i \in I} U_i \right)^\perp = \bigcap_{i \in I} U_i^\perp. $$

---

#### Bem 3.2(18):

- Jedes **Orthonormalsystem** $(e_i)_{i \in I}$ ist **linear unabhängig**, denn $$ 0 = v = \sum_{j \in I} e_j x_j \quad \forall i \in I : \sigma(e_i, v) = \pm x_i. $$ - Ist $(e_i)_{i \in I}$ eine **Orthonormalbasis** von $V$, so gilt für $v \in V$: $$ v = \sum_{i \in I} e_i \frac{\sigma(v, e_i)}{\sigma(e_i, e_i)}. $$ Insbesondere **impliziert die Existenz** einer **Orthonormalbasis**, dass $\sigma$ **nicht-degeneriert** ist.

---

#### Def 3.2(20):

Definition von **Radiakalfrei** #flashcard 

Eine **symmetrische Sesquilinearform** $\sigma : V \times V \to K$ ist genau dann ein **Skalarprodukt**, wenn sie **radikalfrei** ist, d.h., wenn ihr **Radikal** $V^\perp$ trivial ist: $$ V^\perp = \{0\}. $$
<!--ID: 1738243601988-->


---

#### Bsp 3.2(21):

Betrachte $V = \mathbb{R}^2$ mit der **Standardbasis** $E = (e_1, e_2)$ und die **symmetrische Sesquilinearform** (**Bilinearform**) $\sigma : V \times V \to \mathbb{R}$, sei durch ihre **Gramsche Matrix** $\Gamma_E(\sigma)$ gegeben: 1. **Fall 1:** - $$\Gamma_E(\sigma) = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$$ - liefert ein **Skalarprodukt**, da $V^\perp = \{0\}$. - Die Einschränkungen $\sigma|_{\langle e_i \rangle}, i = 1, 2$, sind jedoch **keine Skalarprodukte**. 2. **Fall 2:** - $$\Gamma_E(\sigma) = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$$ - liefert **kein Skalarprodukt**, da $V^\perp = [e_1 - e_2] \neq \{0\}$. - Jede Einschränkung $\sigma|_{\langle v \rangle}$ für $v = e_1 + e_2 y$ mit $y \neq 0$ liefert jedoch ein **Skalarprodukt**.

---

#### Bem&Def 3.2(24):

Definition eines Orthogonalen Komplimentes #flashcard 

Ist $V = U \oplus U^\perp$ für einen Unterraum $U \leq V$, so nennt man **$U^\perp$** auch das **orthogonale Komplement** von $U \leq V$.
<!--ID: 1738243649513-->

---
#### Bsp 3.2(25):

Nicht jeder Unterraum $U \leq V$ hat ein **orthogonales Komplement**: Im vorigen Beispiel ist für $U = [w]$ mit $w = e_1 y_1 + e_2 y_2 \neq 0$: --- ## **3.2 Symmetrische Sesquilinearformen & der Satz von Sylvester** 1. **(i)** $$U^\perp = [e_1 y_1 - e_2 y_2],$$ also ist $V = U \oplus U^\perp$ genau dann, wenn $y_1 y_2 \neq 0$. 2. **(ii)** $$U^\perp = \begin{cases} [e_1 - e_2] = V^\perp, & \text{falls } U \neq V^\perp, \text{ also } V = U \oplus U^\perp; \\ V, & \text{falls } U = V^\perp, \text{ also } V \neq U \oplus U^\perp. \end{cases} $$

---

### Bem&Def 3.2(28):

Definiton von **Orthogonaleprojektion** #flashcard 

Ist $(e_i)_{i \in I}$, $\# I < \infty$, ein **endliches Orthonormalsystem**, so definiert man die **Orthogonalprojektion** $p : V \to V$ auf $U := \langle (e_i)_{i \in I} \rangle \leq V$ durch: $$ p : V \to V, \quad v \mapsto p(v) := \sum_{i \in I} e_i \frac{\sigma(v, e_i)}{\sigma(e_i, e_i)}. $$ Dabei gilt $p^2 = p$ und $p(V) \perp \ker p$.
<!--ID: 1738257467433-->


**Erklärung** Mit der **komplementären Projektion** $p' := \text{id}_V - p$ gilt für $v \in V$: $$ \forall j \in I : \quad \sigma(e_j, p'(v)) = \sigma(e_j, v) - \sum_{i \in I} \delta_{ji} \sigma(e_i, v) = 0. $$ - Also gilt $p'(v) \in U^\perp$, und **somit** $p'(V) \subseteq U^\perp$. - Weiter gilt für $v \in U^\perp$: $$ v \in U^\perp \quad \Rightarrow \quad \forall i \in I : \sigma(e_i, v) = 0 \quad \Rightarrow \quad v \in \ker p. $$ - Also ist $U^\perp = \ker p$ das **orthogonale Komplement** von $U = p(V)$. - Offenbar ist $p^\perp$ damit dann auch eine **Orthogonalprojektion**.
<!--ID: 1738243602017-->

---

#### Mission 3.2(29):

Mit diesen **Vorbereitungen** kann jetzt der **erste zentrale Satz** des Kapitels **bewiesen** werden: Die beiden folgenden **Lemmata** liefern einen **wesentlichen Teil des Beweises**.

---

#### Lem 3.2(30):

Für eine **symmetrische Sesquilinearform** $\sigma$ auf $V$ und $b \in V$ mit $\sigma(b, b) \neq 0$ gilt $$ V = [b] \oplus \{b\}^\perp. $$ #flashcard 

Es ist $V = [b] + \{b\}^\perp$, denn für beliebiges $v \in V$ ist $$ v = b \frac{\sigma(b, v)}{\sigma(b, b)} + u \in [b] + \{b\}^\perp $$ mit $u := v - b \frac{\sigma(b, v)}{\sigma(b, b)} \in \{b\}^\perp$. Weiter ist $[b] \cap \{b\}^\perp = \{0\}$, denn ist $v = bx \in [b] \cap \{b\}^\perp$, so gilt $$ 0 = \sigma(b, v) = \sigma(b, b) x \quad \Rightarrow \quad 0 = x. $$ Also folgt $$ V = [b] \oplus \{b\}^\perp. \quad $$
<!--ID: 1738244129863-->

---

#### Bem 3.2(31):

- Ist $\sigma(b, b) = 0$ für $b \in V$, so gilt $b \in [b] \cap \{b\}^\perp$, d.h., ist $b \neq 0$, so ist $[b] \cap \{b\}^\perp \neq \{0\}$. - Außerdem ist dann oft $U = \{b\}^\perp$ **degeneriert**, da $$ \exists x \in b^\perp \forall w \in U : \sigma(x, w) = 0. $$

---

#### Lem 3.2(32):

Das Othogonalisierungslemma #flashcard 

Sei $\sigma : V \times V \to K$ eine **symmetrische Sesquilinearform**, $n = \dim V < \infty$; dann gibt es eine **Basis** $(b_1, \dots, b_n)$ von $V$, die **diagonalisiert** ist, d.h., für die gilt $$ \sigma(b_i, b_j) = 0 \quad \text{falls } i \neq j. $$
<!--ID: 1738244414538-->

---

#### Bew 3.2(33) von ([[#Lem 3.2(32)]])

Beweis vom orthinomalisierungslemma #flashcard 

Durch **Induktion** über $n = \dim V$. - **Für $n = 1$:** $V = [b]$, ist die Behauptung trivial. - **Induktionsannahme:** Die Behauptung ist bewiesen für Dimension $n \in \mathbb{N}$. - **Induktionsschritt:** Sei $\sigma$ eine **symmetrische Sesquilinearform** auf $V$, wobei $\dim V = n + 1$. **OBdA** ist $\sigma(b, b) \neq 0$, also existiert $b \in V$ mit $\sigma(b, b) \neq 0$. **Definiere** $U := \{b\}^\perp$ mit $\dim U = n$. Nach Induktionsannahme existiert eine Basis $(b_1, \dots, b_n)$ auf $U$, die $\sigma|_U$ **diagonalisiert**. Da $b \perp b_i$, folgt die Basis $$B := (b, b_1, \dots, b_n)$$ als eine **diagonalisierbare Basis**.
<!--ID: 1738244491469-->

---
