
$$ y' = A(t)y \tag{(3.2)}$$

- $\mathcal{L}$  := Menge aller Lösungen von (3.2)
- Die Abbildung  $$\mathbb{R}^d \to \mathcal{L}, \quad y_0 \mapsto y(t, y_0) \in \mathcal{L}$$  ist ein **Vektorraumisomorphismus** (linear, bijektiv).
- d l.u. Lösungen $y^1, \dots, y^d \in \mathcal{L}$  bilden eine **Basis** von $\mathcal{L}$ .  Sie heißen auch **Fundamentalsystem**.
- Die Matrix  $Y(t) := (y^1(t), \dots, y^d(t))$  
  heißt **Lösungsmatrix**, falls $y^1, \dots, y^d \in \mathcal{L}$ .
- $Y$  heißt **fundamental**, falls $y^1, \dots, y^d$  linear unabhängig sind.
- Die **Wronskideterminante** ist definiert als  $W(t) := \det Y(t)$

---

### Satz 3.7 (Liouville)

##### Voraussetzung:  
keine  
##### Behauptung:  
Die Wronskideterminante erfüllt die ODE:
$$
W'(t) = \operatorname{tr} A(t) \cdot W(t), \quad \forall t \in J
$$
wobei die Spur einer Matrix definiert ist als  
$$\operatorname{tr} A(t) = \sum_{i=1}^{d} A_{ii}(t).$$  Insbesondere gilt für jedes feste $t_0 \in J$ : $W(t) = W(t_0) \cdot e^{\int_{t_0}^{t} \operatorname{tr} A(s) ds}$

---
### Korollar 3.9

##### Voraussetzung:  
Sei $Y \in C^1(J; \mathbb{R}^{d \times d})$ eine Lösungsmatrix für  $y' = A(t) \cdot y$
##### Behauptung:  
Dann gilt: \( Y \) ist Fundamentalmatrix  
$$ \iff \exists \bar{t} \in J \text{ mit } W(\bar{t}) \neq 0 \iff \forall t \in J \quad W(t) \neq 0 $$
#### Beweis von [[#Korollar 3.9]]:
Übung.

---

## 3.1.2 Bestimmen von FS

### 1. Möglichkeit  
Sei $\{ y^1, \dots, y^d \}$ Basis von $\mathbb{R}^d$  Dann ist  $y_{t_0, y^1}, \dots, y_{t_0, y^d}$  
ein **Fundamentalsystem**.

### 2. Möglichkeit (el. algebraisches Reduktionsprinzip)  
Falls eine Lösung von (3.2) bekannt ist, dann kann das $d \times d$ **Fundamentalsystem**  
auf ein $(d-1) \times (d-1)$ **FS** reduziert werden.

---

#### Beispiel 3.11

$\bigstar$ Betrachte das System:  
$$
y' =
\begin{pmatrix}
\frac{1}{t} & -1 \\
\frac{1}{t^2} & \frac{2}{t}
\end{pmatrix}
y, \quad J = (0, \infty)
$$

Eine Lösung ist $u = \begin{pmatrix} t^2 \\ -t \end{pmatrix}$.  Gesucht ist eine 2. Lösung, die linear unabhängig ist.  

Wir machen den Ansatz:
$$y = \Phi(t) u(t) + \begin{pmatrix} 0 \\ z_2(t) \end{pmatrix}, \quad \text{wobei } \Phi, z_2 \in C^1(J; \mathbb{R}).$$Einsetzen liefert:
$$\Phi'(t) u(t) + \Phi(t) u'(t) + \begin{pmatrix} 0 \\ z_2'(t) \end{pmatrix}
= \begin{pmatrix} 1/t & -1 \\ 1/t^2 & 2/t \end{pmatrix}
\left[ \Phi(t) u(t) + \begin{pmatrix} 0 \\ z_2(t) \end{pmatrix} \right]$$
Das heißt:

$$\Phi'(t) u(t) + \begin{pmatrix} 0 \\ z_2'(t) \end{pmatrix} =\begin{pmatrix}1/t & -1 \\ 1/t^2 & 2/t \end{pmatrix}\begin{pmatrix} 0 \\ z_2(t) \end{pmatrix}$$

Also:
$$\Phi' \cdot t^2 + 0 = \frac{1}{t} z_2(t) \quad \Rightarrow \quad \Phi'(t) = -\frac{z_2(t)}{t^2}$$
$$\Phi' \cdot (-t) + z_2'(t) = \frac{2}{t} z_2(t) \quad \Rightarrow \quad \Phi'(t) = -\frac{z_2(t)}{t^2}$$Umformen:
$$\frac{z_2(t)}{t^2} \cdot t + z_2'(t) = \frac{2}{t} z_2(t) \quad \Rightarrow \quad z_2'(t) = \frac{1}{t} z_2(t)$$

Lösung:
Eine Lösung ist $z_2(t) = t$, damit ist $\Phi'(t) = -\frac{1}{t}$, das heißt $\Phi(t) = -\ln(t)$.  

Damit ist die 2. Lösung des Systems:
$$\Phi(t) u(t) + \begin{pmatrix} 0 \\ z_2(t) \end{pmatrix} =
\begin{pmatrix} -t^2 \ln(t) \\ t + t \ln(t) \end{pmatrix}$$
Fundamentalmatrix:
$$Y(t) =
\begin{pmatrix}
t^2 & -t^2 \ln t \\
- t & t + t \ln t
\end{pmatrix}$$
Wronski-Determinante:
$W(t) = \det Y(t) = t^3 \ln(t) + t^3 - t^3 \ln t = t^3 \neq 0 \quad \text{auf } J$

## Allgemeines Vorgehen bei der elementarischen Reduktion

$$y' = A(t) y$$
Sei $t \mapsto u_1(t)$ eine Lösung mit $u_1 \neq 0$.  

Ansatz:
$$y = \Phi u_1 + \begin{pmatrix} 0 \\ z_2(t) \\ \vdots \\ z_d(t) \end{pmatrix}$$

Einsetzen in $\bigstar$ liefert:
$$\Phi' u_1 + \Phi u_1' + \begin{pmatrix} 0 \\ z_2' \\ \vdots \\ z_d' \end{pmatrix} 
= A \left( \Phi u_1 + \begin{pmatrix} 0 \\ z_2 \\ \vdots \\ z_d \end{pmatrix} \right)$$

Daraus folgt:
$$\begin{pmatrix} 0 \\ z_2' \\ \vdots \\ z_d' \end{pmatrix} = A \begin{pmatrix} 0 \\ z_2 \\ \vdots \\ z_d \end{pmatrix} - \Phi' u_1$$

Die 1. Komponente ergibt:
$$(3.2) \quad 0 = \sum_{j=2}^{d} A_{1j} z_j - \Phi' u_1$$

Daraus folgt:
$$\Phi' = \frac{1}{u_1} (A_{12}, \dots, A_{1d}) \begin{pmatrix} z_2 \\ \vdots \\ z_d \end{pmatrix}$$

Einsetzen in die Komponenten 2 bis $d$ liefert ...
$$(3) \quad 
\begin{pmatrix} 
z_2 \\ 
\vdots \\ 
z_d 
\end{pmatrix}'
=
\begin{pmatrix} 
A_{22} & \dots & A_{2d} \\ 
\vdots & \ddots & \vdots \\ 
A_{d2} & \dots & A_{dd} 
\end{pmatrix}
\begin{pmatrix} 
z_2 \\ 
\vdots \\ 
z_d 
\end{pmatrix}
- \Phi' 
\begin{pmatrix} 
u_2 \\ 
\vdots \\ 
u_d 
\end{pmatrix}$$

Umformen:
$$
\begin{pmatrix} 
z_2 \\ 
\vdots \\ 
z_d 
\end{pmatrix}'
\underbrace{ 
=\begin{pmatrix} 
A_{22} & \dots & A_{2d} \\ 
\vdots & \ddots & \vdots \\ 
A_{d2} & \dots & A_{dd} 
\end{pmatrix}
\begin{pmatrix} 
z_2 \\ 
\vdots \\ 
z_d 
\end{pmatrix}
- \frac{1}{u_1} 
\begin{pmatrix} 
A_{12}, \dots, A_{1d} 
\end{pmatrix}
\begin{pmatrix} 
z_2 \\ 
\vdots \\ 
z_d 
\end{pmatrix}}_{B(t) \in \mathbb{R}^{(d-1) \times (d-1)}}
$$

Damit = Wenn $\bar{z}$ die reduzierte Gleichung (3.3) erfüllt, dann ist:
$$ y = \Phi u_1 + \begin{pmatrix} 0 \\ z \end{pmatrix} $$
eine Lösung von $y' = A y$.  

Wir suchen nun $(d-1)$ linear unabhängige Lösungen von (3.3).  
Sei:
$$ z^2 := \begin{pmatrix} 0 \\ z_2^2 \\ \vdots \\ z_d^2 \end{pmatrix}, \quad z^3 := \begin{pmatrix} 0 \\ z_2^3 \\ \vdots \\ z_d^3 \end{pmatrix}, \quad \dots, \quad z^d := \begin{pmatrix} 0 \\ z_2^d \\ \vdots \\ z_d^d \end{pmatrix} $$

Dann bilden die Funktionen $z^2, \dots, z^d$ ein **Fundamentalsystem** von (3.3).  

Definiere:
$$
\Phi' := \frac{1}{u_1} \begin{pmatrix} A_{12}, \dots, A_{1d} \end{pmatrix} z
$$

Dies kann umgeschrieben werden als:
$$
\Phi' = \frac{1}{u_1} \sum_{j=2}^{d} A_{1j} z_j
$$

Integration liefert:
$$
\Phi(t) = \int_{t_0}^{t} \frac{1}{u_1(s)} \sum_{j=2}^{d} A_{1j}(s) z_j(s) ds
$$
Setze$y_1^i = u_1$  $y^i := \Phi^i u_1 + z^i, \quad i = 2, \dots, d$  ein Fundamentalsystem für (3.2).

##### Behauptung:  
$$\{ y^i \mid i = 1, \dots, d \}$$  
bildet ein Fundamentalsystem für (3.2).

##### Beweis: 
Nach Konstruktion sind $y^i, i = 1, \dots, d$ Lösungen von $y' = A y$.  
Die Funktionen $y^i$ sind linear unabhängig.  

Sei $a_1, \dots, a_d \in \mathbb{R}$ mit  
$$\sum_{i=1}^{d} a_i y^i \equiv 0$$
##### zz:
$$ a_1 = \dots = a_d = 0 $$  Ausgeschrieben:  
$$ 0 = a_1 u_1 + \sum_{i=2}^{d} a_i \Phi^i u_1 + \sum_{i=2}^{d} a_i z^i $$

**1. Komponente liefert:**  
$$ 0 = u_1 \left( a_1 + \sum_{i=2}^{d} a_i \Phi^i \right) $$  
Daraus folgt:  
$$ a_1 + \sum_{i=2}^{d} a_i \Phi^i = 0 $$  

Einsetzen in (3.5) liefert für die Komponenten $2$ bis $d$:  
Da $z^i$ für $i = 2, \dots, d$ Lösungen von (3.3) sind:  
$$ 0 = \sum_{i=2}^{d} a_i z^i = \sum_{i=2}^{d} a_i \begin{pmatrix} 0 \\ z_2^i \\ \vdots \\ z_d^i \end{pmatrix} $$
Da die $z^i, i = 2, \dots, d$ ein **Fundamentalsystem** bilden,  und sie linear unabhängig sind, folgt:  
$$ a_2 = \dots = a_d = 0 $$  
Aus (3.5) folgt dann:  
$$ a_1 = 0 $$  

---

## 3.2 Homogene Systeme mit konstanten Koeffizienten

### (3.7)
$$ y' = A y, \quad A \in \mathbb{R}^{d \times d} $$

**Ziel:** Fundamentalmatrix  

**Motivation:** Picard-Lindelöf mit $y(0) = y_0$  

1. Schritt:
$$ y^0 = y_0 $$  

2. Schritt:
$$ y^1 = y_0 + \int_0^t A y^0 \, ds = (I + t A) y_0 $$  

3. Schritt:
$$ y^2 = y_0 + \int_0^t A y^1 \, ds = y_0 + \int_0^t A (I + s A) y_0 \, ds $$
$$ = \left( I + t A + \frac{1}{2} t^2 A^2 \right) y_0 $$  

Allgemein:
$$ y^n = \left[ I + t A + \dots + \frac{1}{n!} (t A)^n \right] y_0 $$  

**Das motiviert** ...
## Das motiviert:

### **Definition 3.12**  
Sei $A \in \mathbb{R}^{d \times d}$ (oder $A \in \mathbb{C}^{d \times d}$).  Die matrixwertige Funktion  
$$
t \mapsto e^{tA} := \sum_{n=0}^{\infty} \frac{(tA)^n}{n!}
$$  
heißt **Matrixexponentialfunktion**.

**Übung:**  
Für jedes feste $t \in \mathbb{R}$ konvergiert die Reihe  
$$ \sum_{n=0}^{\infty} \frac{(tA)^n}{n!} $$  
absolut. Sie konvergiert gleichmäßig in $t \in [-r,r]$ für jedes feste $r > 0$.  

**Hinweis:**  
$$
\sum_{n=0}^{\infty} \frac{1}{n!} \| t^n A^n \| \leq \sum_{n=0}^{\infty} \frac{t^n}{n!} \| A^n \| \leq e^{\| A \|}, \quad t \in [-r,r], \quad \text{für festes } r > 0.
$$  

---

### Lemma 3.13
##### Voraussetzung:
Seien $A, B \in \mathbb{R}^{d \times d}$ (oder $\mathbb{C}^{d \times d}$) mit $AB = BA$.  

##### Behauptung:  
Dann gilt:  
$$
e^{A+B} = e^A e^B = e^B e^A
$$

---

### Satz 3.14
**Voraussetzung:**  
Sei $A \in \mathbb{R}^{d \times d}$ (oder $\mathbb{C}^{d \times d}$).  

**Behauptung:**  
Dann gilt:
1. $t \mapsto e^{tA}$ ist stetig auf $\mathbb{R}$.
2. $e^{(t+s)A} = e^{sA} e^{tA}, \quad t,s \in \mathbb{R}$.
3. $e^{tA}$ ist invertierbar für $t \in \mathbb{R}$ mit $(e^{tA})^{-1} = e^{-tA}$.
4. $t \mapsto e^{tA}$ ist stetig differenzierbar mit  $$\frac{d}{dt} e^{tA} = e^{tA} A = A e^{tA}$$
#### Beweis[[#Satz 3.14]]
##### ad(i):
$$
\| e^{tA} - e^{t'A} \| = \| \sum_{m=0}^{\infty} \frac{1}{m!} ((tA)^m - (t'A)^m) \|
$$
$$
\leq \sum_{m=1}^{\infty} \frac{1}{m!} |t^m - t'^m| \| A^m \|
$$
$$
\leq \sum_{m=1}^{\infty} \frac{1}{m!} |t^m - t'^m| \| A^m \|
$$
$$
= \sum_{m=1}^{\infty} \frac{1}{m!} \left( \int_{t'}^t m s^{m-1} ds \right) \| A^m \|
$$
$$
\sum_{m=1}^{\infty} \frac{1}{m!} |t - t'| \max_{s \in [t,t']} |s^{m-1}| \|A^m\|
$$
$$
\leq \|A\| |t - t'| e^S \|A\|
$$

##### ad(ii)
Stetigkeit  Folgt direkt aus Lemma 3.13.

##### ad(iii): Übung
##### ad(iv).
Eigenschaft $e^{(t+s)A} = e^{sA} e^{tA}$**  
Aus der Definition:
$$
e^{(t+s)A} - e^{tA} e^{sA} = e^{tA} (e^{sA} - I)
$$
$$
(e^{sA} - I) = \sum_{m=1}^{\infty} \frac{1}{m!} (sA)^m
$$
$$
= A \sum_{m=0}^{\infty} \frac{1}{(m+1)!} (sA)^m
$$
Daraus folgt die gewünschte Identität
$$
\lim_{h \to 0} \frac{1}{h} (e^{(t+h)A} - e^{tA})
$$
$$
= \lim_{h \to 0} \sum_{m=1}^{\infty} \frac{1}{m!} h (A^m)
$$
$$
= A e^{tA}
$$

---

**Alternative Darstellung der Matrixexponentialreihe:**
$$
\sum_{m=0}^{\infty} \frac{1}{m!} (tA)^m = I + \sum_{m=1}^{\infty} \frac{1}{m!} (tA)^m
$$
$$
\lim_{h \to 0} \sum_{m=1}^{\infty} \frac{1}{m!} h (A^m) \to A e^{tA}
$$
### Satz 3.15
##### Voraussetzung:  
Sei $A \in \mathbb{R}^{d \times d}$.  

##### Behauptung:
Dann gilt:  
1. $Y(t) = e^{tA}$ ist eine Fundamentalmatrix für $y' = A y$.  
2. Das Anfangswertproblem (AWP) $y' = A y, \quad y(0) = y_0$ wird gelöst von  
   $$ y(t) = e^{tA} y_0 $$
3. $e^{tA}$ ist Hauptfundamentalmatrix bezüglich $t_0 = 0$.  
4. $e^{(t-t_0)A}$ ist Hauptfundamentalmatrix bezüglich $t_0$.  

---

#### Beweis von ([[#Satz 3.15]])  

#### **Zu (i):**  
Da $e^{tA}$ eine Matrix ist, deren Spalten Lösungen der DGL $y' = A y$ sind, folgt, dass $Y(t) = e^{tA}$ eine Fundamentalmatrix ist.

#### **Zu (ii):**  
Aus der Definition von $e^{tA}$ folgt:
$$ Y(t) Y^{-1}(0) = e^{tA} e^{0A} = e^{tA} I = e^{tA} $$
Damit ist $y(t) = e^{tA} y_0$ die Lösung des AWP.

#### **Zu (iii):**  
Per Definition der Hauptfundamentalmatrix muss gelten:
$$ Y(0) = e^{0A} = I $$  
Dies ist erfüllt, also ist $e^{tA}$ die Hauptfundamentalmatrix für $t_0 = 0$.

#### **Zu (iv):**  
Die Matrix $e^{(t-t_0)A}$ ist ebenfalls eine Fundamentalmatrix.  
Für $t = t_0$ gilt:
$$ e^{(t_0 - t_0)A} = e^0 = I $$
Also ist $e^{(t - t_0)A}$ eine Hauptfundamentalmatrix bezüglich $t_0$.  
■
