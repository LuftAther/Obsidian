### Satz 3.15
##### Vorraussetzung:
Sei $A\in \mathbb{R}^{d\times d}$
##### Behauptung: 
Dann gilt: 
- *$Y(t) = e^{tA}$* Eine Fundamentalmatrix für $$y' = A y,\quad A \in \mathbb{R}^{n \times n}\tag{3.7}$$
- Das AWP $y' = Ay,\ y(0) = y_0$ wird gelöst von  
$$
y(t) = e^{tA} y_0
$$
- $e^{tA}$ ist Hauptfundamentalmatrix bezüglich $t_0 = 0$
- $e^{(t - t_0)A}$ ist Hauptfundamentalmatrix bezüglich $t_0$


---

### 3.2.1 Berechnung von $e^{tA}$

$e^{tA}$ wird typischerweise durch Diagonalisieren bestimmt.  Sei $A = V D V^{-1}$ Dann gilt:
$$
\begin{aligned}
e^{tA} &= \sum_{n=0}^{\infty} \frac{1}{n!} (tA)^n = \sum_{n=0}^{\infty} \frac{1}{n!} (t V D V^{-1})^n
= \sum_{n=0}^{\infty} \frac{1}{n!} t^n V D^n V^{-1} = V \left( \sum_{n=0}^{\infty} \frac{1}{n!} (t D)^n \right) V^{-1}
= Ve^{tD}V^{-1}
\end{aligned}
$$
Für $D = \text{diag}(d_1, \dotsc, d_n)$ gilt:

$$
e^{tD} = \sum_{n=0}^{\infty} \frac{1}{n!} t^n D^n = 
\begin{pmatrix}
e^{t d_1} & & \\
& \ddots & \\
& & e^{t d_n}
\end{pmatrix}
= \text{diag}(e^{t d_1}, \dotsc, e^{t d_n})
$$

Also:
$$e^{tA} = V \cdot \text{diag}(e^{t d_1}, \dotsc, e^{t d_n}) \cdot V^{-1} \tag{$\bigstar$}$$
#### Bemmerkung:
Formel $\bigstar$ kann auch als Lösung von AWP nach Basiswechsel erhalten werden:

$$
y' = A y,\quad y(0) = y_0
$$
$$
\Leftrightarrow y' = V D \underbrace{V^{-1}y}_{=:\tilde{y}},\quad \underbrace{V^{-1} y(0)}_{{\tilde{y}(0)}} = \underbrace{\tilde{y}_0}_{{=: \tilde{y_{o}}}}
$$
$$
\begin{split}
\Leftrightarrow:  \tilde{y} &= V^{-1} y  \tilde{y}(0) = \tilde{y}_0\\
\Leftrightarrow \tilde{y}' &= D \tilde{y},\quad \tilde{y}(0) = \tilde{y}_0\\
\Leftrightarrow \tilde{y}_i' &= d_i \tilde{y}_i,\quad \tilde{y}_i(0) =\tilde{y}_{0,i},\text{ mit } i = 1, \dotsc, d\\
\Leftrightarrow \tilde{y}_i(t) &= e^{d_i t} \tilde{y}_{0,i}, \text{ mit }i = 1, \dots, d
\end{split}$$
$$
\Leftrightarrow \tilde{y}(t) = 
\begin{pmatrix}
e^{d_1 t} & & \\
& \ddots & \\
& & e^{d_d t}
\end{pmatrix}
\tilde{y}_0
$$
$\Leftrightarrow V \tilde{y}(t) = V e^{tD} \tilde{y}_0$
$\Leftrightarrow y(t) = V e^{tD} V^{-1} y_0$

---

##### Frage: 
$e^{tA}$, falls $A$ nicht diagonalisierbar?
##### Antwort:  Jordanform
##### Erklärung:
Sei  
$$
B = \begin{pmatrix} B_1 & & \\ & \ddots & \\ & & B_m \end{pmatrix}
\quad \text{mit Jordanblöcken } B_i
$$

Dann:  
$$
e^{tA} = \begin{pmatrix} e^{tB_1} & & \\ & \ddots & \\ & & e^{tB_m} \end{pmatrix}
$$

Jordanform von $A$:  
$$
A = V J V^{-1}, \quad J = \begin{pmatrix} J_1 & & \\ & \ddots & \\ & & J_m \end{pmatrix}
$$

mit  
$$
J_i = \begin{pmatrix}
\lambda_i & 1 & & 0 \\
& \lambda_i & \ddots & \\
& & \ddots & 1 \\
0 & & & \lambda_i
\end{pmatrix}
$$

Daraus:  
$$
e^{tA} = V e^{tJ} V^{-1}
$$

$e^{tJ_i}$ ergibt sich nicht aus [[#Lemma 3.16]]
### Lemma 3.16

##### Voraussetzung:  
Sei  
$$
J = 
\begin{pmatrix}
\lambda & 1 & & \\
& \lambda & \ddots & \\
& & \ddots & 1 \\
& & & \lambda
\end{pmatrix}
\in \mathbb{C}^{r \times r}, \quad \lambda \in \mathbb{C}
$$

##### Behauptung:  
Dann gilt:

$$
e^{tJ} = \sum_{n=0}^\infty \frac{1}{n!} (tJ)^n 
= e^{\lambda t} \sum_{n=0}^\infty \frac{1}{n!} (tN)^n 
= e^{\lambda t}
\begin{pmatrix}
1 & t & \frac{t^2}{2!} & \cdots & \frac{t^{r-1}}{(r-1)!} \\
& 1 & t & \ddots & \vdots \\
& & \ddots & \ddots & \frac{t^2}{2!} \\
& & & 1 & t \\
& & & & 1
\end{pmatrix}
$$

wobei  

$$
N := 
\begin{pmatrix}
0 & 1 & & \\
& 0 & \ddots & \\
& & \ddots & 1 \\
& & & 0
\end{pmatrix}
$$

---

#### Beweis von [[#Lemma 3.16]]:

$$
J = \lambda I + N \quad \text{mit} \quad N = 
\begin{pmatrix}
0 & 1 & & \\
& 0 & \ddots & \\
& & \ddots & 1 \\
& & & 0
\end{pmatrix}
\in \mathbb{R}^{r \times r}
$$

---

##### Beobachtung:
Für alle $k \geq r$ gilt:

$$
N =
\begin{pmatrix}
0 & 1 & & \\
& \ddots & \ddots & \\
& & 0 & 1 \\
& & & 0
\end{pmatrix},
\quad
N^2 =
\begin{pmatrix}
0 & 0 & 1 & \\
& \ddots & \ddots & \ddots \\
& & 0 & 0 \\
& & & 0
\end{pmatrix},
\quad
N^{r-1} =
\begin{pmatrix}
0 & \cdots & 0 & 1 \\
& \ddots & \ddots & 0 \\
& & \ddots & \vdots \\
& & & 0
\end{pmatrix},
\quad
N^k = 0
$$

Also:$e^{tJ} = e^{t\lambda I + tN} = e^{t\lambda I} \cdot e^{tN} = e^{t\lambda} \cdot e^{tN}$
Da $I$ und $N$ kommutieren gilt: $e^{I + N} = e^I e^N$
Entwicklung:

$$
e^{tJ} = e^{t\lambda} \sum_{n=0}^{\infty} \frac{t^n}{n!} N^n
= e^{t\lambda} \sum_{n=0}^{r-1} \frac{t^n}{n!} N^n
$$

= angegebene Form

---

## 3.3 Inhomogene Systeme

- ##### Allgemeine Form:

$$
y' = A(t) y + b(t) \tag{$\tilde{\star}$}
$$

- ##### Beobachtung:  
Lösungsraum ist affin: Falls $y_p$ eine **Partikularlösung** ist, dann ist jede $\tilde{\star}$Lösung $y$ von der Form:  
$$
y = y_p + \tilde{y}
$$  
wobei $\tilde{y} \in \mathcal{L}$ ist.
> d.h $\tilde{y}'= A \tilde{y}$

- ##### Ziel:  
Finde eine Partikularlösung $y_p$

- ##### Technik:  
„**Methode der Variation der Konstanten**“ Sei $Y \in C^1(J, \mathbb{R}^{n \times n})$ Fundamentalmatrix für $y' = A(t)y$.

Ansatz für partikuläre Lösung $y_{p}$:
$$
\begin{split}
y_p(t) &= Y(t) c(t) \\
y_p'(t) &= A(t) Y(t) c(t) + Y(t) c'(t)\\
y_p'(t) &= A(t) Y(t) c(t) + Y(t) c'(t)
\end{split}
$$
$\Rightarrow c'(t) = Y(t)^{-1} b(t)$  
(Denn $Y$ ist Fundamentalmatrix, also invertierbar)
$\Rightarrow$ Für beliebiges $t_0 \in J$ kann man schreiben:

$$
c(t) = \int_{t_0}^t Y(s)^{-1} b(s)\,ds
$$

und somit:
$$
y_b(t) = Y(t) \int_{t_0}^t Y(s)^{-1} b(s)\,ds
$$

---

#### Satz 3.17

##### Voraussetzung: 
Keine
##### Behauptung:
(i) Für jedes $t_0 \in J$ ist  
$$
y_p(t) := Y(t) \int_{t_0}^t Y(s)^{-1} b(s)\, ds
$$  
eine Partikularlösung von  
$$
y' = A(t)y + b(t)
$$

(ii)  
$$
y(t) = Y(t) Y(t_0)^{-1} y_0 + Y(t) \int_{t_0}^t Y(s)^{-1} b(s)\, ds
$$  
ist die Lösung des Anfangswertproblems  
$$
y' = A(t)y + b(t), \quad y(t_0) = y_0
$$

---

#### Beweis von [[#Satz 3.17]]
Ohne Beweis (siehe Herleitung)
![[Kriese.png]]
#### Wichtiger Spezialfall:
$$A(t) \equiv A \in \mathbb{R}^{d \times d}, \quad Y(t) = e^{tA}$$

$$
y' = A y + b(t), \quad A \in \mathbb{R}^{n \times n}
$$

- ##### Lösung: 
$$y(t) = e^{tA} e^{-t_0 A} y_0 + e^{tA} \int_{t_0}^t e^{-sA} b(s) \, ds$$
- ##### auch schreibbar als:
$$
y(t) = e^{(t - t_0)A} y_0 + \int_{t_0}^t e^{(t - s)A} b(s)\, ds
$$

---

## 3.4 Lineare, skalare ODEs $n$-ter Ordnung

### Homogene Gleichung:

$$

y^{(n)} + \sum_{j=0}^{n-1} a_j(t) y^{(j)} = 0, \quad t \in J \tag{3.8}
$$

### Inhomogene Gleichung:

$$
y^{(n)} + \sum_{j=0}^{n-1} a_j(t) y^{(j)} = b(t), \quad t \in J \tag{3.9}
$$

---

 Umformung in ein System 1. Ordnung:
 Einführung neuer Variablen:

$$
\begin{split}
y_1 &= y \\
y_2 &= y' \\
y_3 &= y'' \\
\vdots \\
y_n &= y^{(n-1)}
\end{split}
$$

Dann wird Gleichung (3.9) zu einem System erster Ordnung:

(3.10)  
$$
\vec{y}' = A(t) \vec{y} + \left(\begin{array}{c} 0 \\ 0 \\  \vdots \\ 0 \\ b(t)\end{array}\right)
$$

mit:

$$
A(t) = \begin{pmatrix}
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1 \\
-a_0(t) & -a_1(t) & -a_2(t) & \cdots & -a_{n-1}(t)
\end{pmatrix}
\quad \text{(Begleitmatrix)}
$$
Wir können die Techniken und Resultate für Systeme verwenden. Wir haben:

- (i) **Superpositionsprinzip**:   
	Die Lösungen von (3.8) bilden einen Vektorraum,  die Lösungen von (3.9) einen affinen Raum.  Durch Vorgabe von  
$$
y(t_{0}),\quad y'(t_{0),\dots,}y^{n-1}(t_{0}) 
$$  
ist die Lösung von (3.8)/(3.9) eindeutig festgelegt.

- (ii) Jede Lösung von (3.9) hat die Form  
$$
y = y_p + y_h
$$  
mit partikulärer Lösung $y_p$ und einer Lösung $y_h$ von (3.8)

- (iii) Die Wronski-Determinante von $n$ Lösungen $y^1, \dotsc, y^n$ ist

$$
w(t) = \det
\begin{pmatrix}
y^1 & y^2 & \dots & y^n \\
(y^1)' & (y^2)' & \dots & (y^n)' \\
\vdots & \vdots & \ddots & \vdots \\
(y^1)^{(n-1)} & (y^2)^{(n-1)} & \dots & (y^n)^{(n-1)}
\end{pmatrix}
$$

Es gilt:

- **Satz von Liouville**:
  $$
  w(t) = w(t_0) \cdot \exp\left( -\int_{t_0}^t a_{n-1}(s)\, ds \right)
  $$

- Die Funktionen $y^1, \dotsc, y^n$ sind linear unabhängig auf $J \iff w(t) \neq 0$ für alle $t \in J$ 

- (iv) Man kann so auch Lösungen liefern für *Fundamentallösungen*

 - (v) Alternativer Reduktionsansatz:  Falls $y_1$ eine Lösung von (3.8) ist, dann liefert sie den Ansatz:  
$$y = \Phi(t) c(t)$$ sei eine ODE der Ordnung $n-1$ für $\Phi'$

- (vi) Variation der Konstanten für Partikulärlösung

Wir illustrieren das Vorgehen für $n = 2$:  
Seien $y^1, y^2 \in C^2(J; \mathbb{R})$ ein **Fundamentalsystem** für (3.8).  
Ansatz für die Partikulärlösung:

$$
y_p(t) = y^1(t) c_1(t) + y^2(t) c_2(t)
$$

Notationsfreundlicher formuliert: Betrachte den Vektoransatz

$$
Y(t) \cdot c(t) \quad \text{mit} \quad
Y(t) =
\begin{pmatrix}
y^1 & y^2 \\
(y^1)' & (y^2)'
\end{pmatrix}
$$

Einsetzen in (3.10) liefert:

$$
Y'(t)c(t) + Y(t)c'(t) \overset{!}{=} A(t)Y(t)c(t) +
\begin{pmatrix}
0 \\
b(t)
\end{pmatrix}
$$

Das heißt:

$$
Y(t) c'(t) \overset{!}{=} 
\begin{pmatrix}
0 \\
b(t)
\end{pmatrix} \tag{3.12}
$$

Da $y^1, y^2$ ein *Fundamentalsystem* sind, ist $W(t) = \det Y(t) \neq 0$.  
D.h:
$$
c'(t) = Y(t)^{-1}
\begin{pmatrix}
0 \\
b(t)
\end{pmatrix}
$$

$\Rightarrow$ Wähle 
$$
c(t) =
\begin{pmatrix}
c_1(t) \\
c_2(t)
\end{pmatrix}
$$
als Stammfunktion von
$$
\left(
\begin{pmatrix}
y^1 & y^2 \\
(y^1)' & (y^2)'
\end{pmatrix}
\right)^{-1}
\begin{pmatrix}
0 \\
b(t)
\end{pmatrix}
$$


### Beispiel:

$$
y'' + \frac{1}{t+1} y' = 1
$$

Ziel: Partikulärlösung

1. Schritt: Finde FS, d.h. löse homogene Gleichung  
$$
y'' + \frac{1}{t+1} y' = 0
$$

 -  Lösung: $y_1 \equiv 1$
- Lösung: Ansatz $z = y'$, löst $z' + \frac{z}{t+1} = 0$

>Separierbar!

$$
-\frac{dz}{z} = \frac{dt}{t+1} \Rightarrow \ln|z| = -\ln(t+1) + C \Rightarrow z = \frac{C}{t+1}
$$

Also:  
$$
\Rightarrow y^{2} = c\ln|1+t|+ c'
$$
Wähle $c = 1, c'=0$ d.h
$$
y_2 = \ln(1 + t)
$$

---

- ##### Behauptung $y_{1}, y_{2}$ sind linear unanbhängig:  
$$
y_1 = 1, \quad y_2 = \ln(1 + t)
$$  

###### Wronski-Determinante:

$$
w(t) = \det
\begin{pmatrix}
y_1 & y_2 \\
y_1' & y_2'
\end{pmatrix}
=
\det \begin{pmatrix}
1 & \ln(1 + t) \\
0 & \frac{1}{1 + t}
\end{pmatrix}
\not= 0
$$

2. Schritt: Ansatz für Partikulärlösung

Ansatz:  Einsetzen von $y(t) = Y(t) c(t)$ in (3.10) liefert mit (3.12):

$$
\begin{pmatrix}
1 & \ln(1 + t) \\
0 & \frac{1}{1 + t}
\end{pmatrix}
\begin{pmatrix}
c_1(t) \\
c_2(t)
\end{pmatrix}' 
\overset{!}{=} 
\begin{pmatrix}
0 \\
1
\end{pmatrix}
$$

Daraus folgen die Gleichungen: 
$$\begin{split}
c_1'(t) + c_2'(t) \ln(1 + t) &\overset{!}{=} 0  \Rightarrow \quad c_1' = -(1 + t) \ln(1 + t)\\
c_2'(t) \cdot \frac{1}{1 + t} &\overset{!}{=} 1 
\quad \Rightarrow \quad 
c_2' = 1 + t
\end{split}$$


Spezielle Lösungen durch Integration:

$$
c_2(t) = t + \frac{t^2}{2}
c_1(t) = -\frac{1}{2}(1 + t)^2 \left( \frac{1}{2} - \ln(1 + t) \right)
$$

$$
\Rightarrow y_p(t) = c_1(t) \cdot 1 + c_2(t) \cdot \ln(1 + t)
$$
Ausgerechnet:
$$
y_p(t) = -\frac{1}{2}(1 + t)^2 \left( \frac{1}{2} - \ln(1 + t) \right)
+ t \left(1 + \frac{t}{2}\right) \ln(1 + t)
$$
##### Die allgemeine Lösung ist:

$$
y(t) = y_p(t) + d_1 + d_2 \ln(1 + t), \quad \text{für } d_1, d_2 \in \mathbb{R}
$$
$$
c_1 = - \int (1 + t) \ln(1 + t)\, dt = -\frac{1}{2} (1 + t)^2 \left( \frac{1}{2} - \ln(1 + t) \right)
$$
---

#### Bemerkung:  
Für beliebige Funktionen $y^1, \dotsc, y^n \in C^n(J; \mathbb{R})$ kann aus dem **Verschwinden der Wronskischen Determinante NICHT** geschlossen werden, dass sie linear abhängig sind.

$$
W(t) = \det
\begin{pmatrix}
y^1 & \cdots & y^n \\
(y^1)' & \cdots & (y^n)' \\
\vdots & \ddots & \vdots \\
(y^1)^{(n-1)} & \cdots & (y^n)^{(n-1)}
\end{pmatrix}
= 0,
$$

dann folgt **nicht**, dass $y^1, \dotsc, y^n$ linear abhängig sind!

**Beispiel:**  
Betrachte $y^1(t) = t^2$, $y^2(t) = |t|t$ auf $J = \mathbb{R}$.  
Dann gilt:

- $y^1, y^2$ sind linear unabhängig auf $J$

- Weiters gilt:

$$
W(t) = \det
\begin{pmatrix}
y^1 & y^2 \\
(y^1)' & (y^2)'
\end{pmatrix}
= \det
\begin{pmatrix}
t^2 & |t|t \\
2t & 2t \cdot \text{sgn}(t)
\end{pmatrix}
$$


Berechnung:

$$
= 2t^3 \cdot \text{sgn}(t) - 2t^2 |t| = 2t^2 (t \cdot \text{sgn}(t) - |t|) = 2t^2 (|t| - |t|) = 0
$$

 bzw.: 
 Betrachte den Fall $n = 2$:

$$
y^1(t) = t^4, \quad y^2(t) = |t| \cdot t^3, \quad J \subseteq \mathbb{R}
$$

- Prüfe lineare Abhängigkeit:  
  Angenommen, $\alpha y^1 + \beta y^2 \equiiur möglich, wenn   
  $\Rightarrow y^1, y^2$ sind linear unabhängig

- Trotzdem gilt:

$$
W(t) = \det
\begin{pmatrix}
y^1 & y^2 \\
(y^1)' & (y^2)'
\end{pmatrix}
=
\det
\begin{pmatrix}
t^4 & |t| t^3 \\
4t^3 & 4t^3 \cdot \text{sgn}(t)
\end{pmatrix}
$$

Berechnung:

$$
= 4t^6 \cdot \text{sgn}(t) - 4t^6 \cdot |t| = 4t^6 (\text{sgn}(t) - |t|) = 0
$$

##### Fazit:  
Obwohl $W(t) = 0$, sind $y^1$ und $y^2$ **nicht** linear abhängig.  
Das zeigt, dass das Verschwinden der Wronski-Determinante **nicht** die lineare Abhängigkeit impliziert.


---

## 3.5 Gleichungen mit konstanten Koeffizienten

### Homogene Form:

$y^{(n)} + \sum_{j=0}^{n-1} a_j y^{(j)} = 0 \tag{3.13}$

### Inhomogene Form:

$y^{(n)} + \sum_{j=0}^{n-1} a_j y^{(j)} = b(t) \tag{3.14}$

mit $a_0, \dotsc, a_{n-1} \in \mathbb{R}$
### Beispiel:  
Der Ansatz  $y(t) = e^{\lambda t}$  in (3.13) liefert:$\frac{d^n}{dt^n} e^{\lambda t} + \sum_{j=0}^{n-1} a_j \frac{d^j}{dt^j} e^{\lambda t} = 0$

##### Ableitung: 
$$\lambda^n e^{\lambda t} + \sum_{j=0}^{n-1} a_j \lambda^j e^{\lambda $t} = 0$$

Faktorisieren:$\left( \lambda^n + \sum_{j=0}^{n-1} a_j \lambda^j \right) e^{\lambda t} = 0$

$\Rightarrow$ $e^{\lambda t}$ ist Lösung von (3.13)  $\iff$ $\lambda$ ist eine Nullstelle des charakteristischen Polynoms
##### Polynom:
$\chi(\lambda) := \lambda^n + \sum_{j=0}^{n-1} a_j \lambda^j$
