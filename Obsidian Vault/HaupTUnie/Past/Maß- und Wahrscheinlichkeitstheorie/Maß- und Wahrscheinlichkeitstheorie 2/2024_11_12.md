## 3.4 Mehr Beispiele zur Anwendung von Fubini's theorem:

### Aufgabe 3.1: 

Sei $X, Y$ zwei unabhängige zuvalsfariablen $\in \mathbb{R}$. Angenommen $X \sim Exp(\tau)$ und $Y \sim Exp(\gamma)$, wobei $\tau, \gamma >0$. Gib die verteilung von $Q = \frac{X}{Y}$ an.

## 3.4 Mehr über den Satz von Fubini in der Wahrscheinlichkeitstheorie.

Erinnern wir uns daran, dass, wenn $\nu_1$ und $\nu_2$ zwei Wahrscheinlichkeitsmaße auf $(\Omega_1, \mathcal{A}_1)$ und $(\Omega_2, \mathcal{A}_2)$ sind, und $\Omega = \Omega_1 \times \Omega_2$, $\mathcal{A} = \mathcal{A}_1 \otimes \mathcal{A}_2$, $\nu = \nu_1 \times \nu_2$, dann sind die Koordinatenabbildungen $X_i: (\omega_1, \omega_2) \mapsto \omega_i$, $i = 1, 2$, unabhängige Zufallsvariablen mit den Verteilungen $\nu_1$ und $\nu_2$ jeweils. Seien $X$ und $Y$ zwei unabhängige Zufallsvariablen mit den Verteilungen $\nu_X$ und $\nu_Y$ jeweils. Sei $Z = f(X, Y)$ und es gelte entweder, dass $f$ positiv ist oder dass $Z$ integrierbar ist. Dann gilt: $$ \mathbb{E}(Z) = \int \nu_X(dx) \left( \int \nu_Y(dy) f(x, y) \right) = \int \nu_Y(dy) \left( \int \nu_X(dx) f(x, y) \right). \tag{3.5} $$

Das bedeutet, dass wir zuerst den Erwartungswert von $f(X, y)$ bei festem $y$ nehmen, eine Funktion $g(y)$ als Ergebnis finden und dann den Erwartungswert von $g(Y)$ berechnen. Alternativ können wir auch zuerst den Erwartungswert bezüglich $X$ und dann bezüglich $Y$ berechnen. Das Ergebnis ist dasselbe. Oft lassen wir die Klammern weg. Wir haben den Satz von Fubini angewendet, da die Verteilung $\nu$ des Paares $(X, Y)$ das Produktmaß $\nu_X \times \nu_Y$ ist. Wie prüfen wir, ob $Z$ integrierbar ist? Nun, wir können $\mathbb{E}(|Z|)$ berechnen, indem wir Fubini anwenden, da $|Z|$ nicht-negativ ist: $$ \mathbb{E}(|Z|) = \int \nu_X(dx) \int \nu_Y(dy) |f(x, y)|. $$ Falls dieses Integral endlich ist, können wir $\mathbb{E}(Z)$ mithilfe von (3.5) berechnen.

---

### Aufgabe 3.1: 

Sei $X, Y$ zwei unabhängige zuvalsfariablen $\in \mathbb{R}$. Angenommen $X \sim Exp(\tau)$ und $Y \sim Exp(\gamma)$, wobei $\tau, \gamma >0$. Gib die verteilung von $Q = \frac{X}{Y}$ an.
#### Lösung([[#Aufgabe 3.1]]):

$\mathbb{P}(Q \leq z) = \mathbb{P}((X,Y) \in A)$ mit $A = \{(x,y): \frac{x}{y}\leq z\}$ wobei 
$A = \{(x, y): x / y \leq z\}$ . (Wir können den Satz von Fubini anwenden, da die Indikatorfunktion nicht-negativ ist.) Somit erhalten wir: 
$$ \mathbb{P}(Q \leq z) = \int_{(\mathbb{R}^+)^2} \mathbb{1}_A(x, y) \gamma e^{-\tau x} e^{-\gamma y} \, dx \, dy $$
$$  = \int_{\mathbb{R}^+} \left( \int_{\mathbb{R}^+} \gamma e^{-\gamma y} \mathbb{1}_{[x/z, \infty)}(y) \, dy \right) \tau e^{-\tau x} \, dx $$ Nun berechnen wir das innere Integral: $$ = \gamma^{\tau} \int_{0}^{\infty} \left( \int_{x/z}^{\infty} e^{-\gamma y} \, dy \right) e^{-\tau x} \, dx $$ Das innere Integral bezüglich \( y \) ergibt: $$ = \tau \int_{0}^{\infty} e^{-(x/z)} e^{-\tau x} \, dx $$ Dies vereinfacht sich weiter zu: $$ = \frac{\tau}{\tau + \gamma / z} $$

---
#### Bemerkung 3.19:

Ich schreibe weiterhin pedantisch $\int_{(a;b)} f(x) \, \lambda(dx)$ für das Lebesgue-Integral, um es vom Riemann-Integral $\int_a^b f(x) \, dx$ zu unterscheiden (in dem Sinne, dass die zweite Schreibweise nur für diejenigen Funktionen sinnvoll ist, die auch Riemann-integrierbar sind). Oft wird jedoch aus Einfachheitsgründen auch $\int_{(a;b)} f(x) \, dx$ verwendet, um das Lebesgue-Integral von $f$ eingeschränkt auf $(a, b)$ zu bezeichnen.

---

#### Aufgabe 3.2:

Seien $\nu_1$ und $\nu_2$ Lebesgue-Stieltjes-Maße auf $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$, bestimmt durch die Bedingung $$ \nu_1((a, b]) = F_1(b) - F_1(a), \quad \nu_2((a, b]) = F_2(b) - F_2(a), \quad a < b \in \mathbb{R}, $$ wobei $F_1$ und $F_2$ zwei rechtsseitig stetige, monoton wachsende Funktionen sind. Was ist das Produktmaß $\nu = \nu_1 \times \nu_2$? Wenn zusätzlich $\nu_1$ und $\nu_2$ absolut stetig bezüglich des Lebesgue-Maßes sind, wie kann man dann das Integral bezüglich $\nu$ einer integrierbaren Funktion $G$ schreiben? Per Definition ist $\nu$ ein Maß auf $\mathcal{B}(\mathbb{R}) \times \mathcal{B}(\mathbb{R}) = \mathcal{B}(\mathbb{R}^2)$, und es wird durch das Maß bestimmt, das es Rechtecken zuweist. Für $a_1 < b_1$ und $a_2 < b_2$ gilt $$ \nu((a_1, b_1] \times (a_2, b_2]) = (F_1(b_1) - F_1(a_1))(F_2(b_2) - F_2(a_2)). $$ Im Fall, dass $\nu_i$ eine Dichte bezüglich des Lebesgue-Maßes $\lambda_1$ haben (das heißt, wenn $F_i(b) - F_i(a) = \int_{(a, b)} f_i \, d\lambda_1$ für einige messbare, nicht-negative Funktionen $f_1$ und $f_2$), dann hat $\nu$ die Dichte $f_1 \times f_2$ bezüglich des zweidimensionalen Lebesgue-Maßes $\lambda_2$. Dann, wenn $G$ integrierbar (oder positiv) ist, können wir schreiben: $$ \int G \, d\nu = \int \lambda_1(dx) \int \lambda_1(dy) \, f_1(x) f_2(y) G(x, y)$$$$= \int \lambda_1(dy) \int \lambda_1(dx) \, f_1(x) f_2(y) G(x, y), $$ und nach dem Satz von Fubini ist es egal, in welcher Reihenfolge wir die Variablen integrieren.

#### Aufgabe 3.3.:

Seien $X$ und $Y$ zwei unabhängige Zufallsvariablen, die gleichverteilt auf $[0, 1]$ sind. Berechne den Erwartungswert von $Z = \max(X, Y)$. Wir haben $$ \mathbb{E}(Z) = \int_{[0,1]^2} \max(x, y) \, \nu(d(x, y)) $$ wobei $\nu$ die Gleichverteilung auf $[0, 1]$ ist. Da die Gleichverteilung bezüglich des Lebesgue-Maßes die Dichte 1 hat, ergibt dieses Integral: $$ \mathbb{E}(Z) = \int_{[0,1]^2} \max(x, y) \, dx \, dy$$$$
 = \int_0^1 \left( \int_0^1 \max(x, y) \, dx \right) \, dy = \int_0^1 \left( \int_0^y y \, dx + \int_y^1 y \, dx \right) \, dy. $$ Dies vereinfacht sich zu: $$ \mathbb{E}(Z) = \int_0^1 \left( y^2 + \frac{1 - y^2}{2} \right) \, dy = \frac{1}{2} + \frac{1}{6} = \frac{2}{3}. $$ 
#### Aufgabe 3.4:

Zeige, ohne eine Berechnung durchzuführen, sondern unter Verwendung des Ergebnisses der vorherigen Übung, dass $\mathbb{E}(\min(X, Y)) = \frac{1}{3}$, falls $X$ und $Y$ unabhängige, gleichverteilte Zufallsvariablen auf $[0, 1]$ sind. 

#### Aufgabe 3.5:

Viel Spaß beim Berechnen des Erwartungswertes des Maximums von zwei unabhängigen exponentiellen Zufallsvariablen.

---

#### Bemerkung 3.20. (Fubini für Wahrscheinlichkeitsmaße):

Seien $X$ und $Y$ zwei unabhängige Zufallsvariablen mit Verteilungen $\nu_X$ und $\nu_Y$ jeweils. Sei $Z = f(X, Y)$ und es gelte entweder, dass $f$ positiv ist oder dass $Z$ integrierbar ist. Dann gilt $$ \mathbb{E}(Z) = \int \nu_X(dx) \int \nu_Y(dy) f(x, y) = \int \nu_Y(dy) \int \nu_X(dx) f(x, y). \tag{3.5} $$ Das heißt, wir nehmen zuerst den Erwartungswert von $f(X, y)$ bei festem $y$, um eine Funktion $g(y)$ zu erhalten, und dann den Erwartungswert von $g(Y)$. Oder umgekehrt, zuerst den Erwartungswert bezüglich $X$ und dann bezüglich $Y$. Wie prüfen wir, ob $Z$ integrierbar ist? Nun, wir können $\mathbb{E}(|Z|)$ berechnen, indem wir Fubini anwenden, da $|Z|$ nicht-negativ ist: $$ \mathbb{E}(|Z|) = \int \nu_X(dx) \int \nu_Y(dy) |f(x, y)|. $$ Wenn sich herausstellt, dass dieses Integral endlich ist, dann können wir $\mathbb{E}(Z)$ mithilfe von (3.5) berechnen.

---

#### Beispiel 3.21:

Sei $X \sim \text{Unif}([0, 1])$ und $Y \sim \text{Exp}(1)$, wobei $X$ und $Y$ unabhängig sind. Definiere $Z = e^{tXY}$ für $t \in \mathbb{R}$. Berechne $\mathbb{E}(Z)$. 

#### Lösung([[#Beispiel 3.21]]):

Zunächst ist $(x, y) \mapsto e^{txy}$ messbar und positiv, daher können wir den Satz von Fubini anwenden. Wir haben unter Verwendung von (3.5) und indem wir zuerst nach $y$ und dann nach $x$ integrieren: $$ \mathbb{E}(Z) = \int_0^1 dx \int_0^\infty e^{-y} e^{txy} \, dy. $$ Das Integral bezüglich $y$ ist $+\infty$, wenn $xt > 1$, und beträgt $\frac{1}{1 - xt}$ andernfalls. Daher können wir bereits folgern, dass für $t > 1$ gilt $\mathbb{E}(Z) = \infty$. Für $t \leq 1$ haben wir $$ \mathbb{E}(Z) = \int_0^1 \frac{1}{1 - xt} \, dx = \frac{1}{t} \log \frac{1}{1 - t}. $$ Beachte, dass für $t = 1$ dies $\mathbb{E}(Z) = +\infty$ ergibt, während für $t = 0$ $Z = 1$ ist und daher $\mathbb{E}(Z) = 1$. Beachte auch, dass $\frac{1}{t} \log \frac{1}{1 - t} \to 1$ für $t \uparrow 0$.

---

#### Beispiel 3.22:

Sei $X \sim N(0, 1)$ und $Y$ eine Zufallsvariable, die mit gleicher Wahrscheinlichkeit $1/2$ die Werte $-1$ und $+1$ annimmt. Sei $X$ unabhängig von $Y$. Ist die Zufallsvariable $Z = XY$ integrierbar? Schauen wir nach. Da $Y$ nur die Werte $\pm 1$ annimmt, gilt $|Z| = |X| Y$, und somit $$ \mathbb{E}(|Z|) = \int \nu_X(dx) \int \nu_Y(dy) |x| y = \frac{1}{2} \int \nu_X(dx) \left( |x| + \frac{1}{|x|} \right) $$$$= \frac{1}{2} \left( \mathbb{E}(|X|) + \mathbb{E} \left( \frac{1}{|X|} \right) \right). $$ Erinnern wir uns an die Verteilung einer standardnormalverteilten Zufallsvariablen, so sehen wir, dass $$ \mathbb{E} \left( \frac{1}{|X|} \right) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} \frac{1}{|x|} e^{-\frac{1}{2} x^2} \, dx = \frac{2}{\sqrt{2 \pi}} \int_0^\infty e^{-\frac{1}{2} x^2} \frac{1}{x} \, dx = +\infty $$ (Aufgabe: Warum ist dieses Integral unendlich?). Schlussfolgerung: Nein, $Z$ ist nicht integrierbar. 

#### Aufgabe 3.6:

Seien $X$ und $Y$ unabhängige, exponentiell verteilte Zufallsvariablen mit Parametern $\lambda_1$ und $\lambda_2$. Berechne den Erwartungswert $\mathbb{E}(\min(X, Y))$. Berechne die Verteilung von $\min(X, Y)$. 
#### Lösung:
Das erwartete Ergebnis ist, dass der Erwartungswert $\frac{1}{\lambda_1 + \lambda_2}$ ist und die Verteilung exponentiell mit Parameter $\lambda_1 + \lambda_2$. 

#### Aufgabe 3.7:
Sei $\{V_i\}_{i \geq 1}$ eine iid-Folge integrierbarer Zufallsvariablen mit Erwartungswert $m$, und sei $S$ eine Zufallsvariable (unabhängig von den $V_i$), die Werte in $\mathbb{N}$ annimmt. Zeige, dass $Z = V_S$ eine integrierbare Zufallsvariable ist und dass $\mathbb{E}(Z) = m$ gilt. 

#### Aufgabe 3.8:
Seien $X$ und $Y$ unabhängige Exp(1)-Zufallsvariablen. Ist die Zufallsvariable $Z = \frac{1}{X + Y}$ integrierbar?

---
# 4 Bedingte Erwartung 
**„Bedingte Erwartung ist das wirklich nicht-triviale Element der Wahrscheinlichkeitstheorie. Alles andere ist entweder Analysis oder Mengenlehre.“** *(ein Kollege von mir an der TU Wien)* 
## 4.1 Einleitung:

Dies ist ein sehr wichtiges Kapitel für die Wahrscheinlichkeitstheorie. Zuerst definieren wir das Konzept der bedingten Wahrscheinlichkeit (was zur sogenannten Bayes-Formel führt, die sehr einfach, aber wesentlich in der Statistik ist) und anschließend das der bedingten Erwartung (zuerst bezüglich eines Ereignisses, dann bezüglich einer $\sigma$-Algebra). In diesem ganzen Kapitel arbeiten wir auf einem Wahrscheinlichkeitsraum $(\Omega, \mathcal{A}, P)$. Seien $A$ und $B$ zwei Ereignisse (also Elemente von $\mathcal{A}$). Wir möchten die Wahrscheinlichkeit von $A$, bedingt auf $B$, definieren, die als $P(A | B)$ bezeichnet wird. Die Idee ist, dass dies die Wahrscheinlichkeit sein sollte, dass $A$ eintritt, gegeben das Wissen, dass $B$ eintritt. Angenommen, wir werfen einen sechsseitigen Würfel; $A$ sei das Ereignis, dass das Ergebnis $>4$ ist, und $B$ sei das Ereignis, dass das Ergebnis gerade ist. Wir wissen, dass $\mathbb{P}(A) = 3/6 = 1/2$ (da es drei Ergebnisse gibt, die größer oder gleich 4 sind, von sechs möglichen Ergebnissen) und auch $\mathbb{P}(B) = 1/2$, weil es drei mögliche gerade Ergebnisse gibt. Wenn wir jedoch wissen, dass $B$ eintritt, d. h. das Ergebnis entweder 2, 4 oder 6 ist, dann gibt es zwei von drei dieser Ergebnisse, die größer oder gleich 4 sind. Wir möchten daher sagen, dass 

$$ \mathbb{P}(A | B) = \frac{2}{3} = \frac{\text{Anzahl der Ergebnisse, die gerade und größer oder gleich 4 sind}}{\text{Anzahl der geraden Ergebnisse}} $$$$= \frac{P(A \cap B)}{P(B)}. $$ Beachten Sie, dass diese Formel keinen Sinn ergeben würde, wenn $P(B)$ null wäre (wir hätten dann $0/0$). In ähnlicher Weise möchten wir die bedingte Erwartung definieren. Sei $X$ das Ergebnis des Würfelwurfs und $B$ das gleiche Ereignis wie oben. Was ist das erwartete Ergebnis $X$, bedingt auf das Wissen, dass $B$ eintritt? Wir werden dies als $\mathbb{E}(X | B)$ bezeichnen. Intuitiv sind die möglichen Ergebnisse von $X$, gegeben dass $B$ eintritt, $2$, $4$, $6$, also würden wir dies definieren als $(2 + 4 + 6)/3 = 4$, das heißt, $$ \mathbb{E}(X | B) = 4 = \frac{(2 + 4 + 6)/6}{3/6} = \frac{\mathbb{E}(X \mathbf{1}_B)}{P(B)}. $$ Schließlich wollen wir das Konzept der bedingten Erwartung in der folgenden Weise verallgemeinern. Angenommen, wir haben zwei Zufallsvariablen $X$ und $Y$, die nicht notwendigerweise unabhängig sind. Wir möchten etwas definieren wie *den erwarteten Wert von $X$, gegeben das Ergebnis von $Y$*, bezeichnet als $\mathbb{E}(X | Y)$. Hier sind zwei wichtige Anmerkungen zu machen: Erstens wird $\mathbb{E}(X | Y)$ keine Zahl sein (wie $\mathbb{E}(X | B)$), sondern selbst eine Zufallsvariable, da sie von der Realisierung von $Y$ abhängt. Zweitens, wenn $Y$ eine stetige Zufallsvariable ist (z. B. eine normalverteilte Zufallsvariable), können wir keine Ausdrücke wie $\mathbb{E}(X | Y = y)$ schreiben, da das Ereignis $\{Y = y\}$ eine Wahrscheinlichkeit von null hat und ein Ausdruck wie $$ \frac{\mathbb{E}(X \mathbf{1}_{\{Y = y\}})}{P(Y = y)} $$ nicht definiert ist, da es $0/0$ ergibt. Betrachten Sie beispielsweise eine gleichverteilte Zufallsvariable $X$ auf $[0, 1]$ und seien $\{Y_i\}_{i \geq 1}$ iid-Bernoulli-Zufallsvariablen mit Parameter $X$. Wir möchten die folgende sehr intuitive Aussage treffen: bedingt auf das Ereignis, dass $X = x$, dann gilt $P(Y_i = 1 \, \forall \, i \leq N) = x^N$. Das Problem ist, dass das Ereignis $X = x$ eine Wahrscheinlichkeit von null hat. Im Rest dieses Kapitels werden wir diese Definitionen und Überlegungen präzise formulieren.

---
## 4.2 Bedingte Wahrscheinlichkeit, Bayes' Formel :

### Definition 4.1:

Gegeben zwei Ereignisse $A$ und $B$ mit $P(B) > 0$, definieren wir $$ P(A | B) = \frac{P(A \cap B)}{P(B)}. \tag{4.1} $$ Wenn $P(B) = 0$, definieren wir $P(A | B)$ einfach nicht. **Beispiel 4.2.** Sei $X$ eine Exp($\lambda$)-Zufallsvariable mit $\lambda > 0$ und seien $s, t > 0$. Dann gilt $$ P(X > t + s | X > t) = \frac{P(X > t + s)}{P(X > t)} = \frac{e^{-\lambda (t + s)}}{e^{-\lambda t}} = e^{-\lambda s}. $$ Dies wird als *Gedächtnislosigkeitseigenschaft* bezeichnet. Wenn man sich vorstellt, dass $X$ die Wartezeit ist, bis etwas geschieht, bedeutet dies, dass bedingt darauf, dass man mindestens $t$ gewartet hat, die Wahrscheinlichkeit, dass man mindestens noch eine Zeitspanne $s$ warten muss, nicht davon abhängt, wie groß $t$ ist. Diese Eigenschaft gilt auch für die geometrische Zufallsvariable (prüfen!). 

Zum Beispiel: Wenn man wiederholt und unabhängig eine Münze wirft und $X$ der erste Zeitpunkt ist, an dem „Kopf“ erscheint, dann bedeutet dies, dass die Wahrscheinlichkeit, dass man $m$ Würfe warten muss, bedingt darauf, dass in den ersten $m$ Würfen kein „Kopf“ erschien, gleich der Wahrscheinlichkeit ist, dass man zu Beginn $m$ Würfe warten muss. Das heißt, selbst wenn in den ersten 1000 Münzwürfen kein „Kopf“ erschien, macht dies die Wartezeit auf einen „Kopf“ in den nächsten Würfen nicht kürzer.

#### Aufgabe 4.1:

Sei $X$ eine positive Zufallsvariable, deren Verteilung eine Dichte $\rho$ bezüglich des Lebesgue-Maßes hat. Zeige, dass, wenn $X$ die Gedächtnislosigkeitseigenschaft $P(X > t + s | X > t) = P(X > s)$ für alle $t, s > 0$ erfüllt, dann $X$ eine exponentielle Zufallsvariable ist. 

---

#### Bemerkung 4.3:

Die folgenden Beobachtungen sind elementar, aber wichtig: - Gegeben zwei Ereignisse $A$ und $B$ mit $P(A), P(B) > 0$, sind $A$ und $B$ unabhängig genau dann, wenn $P(A | B) = P(A)$ und in diesem Fall auch $P(B | A) = P(B)$. - Gegeben eine abzählbare disjunkte Zerlegung $(B_i)_{i \geq 1}$ von $\Omega$ in Ereignisse $B_i$ mit strikt positiver Wahrscheinlichkeit, gilt für jedes Ereignis $A$, $$ P(A) = \sum_i P(B_i) P(A | B_i). \tag{4.2} $$ (Dies ergibt sich einfach aus der $\sigma$-Additivität des Maßes.) Dies wird als *Gesamtwahrscheinlichkeitsformel* bezeichnet. - Gegeben ein Ereignis $B$ mit $P(B) > 0$, definiert die Abbildung $A \in \mathcal{A} \mapsto P(A | B)$ ein Wahrscheinlichkeitsmaß (das bedingte Wahrscheinlichkeitsmaß $P$ bedingt auf $B$). - Gegeben eine endliche oder abzählbare disjunkte Zerlegung von $\Omega$ in Ereignisse $(B_i)_{i \geq 1}$ mit strikt positiver Wahrscheinlichkeit und ein beliebiges Ereignis $A$ mit strikt positiver Wahrscheinlichkeit, gilt für jedes $k$, $$ P(B_k | A) = \frac{P(A | B_k) P(B_k)}{\sum_i P(A | B_i) P(B_i)}. \tag{4.3} $$ Der Beweis ist elementar: Der Zähler ist einfach $P(A \cap B_k)$ und der Nenner ist $P(A)$, unter Verwendung der Gesamtwahrscheinlichkeitsformel. Formula (4.3) is the famous "Bayes's formula". Despite the fact that it is almost a trivial
identity, its meaning is very important. Let us see an example.

#### Beispiel 4.4:

Österreich, als Bundesstaat, besteht aus $M = 9$ Regionen, nummeriert mit $i = 1, \dots, M$, jede mit einer Bevölkerung von $N_i$. Bei einer Wahl hat die Partei $P$ in jeder Region $i \leq M$ einen Stimmenanteil von $x_i$ erzielt. Wähle eine Person $X$ gleichmäßig zufällig in Österreich. Bedingt auf das Ereignis, dass die Person für die Partei $P$ gestimmt hat, was ist die Wahrscheinlichkeit, dass die Person aus der Region $i$ kommt? Das heißt, was ist die Wahrscheinlichkeit, dass ein zufällig gewählter Wähler der Partei $P$ aus der Region $i$ stammt? $X$ ist eine gleichmäßig verteilte Zufallsvariable über die Gesamtbevölkerung, also aus der Menge von $N = \sum_{i=1}^M N_i$ Personen. Die *a priori* Wahrscheinlichkeit, dass $X$ aus der Region $i$ stammt, beträgt $\frac{N_i}{N}$. Bezeichnen wir mit $B_i$ das Ereignis, dass $X$ aus der Region $i$ stammt. Die Mengen $B_i$ sind offensichtlich disjunkt und bilden eine Zerlegung des Stichprobenraums. Wir wollen also $P(B_i | A)$ bestimmen, wobei $A$ das Ereignis ist, dass $X$ für die Partei $P$ gestimmt hat. Nach der Bayes-Formel gilt: $$ P(B_i | A) = \frac{P(A | B_i) P(B_i)}{\sum_{j=1}^M P(A | B_j) P(B_j)} = \frac{x_i \frac{N_i}{N}}{\sum_{j=1}^M x_j \frac{N_j}{N}} = \frac{x_i N_i}{\sum_{j=1}^M x_j N_j}. $$ Zum Beispiel: Angenommen, eine der 9 Regionen (sagen wir Vorarlberg) ist viel kleiner als die anderen, mit $N_{\text{Vorarlberg}} / N = 0.04$, aber die Partei $P$ erhält $x_{\text{Vorarlberg}} = 50\%$ der Stimmen, während die Partei insgesamt in Österreich nur 10\% der Stimmen erhält. Die Wahrscheinlichkeit, dass eine zufällig ausgewählte Person, die für $P$ gestimmt hat, aus Vorarlberg stammt, ist dann $0.5 \cdot 0.04 / 0.1 = 0.2$, also 20\%. Das ist viel höher als die Wahrscheinlichkeit, dass eine zufällig ausgewählte Person aus Österreich aus Vorarlberg stammt (das sind 4\%). 

---
## 4.3 Bedingte Erwartung, Teil I 

### Definition 4.5:

Sei $X$ eine integrierbare Zufallsvariable ($X \in L^1(P)$) und $A$ ein Ereignis positiver Wahrscheinlichkeit. Dann ist die bedingte Erwartung von $X$ gegeben $A$ definiert als $$ \mathbb{E}(X | A) = \frac{\mathbb{E}(X \mathbf{1}_A)}{P(A)}. \tag{4.4} $$ Beachte, dass $|\mathbb{E}(X | A)| \leq \frac{\mathbb{E}(|X|)}{P(A)} < \infty$. Zur Vorbereitung des nächsten Abschnitts nehmen wir an, dass der Stichprobenraum eine disjunkte, abzählbare Zerlegung $\Omega = \bigcup_{i \geq 1} B_i$ in Ereignisse positiver Wahrscheinlichkeit zulässt. Bezeichnen wir mit $\mathcal{F} = \sigma((B_i)_{i \geq 1})$ die $\sigma$-Algebra, die durch diese Ereignisse erzeugt wird. Definieren wir die folgende Zufallsvariable, die wir mit $\mathbb{E}(X | \mathcal{F})$ bezeichnen: $$ \mathbb{E}(X | \mathcal{F})(\omega) = \mathbb{E}(X | B_i) \quad \text{falls } \omega \in B_i. \tag{4.5} $$

---
#### Einige wichtige Anmerkungen:

- Da $\Omega = \bigcup_{i \geq 1} B_i$, definiert diese Formel $\mathbb{E}(X | \mathcal{F})$ überall. 

- Diese Zufallsvariable ist $(\Omega, \mathcal{F})$-$ (\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar, da sie per Definition auf jedem $B_i$ konstant ist.

- $\mathbb{E}(X | \mathcal{F})$ ist nicht nur messbar, sondern auch integrierbar und ihr Integral entspricht $\mathbb{E}(X)$. In der Tat gilt $|\mathbb{E}(X | \mathcal{F})| \leq \mathbb{E}(|X| | \mathcal{F})$ (dies folgt unmittelbar aus der Definition), und $$ \mathbb{E} (\mathbb{E}(|X| | \mathcal{F})) = \int \mathbb{E}(|X| | \mathcal{F}) \, dP$$ $$= \sum_{i \geq 1} P(B_i) \mathbb{E}(|X| | B_i) = \sum_{i \geq 1} \mathbb{E}(|X| \mathbf{1}_{B_i}) = \mathbb{E}(|X|) < \infty. $$ Ähnlich zeigt man, dass $\mathbb{E} (\mathbb{E}(X | \mathcal{F})) = \mathbb{E}(X)$ (ohne Betrag).

---
## 4.4 Bedingte Erwartung II:

Sei wie üblich $(\Omega, \mathcal{A}, P)$ unser Wahrscheinlichkeitsraum, und sei $\mathcal{F} \subset \mathcal{A}$ eine $\sigma$-Algebra (typischerweise ist $\mathcal{F}$ streng kleiner als $\mathcal{A}$). Sei außerdem $X$ eine $L^1(\Omega, \mathcal{A}, P)$-Zufallsvariable, wobei die Notation betont, dass $X$ $(\Omega, \mathcal{A})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar ist, jedoch nicht notwendig $(\Omega, \mathcal{F})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar sein muss. 

### Definition 4.6.(Bedingte Erwartung bezüglich einer $\sigma$-Algebra):

Eine Zufallsvariable $Y$ heißt bedingte Erwartung von $X$ bezüglich $\mathcal{F}$ (und wir schreiben $Y = \mathbb{E}(X | \mathcal{F})$), wenn die folgenden zwei Bedingungen erfüllt sind: 

- $Y$ ist $(\Omega, \mathcal{F})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar. 

- Für jedes $A \in \mathcal{F}$ gilt $\mathbb{E}(X \mathbf{1}_A) = \int_A X \, dP = \mathbb{E}(Y \mathbf{1}_A)$. Falls $B \in \mathcal{A}$ und $f = \mathbf{1}_B$, nennen wir $\mathbb{E}(f | \mathcal{F})$ die bedingte Wahrscheinlichkeit von $B$ bezüglich $\mathcal{F}$ und schreiben $\mathbb{E}(B | \mathcal{F})$. 
---
#### Beispiel 4.7. (Sehr elementare Beispiele):

Sei $X$ eine integrierbare Zufallsvariable und $Y$ eine von $X$ unabhängige Zufallsvariable. Dann gilt: $$ \mathbb{E}(X | X) = X, \quad \mathbb{E}(X | Y) = \mathbb{E}(X). \tag{4.6} $$ Lassen Sie uns überprüfen, ob dies die Definition erfüllt. Ist $X$ messbar bezüglich $\sigma(X)$? Natürlich. Gilt außerdem $\mathbb{E}(X \mathbf{1}_A) = \mathbb{E}(X \mathbf{1}_A)$ für jedes $A \in \sigma(X)$? Ja, das stimmt offensichtlich. Im zweiten Fall: Offensichtlich ist $\mathbb{E}(X)$, da es konstant ist, bezüglich jeder $\sigma$-Algebra messbar. Auch gilt für jedes $A \in \sigma(Y)$, dass $\mathbb{E}(X \mathbf{1}_{A(Y)}) = \mathbb{E}(X) P(Y \in A) = \mathbb{E}(\mathbf{1}_A(Y) \mathbb{E}(X))$, wobei in der ersten Gleichheit die Unabhängigkeit von $X$ und $Y$ verwendet wurde. Beide Eigenschaften der bedingten Erwartung sind also erfüllt. 

### Beispiel 4.8:

Sei $\Omega = \{1, \dots, 6\}^2$ mit $\mathcal{A} = 2^\Omega$ und $P$ das Gleichmaß auf $\Omega$. Sei $\mathcal{F}$ die $\sigma$-Algebra, die durch die Mengen $\{i\} \times \{1, \dots, 6\}$ erzeugt wird. (Man denke an einen Stichprobenraum, der das Ergebnis des Wurfs zweier Würfel darstellt, und an die $\sigma$-Algebra $\mathcal{F}$, die Mengen enthält, die nur vom Ergebnis des ersten Würfels abhängen.) 

Bezeichnen wir das generische Element von $\Omega$ mit $\omega = (\omega_1, \omega_2)$ und betrachten die Zufallsvariable $X: (\omega_1, \omega_2) \mapsto \omega_1 + \omega_2$ (die Summe der Ergebnisse der beiden Würfel). Diese ist nicht $\mathcal{F}$-messbar. Definieren wir $Y$ als $$ Y(\omega_1, \omega_2) = \frac{1}{6} \sum_{i=1}^6 X(\omega_1, i). $$ Offensichtlich ist $Y$ $\mathcal{F}$-messbar. Für jedes $A \in \mathcal{F}$, also ein Ereignis der Form $B \times \{1, \dots, 6\}$ mit $B \subset \{1, \dots, 6\}$, haben wir $$ \mathbb{E}(X \mathbf{1}_A) = \frac{1}{36} \sum_{i, j = 1}^6 X(i, j) \mathbf{1}_{i \in B} \quad \text{und} \quad \mathbb{E}(Y \mathbf{1}_A) = \frac{1}{6} \sum_{i \in B} \frac{1}{6} \sum_{j=1}^6 X(i, j), $$ und die beiden Ausdrücke stimmen überein. Wir sehen daher, dass $Y$ eine bedingte Erwartung von $X$ bezüglich $\mathcal{F}$ ist. Was ist die Logik bei der Definition von $Y(\omega_1, \omega_2) = \frac{1}{6} \sum_{i=1}^6 X(\omega_1, i)$? Die Idee ist, dass wir über den Wert von $\omega_2$ gemittelt haben (da die $\sigma$-Algebra $\mathcal{F}$ darüber keine Information enthält), während wir $\omega_1$ unverändert beibehalten haben. 

### Beispiel 4.9:

Dieses Beispiel ist dem vorherigen sehr ähnlich, nur dass wir diskrete Gleichverteilungen durch stetige ersetzen. Sei $\Omega = (0, 1)^2$ mit der $\sigma$-Algebra $\mathcal{B}((0, 1)^2)$ und dem zweidimensionalen Lebesgue-Maß (das ein Wahrscheinlichkeitsmaß ist, da $(0, 1)^2$ eine Fläche von 1 hat). Definiere $\mathcal{F}$ als die $\sigma$-Algebra, die durch die Mengen $(a, b) \times (0, 1)$ erzeugt wird. Beachten Sie, dass diese viel kleiner ist als $\mathcal{A}$, da sie keine Rechtecke der Form $(a, b) \times (c, d)$ enthält, außer wenn $(c, d)$ entweder leer oder $(0, 1)$ ist. Die Zufallsvariable $X: (x, y) \mapsto x + y$ ist $(\Omega, \mathcal{A})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar, jedoch nicht $(\Omega, \mathcal{F})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar, da sie auch von der $y$-Koordinate abhängt. Definieren wir die Zufallsvariable $Y$ als $$ Y(x, y) = x + \frac{1}{2}. $$ Offensichtlich ist $Y$ $\mathcal{F}$-messbar, da sie nur von $x$ abhängt. Für jedes $A \in \mathcal{F}$, das notwendigerweise die Form $B \times (0, 1)$ für eine Borel-Menge $B \subset (0, 1)$ hat, gilt
 $\int_A X \, dP = \int_B \lambda(dx) \int_{(0,1)} X(x, y) \, \lambda(dy)$ 
 $= \int_B \lambda(dx) \int_0^1 (x + y) \, dy = \int_B \left( x + \frac{1}{2} \right) \lambda(dx)$
$= \int_A Y \, dP,$ 

sodass $Y$ beide Eigenschaften der bedingten Erwartung von $X$ bezüglich $\mathcal{F}$ erfüllt. Die Logik dahinter ist wieder, dass wir $X$ bezüglich der $y$-Koordinate mitteln, da die $\sigma$-Algebra $\mathcal{F}$ darüber keine Informationen enthält. Das nächste grundlegende Resultat besagt, dass die bedingte Erwartung stets existiert und in einem gewissen Sinne eindeutig ist. 

### Satz 4.10:

Wenn $X$ integrierbar ist, dann existiert die bedingte Erwartung $\mathbb{E}(X | \mathcal{F})$ und ist bis auf eine Nullmengen-Eindeutigkeit eindeutig bestimmt, im Sinne von: Wenn zwei bedingte Erwartungen $Y$ und $Y'$ existieren, dann gilt $P(Y = Y') = 1$. 
#### Beweis([[2024_11_12#Satz 4.10]]): 

*(Eindeutigkeit)* Sei $Y$ und $Y'$ Zufallsvariablen, die die Definition der bedingten Erwartung von $X$ bezüglich $\mathcal{F}$ erfüllen. Sei $A = \{\omega \in \Omega : Y(\omega) > Y'(\omega)\}$, das ist ein Element von $\mathcal{F}$, da sowohl $Y$ als auch $Y'$ $\mathcal{F}$-messbar sind. Dann gilt $$ 0 = \mathbb{E}(Y \mathbf{1}_A) - \mathbb{E}(Y' \mathbf{1}_A) = \int_A (Y - Y') \, dP, $$ und da $Y > Y'$ auf $A$, impliziert dies, dass $A$ Wahrscheinlichkeit null hat. Ebenso hat die Menge, auf der $Y < Y'$, Wahrscheinlichkeit null. 

*(Existenz)* Zerlege $X = X^+ - X^-$ in die Differenz seines positiven und seines negativen Teils. Für jedes $A \in \mathcal{F}$ definiere $$ Q^\pm(A) = \mathbb{E}(X^\pm \mathbf{1}_A). \tag{4.7} $$ Da $X$ integrierbar ist, ist der Erwartungswert endlich, sodass die Mengenfunktion $Q^\pm$ $\mathcal{F}$ auf $[0, \mathbb{E}(X^\pm)]$ abbildet und insbesondere endliche Maße auf $(\Omega, \mathcal{F})$ sind. Wenn $P(A) = 0$, dann gilt offensichtlich $Q^\pm(A) = 0$, d.h., $Q^\pm$ sind absolut stetig bezüglich $P$. Nach dem Satz von Radon-Nikodym existiert eine Dichte, also eine positive, $\mathcal{F}$-messbare Funktion $Y^\pm$ mit $$ Q^\pm(A) = \int_A Y^\pm \, dP = \mathbb{E}(Y^\pm \mathbf{1}_A). \tag{4.8} $$ Dann ist $Y = Y^+ - Y^-$ eine bedingte Erwartung von $X$ bezüglich $\mathcal{F}$
$\square$ .

---
## Definition 4.11:

Gegeben eine $L^1$-Zufallsvariable $X$ und eine Zufallsvariable $Y$, bezeichnen wir mit $\mathbb{E}(X | Y)$ die bedingte Erwartung $\mathbb{E}(X | \sigma(Y))$, wobei $\sigma(Y)$ die von $Y$ erzeugte $\sigma$-Algebra ist. ### 4.4.1 Die intuitive Bedeutung Die Definition von $\mathbb{E}(X | \mathcal{F})$ ist möglicherweise nicht sehr anschaulich, und der Existenzbeweis (Satz 4.10) trägt auch nicht wesentlich zur Intuition bei. Der folgende Satz (den ich ohne Beweis angebe; siehe [Klenke], Seite 97, wenn Sie interessiert sind) und die Bemerkung direkt danach verdeutlichen besser die Bedeutung von $\mathbb{E}(X | \mathcal{F})$, zumindest im Fall, dass $X$ nicht nur integrierbar, sondern quadratisch integrierbar ist: 

### Satz 4.12(Bedingte Erwartung als orthogonale Projektion):

Sei $\mathcal{F} \subset \mathcal{A}$ eine $\sigma$-Algebra und $X$ eine $L^2(\Omega, \mathcal{A}, P)$-Zufallsvariable. Jede $\mathcal{F}$-messbare und quadratisch integrierbare Zufallsvariable $Y$ erfüllt $$ \mathbb{E}[(X - Y)^2] \geq \mathbb{E}[(X - \mathbb{E}(X | \mathcal{F}))^2]. \tag{4.9} $$
und Gleichheit gilt nur, wenn $Y = \mathbb{E}(X | \mathcal{F})$ (fast sicher). 

---

#### Bemerkung 4.13:

Um die Bedeutung zu verstehen, denken Sie an einen euklidischen Raum (zum Beispiel $\mathbb{R}^3$). Sei $P$ eine Ebene, die den Ursprung enthält, und $x$ ein Punkt, der nicht in $P$ liegt. Dann existiert ein eindeutiger Punkt $y^*$ in $P$, der die minimale Entfernung zu $x$ hat, und $y^*$ ist einfach die orthogonale Projektion von $x$ auf $P$. Für jedes $y \in P$, $y \neq y^*$, gilt $\|y - x\| > \|y^* - x\|$. Erinnern wir uns daran, dass $L^2(P)$ ein Vektorraum mit einem Skalarprodukt $\langle f, g \rangle = \mathbb{E}(f g)$ ist. Dieses Skalarprodukt induziert eine Distanz $\|f - g\|_{L^2} = \sqrt{\langle f - g, f - g \rangle}$. Außerdem ist die Menge $V_{\mathcal{F}}$ aller quadratisch integrierbaren Zufallsvariablen $Y$, die $\mathcal{F}$-messbar sind, ein linearer Unterraum von $L^2(P)$. Was Satz 4.12 also aussagt, ist, dass $\mathbb{E}(X | \mathcal{F})$ die orthogonale Projektion von $X$ auf $V_{\mathcal{F}}$ ist. Eine andere Sichtweise auf Satz 4.12 ist die folgende: Unter allen Zufallsvariablen, die $\mathcal{F}$-messbar sind, ist $\mathbb{E}(X | \mathcal{F})$ diejenige, die $X$ am nächsten kommt. Angenommen, wir sind an einer zufälligen experimentellen Größe $X$ interessiert, haben jedoch nur eingeschränkten Zugang dazu, das heißt, wir können nur „messen“, ob Ereignisse in $\mathcal{F}$ auftreten. Dann ist $\mathbb{E}(X | \mathcal{F})$ die beste Schätzung für $X$ (im Sinne des mittleren quadratischen Fehlers), basierend auf der Beobachtung des Auftretens von Ereignissen in $\mathcal{F}$. 

#### Beispiel 4.14:

Wir sind an einer Zufallsvariable $Z$ interessiert, die die Summe zweier unabhängiger Zufallsvariablen $X$ und $Y$ ist, die beide gleichverteilt auf $[0, 1]$ sind. Wenn wir den Wert von $Y$ beobachten und den Wert $y$ finden, dann ist unsere beste Schätzung für $Z$ $Z = \mathbb{E}(X) + y$. Tatsächlich ist der durchschnittliche quadratische Fehler, den wir bei dieser Schätzung machen, $\mathbb{E}((X + y - (1/2 + y))^2) = \text{Var}(X)$, und wenn wir eine andere Schätzung vornehmen würden, sagen wir $f(y)$, dann wäre der mittlere quadratische Fehler $\mathbb{E}((X + y - f(y))^2) > \text{Var}(X)$, weil wir wissen, dass $\mathbb{E}((X - a)^2)$ minimal ist, wenn $a = \mathbb{E}(X)$. 

---

### 4.4.2 Eigenschaften der bedingten Erwartung 

### 4.4.3 Randverteilung, bedingte Verteilung, bedingte Erwartung

Gegeben einen Zufallsvektor $(X_1, \dots, X_n)$ mit einer Verteilung $\nu$, nennt man die Verteilung der einzelnen Zufallsvariable $X_i$ die *$i$-te Randverteilung*. Falls $\nu$ eine Dichte $f$ bezüglich des $n$-dimensionalen Lebesgue-Maßes $\lambda_n$ besitzt, dann hat die Randverteilung von $X_i$ die Dichte $$ f_i(x) = \int_{\mathbb{R}^{n-1}} f(y_1, \dots, y_{i-1}, x, y_{i+1}, \dots, y_n) \, \lambda_{n-1}(dy). \tag{4.10} $$ Da $f$ nicht-negativ und integrierbar ist, impliziert der Satz von Fubini, dass $f_i(x)$ für $\lambda$-fast jedes $x$ endlich ist. Für $n = 2$ ergibt sich $$ f_1(x) = \int_{\mathbb{R}} f(x, y) \, \lambda(dy), \quad f_2(x) = \int_{\mathbb{R}} f(y, x) \, \lambda(dx). \tag{4.11} $$ Angenommen, $X_1$ ist integrierbar und wir möchten $Y = \mathbb{E}(X_1 | X_2)$ bestimmen. Definiere $$ Y = \int_{\mathbb{R}} x \frac{f(x, X_2)}{f_2(X_2)} \, \lambda(dx). \tag{4.12} $$ Erinnern wir uns, dass $f_2(x)$
