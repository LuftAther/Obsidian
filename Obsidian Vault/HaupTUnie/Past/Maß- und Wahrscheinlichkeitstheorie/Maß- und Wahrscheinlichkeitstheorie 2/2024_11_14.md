# 4 Bedingte Erwartung 
**„Bedingte Erwartung ist das wirklich nicht-triviale Element der Wahrscheinlichkeitstheorie. Alles andere ist entweder Analysis oder Mengenlehre.“** *(ein Kollege von mir an der TU Wien)* 
## 4.1 Einleitung:

Dies ist ein sehr wichtiges Kapitel für die Wahrscheinlichkeitstheorie. Zuerst definieren wir das Konzept der bedingten Wahrscheinlichkeit (was zur sogenannten Bayes-Formel führt, die sehr einfach, aber wesentlich in der Statistik ist) und anschließend das der bedingten Erwartung (zuerst bezüglich eines Ereignisses, dann bezüglich einer $\sigma$-Algebra). In diesem ganzen Kapitel arbeiten wir auf einem Wahrscheinlichkeitsraum $(\Omega, \mathcal{A}, P)$. Seien $A$ und $B$ zwei Ereignisse (also Elemente von $\mathcal{A}$). Wir möchten die Wahrscheinlichkeit von $A$, bedingt auf $B$, definieren, die als $P(A | B)$ bezeichnet wird. Die Idee ist, dass dies die Wahrscheinlichkeit sein sollte, dass $A$ eintritt, gegeben das Wissen, dass $B$ eintritt. Angenommen, wir werfen einen sechsseitigen Würfel; $A$ sei das Ereignis, dass das Ergebnis $>4$ ist, und $B$ sei das Ereignis, dass das Ergebnis gerade ist. Wir wissen, dass $\mathbb{P}(A) = 3/6 = 1/2$ (da es drei Ergebnisse gibt, die größer oder gleich 4 sind, von sechs möglichen Ergebnissen) und auch $\mathbb{P}(B) = 1/2$, weil es drei mögliche gerade Ergebnisse gibt. Wenn wir jedoch wissen, dass $B$ eintritt, d. h. das Ergebnis entweder 2, 4 oder 6 ist, dann gibt es zwei von drei dieser Ergebnisse, die größer oder gleich 4 sind. Wir möchten daher sagen, dass 

$$ \mathbb{P}(A | B) = \frac{2}{3} = \frac{\text{Anzahl der Ergebnisse, die gerade und größer oder gleich 4 sind}}{\text{Anzahl der geraden Ergebnisse}} $$$$= \frac{P(A \cap B)}{P(B)}. $$ Beachten Sie, dass diese Formel keinen Sinn ergeben würde, wenn $P(B)$ null wäre (wir hätten dann $0/0$). In ähnlicher Weise möchten wir die bedingte Erwartung definieren. Sei $X$ das Ergebnis des Würfelwurfs und $B$ das gleiche Ereignis wie oben. Was ist das erwartete Ergebnis $X$, bedingt auf das Wissen, dass $B$ eintritt? Wir werden dies als $\mathbb{E}(X | B)$ bezeichnen. Intuitiv sind die möglichen Ergebnisse von $X$, gegeben dass $B$ eintritt, $2$, $4$, $6$, also würden wir dies definieren als $(2 + 4 + 6)/3 = 4$, das heißt, $$ \mathbb{E}(X | B) = 4 = \frac{(2 + 4 + 6)/6}{3/6} = \frac{\mathbb{E}(X \mathbf{1}_B)}{P(B)}. $$ Schließlich wollen wir das Konzept der bedingten Erwartung in der folgenden Weise verallgemeinern. Angenommen, wir haben zwei Zufallsvariablen $X$ und $Y$, die nicht notwendigerweise unabhängig sind. Wir möchten etwas definieren wie *den erwarteten Wert von $X$, gegeben das Ergebnis von $Y$*, bezeichnet als $\mathbb{E}(X | Y)$. Hier sind zwei wichtige Anmerkungen zu machen: Erstens wird $\mathbb{E}(X | Y)$ keine Zahl sein (wie $\mathbb{E}(X | B)$), sondern selbst eine Zufallsvariable, da sie von der Realisierung von $Y$ abhängt. Zweitens, wenn $Y$ eine stetige Zufallsvariable ist (z. B. eine normalverteilte Zufallsvariable), können wir keine Ausdrücke wie $\mathbb{E}(X | Y = y)$ schreiben, da das Ereignis $\{Y = y\}$ eine Wahrscheinlichkeit von null hat und ein Ausdruck wie $$ \frac{\mathbb{E}(X \mathbf{1}_{\{Y = y\}})}{P(Y = y)} $$ nicht definiert ist, da es $0/0$ ergibt. Betrachten Sie beispielsweise eine gleichverteilte Zufallsvariable $X$ auf $[0, 1]$ und seien $\{Y_i\}_{i \geq 1}$ iid-Bernoulli-Zufallsvariablen mit Parameter $X$. Wir möchten die folgende sehr intuitive Aussage treffen: bedingt auf das Ereignis, dass $X = x$, dann gilt $P(Y_i = 1 \, \forall \, i \leq N) = x^N$. Das Problem ist, dass das Ereignis $X = x$ eine Wahrscheinlichkeit von null hat. Im Rest dieses Kapitels werden wir diese Definitionen und Überlegungen präzise formulieren.

---
## 4.2 Bedingte Wahrscheinlichkeit, Bayes' Formel :

### Definition 4.1:

Gegeben zwei Ereignisse $A$ und $B$ mit $P(B) > 0$, definieren wir $$ P(A | B) = \frac{P(A \cap B)}{P(B)}. \tag{4.1} $$ Wenn $P(B) = 0$, definieren wir $P(A | B)$ einfach nicht. **Beispiel 4.2.** Sei $X$ eine Exp($\lambda$)-Zufallsvariable mit $\lambda > 0$ und seien $s, t > 0$. Dann gilt $$ P(X > t + s | X > t) = \frac{P(X > t + s)}{P(X > t)} = \frac{e^{-\lambda (t + s)}}{e^{-\lambda t}} = e^{-\lambda s}. $$ Dies wird als *Gedächtnislosigkeitseigenschaft* bezeichnet. Wenn man sich vorstellt, dass $X$ die Wartezeit ist, bis etwas geschieht, bedeutet dies, dass bedingt darauf, dass man mindestens $t$ gewartet hat, die Wahrscheinlichkeit, dass man mindestens noch eine Zeitspanne $s$ warten muss, nicht davon abhängt, wie groß $t$ ist. Diese Eigenschaft gilt auch für die geometrische Zufallsvariable (prüfen!). 

Zum Beispiel: Wenn man wiederholt und unabhängig eine Münze wirft und $X$ der erste Zeitpunkt ist, an dem „Kopf“ erscheint, dann bedeutet dies, dass die Wahrscheinlichkeit, dass man $m$ Würfe warten muss, bedingt darauf, dass in den ersten $m$ Würfen kein „Kopf“ erschien, gleich der Wahrscheinlichkeit ist, dass man zu Beginn $m$ Würfe warten muss. Das heißt, selbst wenn in den ersten 1000 Münzwürfen kein „Kopf“ erschien, macht dies die Wartezeit auf einen „Kopf“ in den nächsten Würfen nicht kürzer.

#### Aufgabe 4.1:

Sei $X$ eine positive Zufallsvariable, deren Verteilung eine Dichte $\rho$ bezüglich des Lebesgue-Maßes hat. Zeige, dass, wenn $X$ die Gedächtnislosigkeitseigenschaft $P(X > t + s | X > t) = P(X > s)$ für alle $t, s > 0$ erfüllt, dann $X$ eine exponentielle Zufallsvariable ist. 

---

#### Bemerkung 4.3:

Die folgenden Beobachtungen sind elementar, aber wichtig: - Gegeben zwei Ereignisse $A$ und $B$ mit $P(A), P(B) > 0$, sind $A$ und $B$ unabhängig genau dann, wenn $P(A | B) = P(A)$ und in diesem Fall auch $P(B | A) = P(B)$. - Gegeben eine abzählbare disjunkte Zerlegung $(B_i)_{i \geq 1}$ von $\Omega$ in Ereignisse $B_i$ mit strikt positiver Wahrscheinlichkeit, gilt für jedes Ereignis $A$, $$ P(A) = \sum_i P(B_i) P(A | B_i). \tag{4.2} $$ (Dies ergibt sich einfach aus der $\sigma$-Additivität des Maßes.) Dies wird als *Gesamtwahrscheinlichkeitsformel* bezeichnet. - Gegeben ein Ereignis $B$ mit $P(B) > 0$, definiert die Abbildung $A \in \mathcal{A} \mapsto P(A | B)$ ein Wahrscheinlichkeitsmaß (das bedingte Wahrscheinlichkeitsmaß $P$ bedingt auf $B$). - Gegeben eine endliche oder abzählbare disjunkte Zerlegung von $\Omega$ in Ereignisse $(B_i)_{i \geq 1}$ mit strikt positiver Wahrscheinlichkeit und ein beliebiges Ereignis $A$ mit strikt positiver Wahrscheinlichkeit, gilt für jedes $k$, $$ P(B_k | A) = \frac{P(A | B_k) P(B_k)}{\sum_i P(A | B_i) P(B_i)}. \tag{4.3} $$ Der Beweis ist elementar: Der Zähler ist einfach $P(A \cap B_k)$ und der Nenner ist $P(A)$, unter Verwendung der Gesamtwahrscheinlichkeitsformel. Formula (4.3) is the famous "Bayes's formula". Despite the fact that it is almost a trivial
identity, its meaning is very important. Let us see an example.

#### Beispiel 4.4:

Österreich, als Bundesstaat, besteht aus $M = 9$ Regionen, nummeriert mit $i = 1, \dots, M$, jede mit einer Bevölkerung von $N_i$. Bei einer Wahl hat die Partei $P$ in jeder Region $i \leq M$ einen Stimmenanteil von $x_i$ erzielt. Wähle eine Person $X$ gleichmäßig zufällig in Österreich. Bedingt auf das Ereignis, dass die Person für die Partei $P$ gestimmt hat, was ist die Wahrscheinlichkeit, dass die Person aus der Region $i$ kommt? Das heißt, was ist die Wahrscheinlichkeit, dass ein zufällig gewählter Wähler der Partei $P$ aus der Region $i$ stammt? $X$ ist eine gleichmäßig verteilte Zufallsvariable über die Gesamtbevölkerung, also aus der Menge von $N = \sum_{i=1}^M N_i$ Personen. Die *a priori* Wahrscheinlichkeit, dass $X$ aus der Region $i$ stammt, beträgt $\frac{N_i}{N}$. Bezeichnen wir mit $B_i$ das Ereignis, dass $X$ aus der Region $i$ stammt. Die Mengen $B_i$ sind offensichtlich disjunkt und bilden eine Zerlegung des Stichprobenraums. Wir wollen also $P(B_i | A)$ bestimmen, wobei $A$ das Ereignis ist, dass $X$ für die Partei $P$ gestimmt hat. Nach der Bayes-Formel gilt: $$ P(B_i | A) = \frac{P(A | B_i) P(B_i)}{\sum_{j=1}^M P(A | B_j) P(B_j)} = \frac{x_i \frac{N_i}{N}}{\sum_{j=1}^M x_j \frac{N_j}{N}} = \frac{x_i N_i}{\sum_{j=1}^M x_j N_j}. $$ Zum Beispiel: Angenommen, eine der 9 Regionen (sagen wir Vorarlberg) ist viel kleiner als die anderen, mit $N_{\text{Vorarlberg}} / N = 0.04$, aber die Partei $P$ erhält $x_{\text{Vorarlberg}} = 50\%$ der Stimmen, während die Partei insgesamt in Österreich nur 10\% der Stimmen erhält. Die Wahrscheinlichkeit, dass eine zufällig ausgewählte Person, die für $P$ gestimmt hat, aus Vorarlberg stammt, ist dann $0.5 \cdot 0.04 / 0.1 = 0.2$, also 20\%. Das ist viel höher als die Wahrscheinlichkeit, dass eine zufällig ausgewählte Person aus Österreich aus Vorarlberg stammt (das sind 4\%). 

---
## 4.3 Bedingte Erwartung, Teil I 

### Definition 4.5:

Sei $X$ eine integrierbare Zufallsvariable ($X \in L^1(P)$) und $A$ ein Ereignis positiver Wahrscheinlichkeit. Dann ist die bedingte Erwartung von $X$ gegeben $A$ definiert als $$ \mathbb{E}(X | A) = \frac{\mathbb{E}(X \mathbf{1}_A)}{P(A)}. \tag{4.4} $$ Beachte, dass $|\mathbb{E}(X | A)| \leq \frac{\mathbb{E}(|X|)}{P(A)} < \infty$. Zur Vorbereitung des nächsten Abschnitts nehmen wir an, dass der Stichprobenraum eine disjunkte, abzählbare Zerlegung $\Omega = \bigcup_{i \geq 1} B_i$ in Ereignisse positiver Wahrscheinlichkeit zulässt. Bezeichnen wir mit $\mathcal{F} = \sigma((B_i)_{i \geq 1})$ die $\sigma$-Algebra, die durch diese Ereignisse erzeugt wird. Definieren wir die folgende Zufallsvariable, die wir mit $\mathbb{E}(X | \mathcal{F})$ bezeichnen: $$ \mathbb{E}(X | \mathcal{F})(\omega) = \mathbb{E}(X | B_i) \quad \text{falls } \omega \in B_i. \tag{4.5} $$

---
#### Einige wichtige Anmerkungen:

- Da $\Omega = \bigcup_{i \geq 1} B_i$, definiert diese Formel $\mathbb{E}(X | \mathcal{F})$ überall. 

- Diese Zufallsvariable ist $(\Omega, \mathcal{F})$-$ (\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar, da sie per Definition auf jedem $B_i$ konstant ist.

- $\mathbb{E}(X | \mathcal{F})$ ist nicht nur messbar, sondern auch integrierbar und ihr Integral entspricht $\mathbb{E}(X)$. In der Tat gilt $|\mathbb{E}(X | \mathcal{F})| \leq \mathbb{E}(|X| | \mathcal{F})$ (dies folgt unmittelbar aus der Definition), und $$ \mathbb{E} (\mathbb{E}(|X| | \mathcal{F})) = \int \mathbb{E}(|X| | \mathcal{F}) \, dP$$ $$= \sum_{i \geq 1} P(B_i) \mathbb{E}(|X| | B_i) = \sum_{i \geq 1} \mathbb{E}(|X| \mathbf{1}_{B_i}) = \mathbb{E}(|X|) < \infty. $$ Ähnlich zeigt man, dass $\mathbb{E} (\mathbb{E}(X | \mathcal{F})) = \mathbb{E}(X)$ (ohne Betrag).

---
## 4.4 Bedingte Erwartung II:

Sei wie üblich $(\Omega, \mathcal{A}, P)$ unser Wahrscheinlichkeitsraum, und sei $\mathcal{F} \subset \mathcal{A}$ eine $\sigma$-Algebra (typischerweise ist $\mathcal{F}$ streng kleiner als $\mathcal{A}$). Sei außerdem $X$ eine $L^1(\Omega, \mathcal{A}, P)$-Zufallsvariable, wobei die Notation betont, dass $X$ $(\Omega, \mathcal{A})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar ist, jedoch nicht notwendig $(\Omega, \mathcal{F})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar sein muss. 

### Definition 4.6.(Bedingte Erwartung bezüglich einer $\sigma$-Algebra):

Eine Zufallsvariable $Y$ heißt bedingte Erwartung von $X$ bezüglich $\mathcal{F}$ (und wir schreiben $Y = \mathbb{E}(X | \mathcal{F})$), wenn die folgenden zwei Bedingungen erfüllt sind: 

- $Y$ ist $(\Omega, \mathcal{F})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar. 

- Für jedes $A \in \mathcal{F}$ gilt $\mathbb{E}(X \mathbf{1}_A) = \int_A X \, dP = \mathbb{E}(Y \mathbf{1}_A)$. Falls $B \in \mathcal{A}$ und $f = \mathbf{1}_B$, nennen wir $\mathbb{E}(f | \mathcal{F})$ die bedingte Wahrscheinlichkeit von $B$ bezüglich $\mathcal{F}$ und schreiben $\mathbb{E}(B | \mathcal{F})$. 
---
#### Beispiel 4.7. (Sehr elementare Beispiele):

Sei $X$ eine integrierbare Zufallsvariable und $Y$ eine von $X$ unabhängige Zufallsvariable. Dann gilt: $$ \mathbb{E}(X | X) = X, \quad \mathbb{E}(X | Y) = \mathbb{E}(X). \tag{4.6} $$ Lassen Sie uns überprüfen, ob dies die Definition erfüllt. Ist $X$ messbar bezüglich $\sigma(X)$? Natürlich. Gilt außerdem $\mathbb{E}(X \mathbf{1}_A) = \mathbb{E}(X \mathbf{1}_A)$ für jedes $A \in \sigma(X)$? Ja, das stimmt offensichtlich. Im zweiten Fall: Offensichtlich ist $\mathbb{E}(X)$, da es konstant ist, bezüglich jeder $\sigma$-Algebra messbar. Auch gilt für jedes $A \in \sigma(Y)$, dass $\mathbb{E}(X \mathbf{1}_{A(Y)}) = \mathbb{E}(X) P(Y \in A) = \mathbb{E}(\mathbf{1}_A(Y) \mathbb{E}(X))$, wobei in der ersten Gleichheit die Unabhängigkeit von $X$ und $Y$ verwendet wurde. Beide Eigenschaften der bedingten Erwartung sind also erfüllt. 

### Beispiel 4.8:

Sei $\Omega = \{1, \dots, 6\}^2$ mit $\mathcal{A} = 2^\Omega$ und $P$ das Gleichmaß auf $\Omega$. Sei $\mathcal{F}$ die $\sigma$-Algebra, die durch die Mengen $\{i\} \times \{1, \dots, 6\}$ erzeugt wird. (Man denke an einen Stichprobenraum, der das Ergebnis des Wurfs zweier Würfel darstellt, und an die $\sigma$-Algebra $\mathcal{F}$, die Mengen enthält, die nur vom Ergebnis des ersten Würfels abhängen.) 

Bezeichnen wir das generische Element von $\Omega$ mit $\omega = (\omega_1, \omega_2)$ und betrachten die Zufallsvariable $X: (\omega_1, \omega_2) \mapsto \omega_1 + \omega_2$ (die Summe der Ergebnisse der beiden Würfel). Diese ist nicht $\mathcal{F}$-messbar. Definieren wir $Y$ als $$ Y(\omega_1, \omega_2) = \frac{1}{6} \sum_{i=1}^6 X(\omega_1, i). $$ Offensichtlich ist $Y$ $\mathcal{F}$-messbar. Für jedes $A \in \mathcal{F}$, also ein Ereignis der Form $B \times \{1, \dots, 6\}$ mit $B \subset \{1, \dots, 6\}$, haben wir $$ \mathbb{E}(X \mathbf{1}_A) = \frac{1}{36} \sum_{i, j = 1}^6 X(i, j) \mathbf{1}_{i \in B} \quad \text{und} \quad \mathbb{E}(Y \mathbf{1}_A) = \frac{1}{6} \sum_{i \in B} \frac{1}{6} \sum_{j=1}^6 X(i, j), $$ und die beiden Ausdrücke stimmen überein. Wir sehen daher, dass $Y$ eine bedingte Erwartung von $X$ bezüglich $\mathcal{F}$ ist. Was ist die Logik bei der Definition von $Y(\omega_1, \omega_2) = \frac{1}{6} \sum_{i=1}^6 X(\omega_1, i)$? Die Idee ist, dass wir über den Wert von $\omega_2$ gemittelt haben (da die $\sigma$-Algebra $\mathcal{F}$ darüber keine Information enthält), während wir $\omega_1$ unverändert beibehalten haben. 

### Beispiel 4.9:

Dieses Beispiel ist dem vorherigen sehr ähnlich, nur dass wir diskrete Gleichverteilungen durch stetige ersetzen. Sei $\Omega = (0, 1)^2$ mit der $\sigma$-Algebra $\mathcal{B}((0, 1)^2)$ und dem zweidimensionalen Lebesgue-Maß (das ein Wahrscheinlichkeitsmaß ist, da $(0, 1)^2$ eine Fläche von 1 hat). Definiere $\mathcal{F}$ als die $\sigma$-Algebra, die durch die Mengen $(a, b) \times (0, 1)$ erzeugt wird. Beachten Sie, dass diese viel kleiner ist als $\mathcal{A}$, da sie keine Rechtecke der Form $(a, b) \times (c, d)$ enthält, außer wenn $(c, d)$ entweder leer oder $(0, 1)$ ist. Die Zufallsvariable $X: (x, y) \mapsto x + y$ ist $(\Omega, \mathcal{A})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar, jedoch nicht $(\Omega, \mathcal{F})$-$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar, da sie auch von der $y$-Koordinate abhängt. Definieren wir die Zufallsvariable $Y$ als $$ Y(x, y) = x + \frac{1}{2}. $$ Offensichtlich ist $Y$ $\mathcal{F}$-messbar, da sie nur von $x$ abhängt. Für jedes $A \in \mathcal{F}$, das notwendigerweise die Form $B \times (0, 1)$ für eine Borel-Menge $B \subset (0, 1)$ hat, gilt
 $\int_A X \, dP = \int_B \lambda(dx) \int_{(0,1)} X(x, y) \, \lambda(dy)$ 
 $= \int_B \lambda(dx) \int_0^1 (x + y) \, dy = \int_B \left( x + \frac{1}{2} \right) \lambda(dx)$
$= \int_A Y \, dP,$ 

sodass $Y$ beide Eigenschaften der bedingten Erwartung von $X$ bezüglich $\mathcal{F}$ erfüllt. Die Logik dahinter ist wieder, dass wir $X$ bezüglich der $y$-Koordinate mitteln, da die $\sigma$-Algebra $\mathcal{F}$ darüber keine Informationen enthält. Das nächste grundlegende Resultat besagt, dass die bedingte Erwartung stets existiert und in einem gewissen Sinne eindeutig ist. 

---

### Satz 4.10:

Wenn $X$ integrierbar ist, dann existiert die bedingte Erwartung $\mathbb{E}(X | \mathcal{F})$ und ist bis auf eine Nullmengen-Eindeutigkeit eindeutig bestimmt, im Sinne von: Wenn zwei bedingte Erwartungen $Y$ und $Y'$ existieren, dann gilt $P(Y = Y') = 1$. 
#### Beweis([[2024_11_12#Satz 4.10]]): 

*(Eindeutigkeit)* Sei $Y$ und $Y'$ Zufallsvariablen, die die Definition der bedingten Erwartung von $X$ bezüglich $\mathcal{F}$ erfüllen. Sei $A = \{\omega \in \Omega : Y(\omega) > Y'(\omega)\}$, das ist ein Element von $\mathcal{F}$, da sowohl $Y$ als auch $Y'$ $\mathcal{F}$-messbar sind. Dann gilt $$ 0 = \mathbb{E}(Y \mathbf{1}_A) - \mathbb{E}(Y' \mathbf{1}_A) = \int_A (Y - Y') \, dP, $$ und da $Y > Y'$ auf $A$, impliziert dies, dass $A$ Wahrscheinlichkeit null hat. Ebenso hat die Menge, auf der $Y < Y'$, Wahrscheinlichkeit null. 

*(Existenz)* Zerlege $X = X^+ - X^-$ in die Differenz seines positiven und seines negativen Teils. Für jedes $A \in \mathcal{F}$ definiere $$ Q^\pm(A) = \mathbb{E}(X^\pm \mathbf{1}_A). \tag{4.7} $$ Da $X$ integrierbar ist, ist der Erwartungswert endlich, sodass die Mengenfunktion $Q^\pm$ $\mathcal{F}$ auf $[0, \mathbb{E}(X^\pm)]$ abbildet und insbesondere endliche Maße auf $(\Omega, \mathcal{F})$ sind. Wenn $P(A) = 0$, dann gilt offensichtlich $Q^\pm(A) = 0$, d.h., $Q^\pm$ sind absolut stetig bezüglich $P$. Nach dem Satz von Radon-Nikodym existiert eine Dichte, also eine positive, $\mathcal{F}$-messbare Funktion $Y^\pm$ mit $$ Q^\pm(A) = \int_A Y^\pm \, dP = \mathbb{E}(Y^\pm \mathbf{1}_A). \tag{4.8} $$ Dann ist $Y = Y^+ - Y^-$ eine bedingte Erwartung von $X$ bezüglich $\mathcal{F}$
$\square$ .

---
## Definition 4.11:

Gegeben eine $L^1$-Zufallsvariable $X$ und eine Zufallsvariable $Y$, bezeichnen wir mit $\mathbb{E}(X | Y)$ die bedingte Erwartung $\mathbb{E}(X | \sigma(Y))$, wobei $\sigma(Y)$ die von $Y$ erzeugte $\sigma$-Algebra ist. ### 4.4.1 Die intuitive Bedeutung Die Definition von $\mathbb{E}(X | \mathcal{F})$ ist möglicherweise nicht sehr anschaulich, und der Existenzbeweis (Satz 4.10) trägt auch nicht wesentlich zur Intuition bei. Der folgende Satz (den ich ohne Beweis angebe; siehe [Klenke], Seite 97, wenn Sie interessiert sind) und die Bemerkung direkt danach verdeutlichen besser die Bedeutung von $\mathbb{E}(X | \mathcal{F})$, zumindest im Fall, dass $X$ nicht nur integrierbar, sondern quadratisch integrierbar ist: 

### Satz 4.12(Bedingte Erwartung als orthogonale Projektion):

Sei $\mathcal{F} \subset \mathcal{A}$ eine $\sigma$-Algebra und $X$ eine $L^2(\Omega, \mathcal{A}, P)$-Zufallsvariable. Jede $\mathcal{F}$-messbare und quadratisch integrierbare Zufallsvariable $Y$ erfüllt $$ \mathbb{E}[(X - Y)^2] \geq \mathbb{E}[(X - \mathbb{E}(X | \mathcal{F}))^2]. \tag{4.9} $$
und Gleichheit gilt nur, wenn $Y = \mathbb{E}(X | \mathcal{F})$ (fast sicher). 

---

#### Bemerkung 4.13:

Um die Bedeutung zu verstehen, denken Sie an einen euklidischen Raum (zum Beispiel $\mathbb{R}^3$). Sei $P$ eine Ebene, die den Ursprung enthält, und $x$ ein Punkt, der nicht in $P$ liegt. Dann existiert ein eindeutiger Punkt $y^*$ in $P$, der die minimale Entfernung zu $x$ hat, und $y^*$ ist einfach die orthogonale Projektion von $x$ auf $P$. Für jedes $y \in P$, $y \neq y^*$, gilt $\|y - x\| > \|y^* - x\|$. Erinnern wir uns daran, dass $L^2(P)$ ein Vektorraum mit einem Skalarprodukt $\langle f, g \rangle = \mathbb{E}(f g)$ ist. Dieses Skalarprodukt induziert eine Distanz $\|f - g\|_{L^2} = \sqrt{\langle f - g, f - g \rangle}$. Außerdem ist die Menge $V_{\mathcal{F}}$ aller quadratisch integrierbaren Zufallsvariablen $Y$, die $\mathcal{F}$-messbar sind, ein linearer Unterraum von $L^2(P)$. Was Satz 4.12 also aussagt, ist, dass $\mathbb{E}(X | \mathcal{F})$ die orthogonale Projektion von $X$ auf $V_{\mathcal{F}}$ ist. Eine andere Sichtweise auf Satz 4.12 ist die folgende: Unter allen Zufallsvariablen, die $\mathcal{F}$-messbar sind, ist $\mathbb{E}(X | \mathcal{F})$ diejenige, die $X$ am nächsten kommt. Angenommen, wir sind an einer zufälligen experimentellen Größe $X$ interessiert, haben jedoch nur eingeschränkten Zugang dazu, das heißt, wir können nur „messen“, ob Ereignisse in $\mathcal{F}$ auftreten. Dann ist $\mathbb{E}(X | \mathcal{F})$ die beste Schätzung für $X$ (im Sinne des mittleren quadratischen Fehlers), basierend auf der Beobachtung des Auftretens von Ereignissen in $\mathcal{F}$. 

#### Beispiel 4.14:

Wir sind an einer Zufallsvariable $Z$ interessiert, die die Summe zweier unabhängiger Zufallsvariablen $X$ und $Y$ ist, die beide gleichverteilt auf $[0, 1]$ sind. Wenn wir den Wert von $Y$ beobachten und den Wert $y$ finden, dann ist unsere beste Schätzung für $Z$ $Z = \mathbb{E}(X) + y$. Tatsächlich ist der durchschnittliche quadratische Fehler, den wir bei dieser Schätzung machen, $\mathbb{E}((X + y - (1/2 + y))^2) = \text{Var}(X)$, und wenn wir eine andere Schätzung vornehmen würden, sagen wir $f(y)$, dann wäre der mittlere quadratische Fehler $\mathbb{E}((X + y - f(y))^2) > \text{Var}(X)$, weil wir wissen, dass $\mathbb{E}((X - a)^2)$ minimal ist, wenn $a = \mathbb{E}(X)$. 

---

### 4.4.2 Eigenschaften der bedingten Erwartung 

### 4.4.3 Randverteilung, bedingte Verteilung, bedingte Erwartung

Gegeben einen Zufallsvektor $(X_1, \dots, X_n)$ mit einer Verteilung $\nu$, nennt man die Verteilung der einzelnen Zufallsvariable $X_i$ die *$i$-te Randverteilung*. Falls $\nu$ eine Dichte $f$ bezüglich des $n$-dimensionalen Lebesgue-Maßes $\lambda_n$ besitzt, dann hat die Randverteilung von $X_i$ die Dichte $$ f_i(x) = \int_{\mathbb{R}^{n-1}} f(y_1, \dots, y_{i-1}, x, y_{i+1}, \dots, y_n) \, \lambda_{n-1}(dy). \tag{4.10} $$ Da $f$ nicht-negativ und integrierbar ist, impliziert der Satz von Fubini, dass $f_i(x)$ für $\lambda$-fast jedes $x$ endlich ist. Für $n = 2$ ergibt sich $$ f_1(x) = \int_{\mathbb{R}} f(x, y) \, \lambda(dy), \quad f_2(x) = \int_{\mathbb{R}} f(y, x) \, \lambda(dx). \tag{4.11} $$ Angenommen, $X_1$ ist integrierbar und wir möchten $Y = \mathbb{E}(X_1 | X_2)$ bestimmen. Definiere $$ Y = \int_{\mathbb{R}} x \frac{f(x, X_2)}{f_2(X_2)} \, \lambda(dx). \tag{4.12} $$ Erinnern wir uns, dass $f_2(x)$
