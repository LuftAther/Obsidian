## Definition 4.11:

Gegeben eine $L^1$-Zufallsvariable $X$ und eine Zufallsvariable $Y$, bezeichnen wir mit $\mathbb{E}(X | Y)$ die bedingte Erwartung $\mathbb{E}(X | \sigma(Y))$, wobei $\sigma(Y)$ die von $Y$ erzeugte $\sigma$-Algebra ist. ### 4.4.1 Die intuitive Bedeutung Die Definition von $\mathbb{E}(X | \mathcal{F})$ ist möglicherweise nicht sehr anschaulich, und der Existenzbeweis (Satz 4.10) trägt auch nicht wesentlich zur Intuition bei. Der folgende Satz (den ich ohne Beweis angebe; siehe [Klenke], Seite 97, wenn Sie interessiert sind) und die Bemerkung direkt danach verdeutlichen besser die Bedeutung von $\mathbb{E}(X | \mathcal{F})$, zumindest im Fall, dass $X$ nicht nur integrierbar, sondern quadratisch integrierbar ist: 

### Satz 4.12(Bedingte Erwartung als orthogonale Projektion):

Sei $\mathcal{F} \subset \mathcal{A}$ eine $\sigma$-Algebra und $X$ eine $L^2(\Omega, \mathcal{A}, P)$-Zufallsvariable. Jede $\mathcal{F}$-messbare und quadratisch integrierbare Zufallsvariable $Y$ erfüllt $$ \mathbb{E}[(X - Y)^2] \geq \mathbb{E}[(X - \mathbb{E}(X | \mathcal{F}))^2]. \tag{4.9} $$
und Gleichheit gilt nur, wenn $Y = \mathbb{E}(X | \mathcal{F})$ (fast sicher). 

---

#### Bemerkung 4.13:

Um die Bedeutung zu verstehen, denken Sie an einen euklidischen Raum (zum Beispiel $\mathbb{R}^3$). Sei $P$ eine Ebene, die den Ursprung enthält, und $x$ ein Punkt, der nicht in $P$ liegt. Dann existiert ein eindeutiger Punkt $y^*$ in $P$, der die minimale Entfernung zu $x$ hat, und $y^*$ ist einfach die orthogonale Projektion von $x$ auf $P$. Für jedes $y \in P$, $y \neq y^*$, gilt $\|y - x\| > \|y^* - x\|$. Erinnern wir uns daran, dass $L^2(P)$ ein Vektorraum mit einem Skalarprodukt $\langle f, g \rangle = \mathbb{E}(f g)$ ist. Dieses Skalarprodukt induziert eine Distanz $\|f - g\|_{L^2} = \sqrt{\langle f - g, f - g \rangle}$. Außerdem ist die Menge $V_{\mathcal{F}}$ aller quadratisch integrierbaren Zufallsvariablen $Y$, die $\mathcal{F}$-messbar sind, ein linearer Unterraum von $L^2(P)$. Was Satz 4.12 also aussagt, ist, dass $\mathbb{E}(X | \mathcal{F})$ die orthogonale Projektion von $X$ auf $V_{\mathcal{F}}$ ist. Eine andere Sichtweise auf Satz 4.12 ist die folgende: Unter allen Zufallsvariablen, die $\mathcal{F}$-messbar sind, ist $\mathbb{E}(X | \mathcal{F})$ diejenige, die $X$ am nächsten kommt. Angenommen, wir sind an einer zufälligen experimentellen Größe $X$ interessiert, haben jedoch nur eingeschränkten Zugang dazu, das heißt, wir können nur „messen“, ob Ereignisse in $\mathcal{F}$ auftreten. Dann ist $\mathbb{E}(X | \mathcal{F})$ die beste Schätzung für $X$ (im Sinne des mittleren quadratischen Fehlers), basierend auf der Beobachtung des Auftretens von Ereignissen in $\mathcal{F}$. 

#### Beispiel 4.14:

Wir sind an einer Zufallsvariable $Z$ interessiert, die die Summe zweier unabhängiger Zufallsvariablen $X$ und $Y$ ist, die beide gleichverteilt auf $[0, 1]$ sind. Wenn wir den Wert von $Y$ beobachten und den Wert $y$ finden, dann ist unsere beste Schätzung für $Z$ $Z = \mathbb{E}(X) + y$. Tatsächlich ist der durchschnittliche quadratische Fehler, den wir bei dieser Schätzung machen, $\mathbb{E}((X + y - (1/2 + y))^2) = \text{Var}(X)$, und wenn wir eine andere Schätzung vornehmen würden, sagen wir $f(y)$, dann wäre der mittlere quadratische Fehler $\mathbb{E}((X + y - f(y))^2) > \text{Var}(X)$, weil wir wissen, dass $\mathbb{E}((X - a)^2)$ minimal ist, wenn $a = \mathbb{E}(X)$. 

---

### 4.4.2 Eigenschaften der bedingten Erwartung 

### 4.4.3 Randverteilung, bedingte Verteilung, bedingte Erwartung

Gegeben einen Zufallsvektor $(X_1, \dots, X_n)$ mit einer Verteilung $\nu$, nennt man die Verteilung der einzelnen Zufallsvariable $X_i$ die *$i$-te Randverteilung*. Falls $\nu$ eine Dichte $f$ bezüglich des $n$-dimensionalen Lebesgue-Maßes $\lambda_n$ besitzt, dann hat die Randverteilung von $X_i$ die Dichte $$ f_i(x) = \int_{\mathbb{R}^{n-1}} f(y_1, \dots, y_{i-1}, x, y_{i+1}, \dots, y_n) \, \lambda_{n-1}(dy). \tag{4.10} $$ Da $f$ nicht-negativ und integrierbar ist, impliziert der Satz von Fubini, dass $f_i(x)$ für $\lambda$-fast jedes $x$ endlich ist. Für $n = 2$ ergibt sich $$ f_1(x) = \int_{\mathbb{R}} f(x, y) \, \lambda(dy), \quad f_2(x) = \int_{\mathbb{R}} f(y, x) \, \lambda(dx). \tag{4.11} $$ Angenommen, $X_1$ ist integrierbar und wir möchten $Y = \mathbb{E}(X_1 | X_2)$ bestimmen. Definiere $$ Y = \int_{\mathbb{R}} x \frac{f(x, X_2)}{f_2(X_2)} \, \lambda(dx). \tag{4.12} $$ Erinnern wir uns, dass $f_2(x)$
