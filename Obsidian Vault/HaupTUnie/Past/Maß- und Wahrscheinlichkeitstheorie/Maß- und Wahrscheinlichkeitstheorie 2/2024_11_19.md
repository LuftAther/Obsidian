### Beispiel 4.10. (Sehr elementare Beispiele):

Sei $X$ eine integrierbare Zufallsvariable und $Y$ eine Zufallsvariable, die unabhängig von $X$ ist. Dann gilt:

$$E(X∣X)=X;E(X∣Y)=E(X).E(X∣X)=X;E(X∣Y)=E(X)$$.

Lassen Sie uns prüfen, ob dies die Definition erfüllt. Ist $X$ messbar bezüglich $\sigma(X)$? Natürlich. Ist es der Fall, dass $E(X \mathbf{1}_A) = E(X \mathbf{1}_A)$ für jedes $A \in \sigma(X)$? Ja, durchaus.

Betrachten wir den zweiten Fall: Offensichtlich ist $E(X)$, da es konstant ist, bezüglich jeder beliebigen $\sigma$-Algebra messbar. Außerdem, wenn $A \in \sigma(Y)$, dann haben wir $E(X \mathbf{1}_A(Y)) = E(X) P(Y \in A) = E(\mathbf{1}_A(Y) E(X))$, wobei wir in der ersten Gleichung die Unabhängigkeit von $X$ und $Y$ benutzt haben. Auch hier sind beide Eigenschaften der bedingten Erwartung erfüllt.

### Beispiel 4.11:

Sei $\Omega = {1, \dots, 6}^2$ mit $A = 2$ und $P$ das gleichmäßige Maß auf $\Omega$. Sei $F$ die $\sigma$-Algebra, die von den Mengen ${i} \times {1, \dots, 6}$ erzeugt wird. (Denken Sie an einen Ergebnisraum, der das Ergebnis des Wurfs von zwei Würfeln repräsentiert, und die $\sigma$-Algebra $F$ als diejenige, die nur Mengen enthält, die vom Ergebnis des ersten Würfels abhängen.)

### Beispiel 4.12

Nennen wir $\omega = (\omega_1, \omega_2)$ das generische Element von $\Omega$. Betrachten wir die Zufallsvariable $X: (\omega_1, \omega_2) \mapsto \omega_1 + \omega_2$, also die Summe der beiden Würfelergebnisse in unserem Beispiel. Diese ist nicht messbar bezüglich $F$. Definieren wir daher $Y: (\omega_1, \omega_2) \mapsto \frac{1}{6} \sum_{i=1}^{6} X(\omega_1, i)$. Offensichtlich ist $Y$ $F$-messbar.

Für ein Ereignis $A \in F$, das die Form $B \subseteq {1, \dots, 6}$ mit $B \subset {1, \dots, 6}$ hat, erhalten wir:

$$E(X \mathbf{1}_A) = \frac{1}{36} \sum_{i, j \leq 6} X(i, j) \mathbf{1}_{i \in B} \quad \text{und} \quad E(Y \mathbf{1}_A) = \frac{1}{6} \sum_{i \in B} \frac{1}{6} \sum_{j \leq 6} X(i, j)$$,

und die beiden Ausdrücke sind identisch. Daher ist $Y$ eine bedingte Erwartung von $X$ bezüglich $F$.

Die Logik hinter der Definition von $Y: (\omega_1, \omega_2) \mapsto \frac{1}{6} \sum_{i=1}^{6} X(\omega_1, i)$ besteht darin, dass wir den Wert von $\omega_2$ „ausmitteln“, weil die $\sigma$-Algebra $F$ keine Information darüber enthält, während $\omega_1$ unverändert bleibt.

### Beispiel 4.13

Dieses Beispiel ist dem vorherigen sehr ähnlich, nur dass wir die diskreten gleichmäßigen Verteilungen durch kontinuierliche ersetzen. Sei $\Omega = (0, 1)^2$ mit der $\sigma$-Algebra $\mathcal{B}((0, 1)^2)$ und dem zweidimensionalen Lebesgue-Maß $\lambda$ (das ein Wahrscheinlichkeitsmaß ist, weil $(0, 1)^2$ die Fläche 1 hat). Definieren wir $F$ als die $\sigma$-Algebra, die von den Mengen $(a, b) \subseteq (0, 1)$ erzeugt wird. Diese ist viel kleiner als $\mathcal{A}$, da sie die Rechtecke $(a, b) \times (c, d)$ nur enthält, wenn $ (c, d)$ entweder leer oder gleich $(0, 1)$ ist.

Definiere die Zufallsvariable $X: (x, y) \mapsto x + y$, die $(\Omega, \mathcal{A}) \rightarrow (\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar ist, aber nicht $(\Omega, F) \rightarrow (\mathbb{R}, \mathcal{B}(\mathbb{R}))$-messbar, weil sie auch von der $y$-Koordinate abhängt. Definiere die Zufallsvariable $Y$ als $Y(x, y) = x + \frac{1}{2}$. Offensichtlich ist $Y$ $F$-messbar, da sie nur von $x$ abhängt.

Für ein Ereignis $A \in F$, das zwangsläufig die Form $B \times (0, 1)$ für eine borelsche Teilmenge $B$ von $(0, 1)$ hat, erhalten wir:

$$\int_A X \, dP = \int_B \lambda(dx) \int_{(0,1)} X(x, y) \, \lambda(dy) = \int_B \lambda(dx) \int_0^1 (x + y) \, dy = \int_B (x + \frac{1}{2}) \, \lambda(dx) = \int_A Y \, dP$$

sodass $Y$ beide Eigenschaften der bedingten Erwartung von $X$ bezüglich $F$ erfüllt.

Die Logik dahinter ist, dass wir $X$ bezüglich der $y$-Koordinate mitteln, weil die $\sigma$-Algebra $F$ darüber keine Informationen enthält.

### Beispiel 4.14

Sei $(X, Y)$ ein diskreter Zufallsvektor mit Werten in ${1, \dots, N}^2$ mit $P((X, Y) = (i, j)) = \frac{1}{N}$ falls $1 \leq i = j \leq N$, und 0, wenn $i \neq j$. Berechnen wir $E(X | Y)$.

Beachte, dass $E(X | Y = i) = N E(X \mathbf{1}_{Y = i}) = N i E(\mathbf{1}_{Y = i}) = i$. Das heißt, $E(X | Y) = Y$.

### Übung 4.3

Sei der Zustandsraum $\Omega = {1, \dots, N}^2$ mit $P({(i, j)}) = c$ für jedes $(i, j) \in \Omega$ mit $i \leq j$ und 0 sonst. Sei $A = 2$ und seien die Zufallsvariablen $X$ und $Y$ die Projektionen auf die erste bzw. zweite Koordinate, also $X(i, j) = i$ und $Y(i, j) = j$. Berechne $c$ und $E(X | Y)$.

**Lösung**: Die Konstante $c$ wird durch die Bedingung $P(\Omega) = 1$ bestimmt, also

$$c \sum_{1 \leq i \leq j \leq N} 1 = 1 \Rightarrow c = \frac{1}{\sum_{1 \leq i \leq j \leq N} 1}$$

Berechnen wir $f(j) := E(X | Y = j)$ für $1 \leq j \leq N$. Wir erhalten

$$f(j) = c \sum_{1 \leq x \leq j} x \bigg/ \sum_{1 \leq x \leq j} 1 = \frac{\sum_{1 \leq x \leq j} x}{\sum_{1 \leq x \leq j} 1} = \frac{j + 1}{2}.$$

Daher sollte intuitiv $E(X | Y) = f(Y) = \frac{Y + 1}{2}$ sein. Überprüfen wir dies. Zunächst ist $\frac{1 + Y}{2}$ offensichtlich $\sigma(Y)$-messbar.

Betrachten wir ein $A \in \sigma(Y)$. $A$ hat zwangsläufig die Form

$$A = \{(i, j) \in \Omega : j \in I\} = \{1, \dots, N\} \times $$

für ein $I \subseteq {1, \dots, N}$, was genau das Urbild $Y^{-1}(I)$ ist. Dann haben wir einerseits

$$E(X \mathbf{1}_A) = c \sum_{1 \leq i \leq j \leq N} i \mathbf{1}_{j \in I} = \sum_{j \in I} \sum_{1 \leq i \leq j} i = \sum_{j \in I} P(Y = j) f(j)$$### Fortsetzung Beispiel 4.14

Da die beiden Ausdrücke für jedes $A \in \sigma(Y)$ übereinstimmen, haben wir $E(X | Y) = f(Y) = \frac{1 + Y}{2}$.

Das nächste fundamentale Ergebnis besagt, dass die bedingte Erwartung immer existiert und in gewissem Sinne eindeutig ist.

### Satz 4.14

Falls $X$ integrierbar ist, dann existiert die bedingte Erwartung $E(X | F)$ und ist bis auf eine Nullmenge eindeutig bestimmt, im Sinne, dass falls zwei bedingte Erwartungen $Y$ und $Y'$ existieren, dann gilt $P(Y = Y') = 1$.

### Bemerkung 4.15

Aufgrund der **Eindeutigkeit bis auf Nullmenge**-Eigenschaft werde ich ab jetzt von „der“ bedingten Erwartung statt „einer“ bedingten Erwartung sprechen.

#### Beweis (Satz 4.14)

**(Eindeutigkeit)** Seien $Y$ und $Y'$ Zufallsvariablen, die die Definition der bedingten Erwartung von $X$ bezüglich $F$ erfüllen. Sei $A = {\omega \in \Omega : Y(\omega) > Y'(\omega)}$, ein Element von $F$, da sowohl $Y$ als auch $Y'$ $F$-messbar sind. Dann gilt:

0=E(Y1A)−E(Y′1A)=∫A(Y−Y′) dP0 = E(Y \mathbf{1}_A) - E(Y' \mathbf{1}_A) = \int_A (Y - Y') \, dP0=E(Y1A​)−E(Y′1A​)=∫A​(Y−Y′)dP

Da $Y > Y'$ auf $A$ gilt, impliziert dies, dass $A$ eine Nullwahrscheinlichkeit hat. Ebenso ist die Wahrscheinlichkeit, dass $Y < Y'$, gleich null.

**(Existenz)** Zerlege $X$ in die Differenz seiner positiven und negativen Teile, also $X = X^+ - X^-$. Definiere dann für jedes $A \in F$ die Setzfunktionen

Q±(A)=E(X±1A).Q^\pm(A) = E(X^\pm \mathbf{1}_A).Q±(A)=E(X±1A​).

Da $X$ integrierbar ist, ist der Erwartungswert endlich, sodass die Setzfunktionen $Q^\pm$ $F$ auf $[0, E(X^\pm)]$ abbilden, und somit sind $Q^\pm$ endliche Maße auf $(\Omega, F)$. Wenn $P(A) = 0$, dann ist offensichtlich $Q^\pm(A) = 0$, das heißt, $Q^\pm$ sind absolut stetig bezüglich $P$. Nach dem Satz von Radon-Nikodym existiert eine Dichte, das heißt eine positive, $F$-messbare Funktion $Y^\pm$ mit

Q±(A)=∫AY± dP=E(Y±1A).Q^\pm(A) = \int_A Y^\pm \, dP = E(Y^\pm \mathbf{1}_A).Q±(A)=∫A​Y±dP=E(Y±1A​).

Dann sehen wir, dass $Y = Y^+ - Y^-$ eine bedingte Erwartung von $X$ bezüglich $F$ ist. Tatsächlich ist $Y$ $F$-messbar (Messbarkeit ist stabil unter linearen Kombinationen), und es gilt:

E((Y+−Y−)1A)=E(Y+1A)−E(Y−1A)=E(X+1A)−E(X−1A)=E(X1A)E((Y^+ - Y^-) \mathbf{1}_A) = E(Y^+ \mathbf{1}_A) - E(Y^- \mathbf{1}_A) = E(X^+ \mathbf{1}_A) - E(X^- \mathbf{1}_A) = E(X \mathbf{1}_A)E((Y+−Y−)1A​)=E(Y+1A​)−E(Y−1A​)=E(X+1A​)−E(X−1A​)=E(X1A​)

für jedes $A \in F$, wobei wir die Linearität des Integrals verwendet haben. $\square$

### Bemerkung 4.16

Satz 4.14 besagt, dass beliebige zwei bedingte Erwartungen $Y$ und $Y'$ von $X$ bezüglich $F$ auf einer Menge mit Wahrscheinlichkeit 1 übereinstimmen. Dies impliziert jedoch nicht, dass es eine Menge mit Wahrscheinlichkeit 1 gibt, auf der alle bedingten Erwartungen von $X$ bezüglich $F$ übereinstimmen.

Um dies zu verstehen, betrachte folgendes Beispiel. Sei $\Omega = [0, 1]$ und für $x \in \Omega$ sei $f_x$ die Funktion, die überall außer bei $x$ gleich null ist. Für alle $x, x'$ stimmen die Funktionen $f_x$ und $f_{x'}$ fast überall überein, aber es existiert keine nichtleere Teilmenge von $[0, 1]$, auf der alle $f_x$ übereinstimmen.