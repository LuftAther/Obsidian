### Lemma 2.14(Grönland)
##### Vorraussetzung:
Seien $\alpha, \beta, v \in C([t_0, T]; \mathbb{R})$ mit $\beta \geq 0$ und  $0 \leq v(t) \leq \alpha(t) + \int_{t_0}^{t} \beta(s)v(s) ds, \quad \forall t \in [t_0, T]$
##### Behauptung: 
Dann gilt $v(t) \leq \alpha(t) + \int_{t_0}^{t} \alpha(s)\beta(s)e^{\int_{s}^{t} \beta(\tau) d\tau} ds,  \quad \forall t \in [t_0, T]$
Speziell für $\alpha = M \in \mathbb{R}$ gilt: $v(t) \leq M e^{\int_{t_0}^{t} \beta(\tau) d\tau}$
%%Alternativ:%% $v'(t) \leq \alpha(t) + \beta(t) v(t) \Rightarrow V(t) \leq V(t_0) + \int_{t_0}^{t} \alpha(s) ds + \int_{t_0}^{t} \beta(s) v(s) ds.$

### Satz 2.15(maimale Existenz für linear beschränkte rechte Seite):
##### Vorraussetzung:
- Sei $J \subset \mathbb{R}$ ein Intervall  ($J = (-\infty, a)$, $J = (a, \infty)$ oder $J = \mathbb{R}$ zugelassen).  
- Sei $f \in C(J \times \mathbb{R}^d, \mathbb{R}^d)$  lokal lipschitzstetig im 2. Argument.  
- Es gebe $A, B \in C(J; \mathbb{R})$ mit $A, B \geq 0$ auf $J$  und  $\|f(t, x) \|_{\mathbb{R}^d} \leq A(t) + B(t) \|x\|_{\mathbb{R}^d},  \quad \forall (t, x) \in J \times \mathbb{R}^d.$
##### Behauptung:
Für alle $(t_0, y_0) \in J \times \mathbb{R}^d$  
hat das Anfangswertproblem (AWP)  $y' = f(t, y), \quad y(t_0) = y_0$
eine (eindeutige) Lösung $y \in C^1(J; \mathbb{R}^d)$.

#### Beweis von [[#Satz 2.15(maimale Existenz für linear beschränkte rechte Seite)]]

Sei $(T^-, T^+) \subset \mathbb{R}$ mit $t_0 \in (T^-, T^+)$  das maximale Existenzintervall der Lösung $y_{t_0, y_0}$
des AWP  (vgl. Satz 2.13[[# Satz 2.13]]).  

Wir zeigen, dass $T^+$ mit der rechten Intervallgrenze von $J$ zusammenfällt  
(analog $T^-$ mit der linken Intervallgrenze von $J$).

- $(T^-, T^+)$ ist das maximale Existenzintervall von $y_{t_0, y_0}$  
  *(vgl. Satz 2.13)*.
- *Zu zeigen:*  Falls $T^+ = \mathbb{R}$, dann ist die rechte Intervallgrenze von $J$ erreicht.  
- **Idee:** Wir führen $T^+ < \mathbb{R}$, d auf einen **Widerspruch**  
  mittels **Grönwall-Lemma**.
- Wir zeigen :  $\{ (t, y_{t_0, y_0}(t)) \mid t \in [t_0, T^+) \} \subset K \subset G$  für ein **kompaktes** $K$,                    damit die Fälle (b), (c) aus **Lemma 2.13(iii)** [[#Lemma 2.13(iii)]] nicht auftreten.
- Sei $T^+ < \mathbb{R}$.  
- Sei $T^+ < b < \mathbb{R}$.  
- $y := y_{t_{0}, J_{a}}$ erfüllt $y = y_0 + \int_{t_0}^{t} f(s, y(s)) ds.$  
![[cd.png]]
Setze : $V(t) := \|y(t)\|_{\mathbb{R}^d},$  dann gilt  
$$0 \leq V(t) = \|y(t)\|_{\mathbb{R}^d} \leq \|y_0\|_{\mathbb{R}^d}  +\int_{t_0}^{t} \| f(s, y(s)) \|_{\mathbb{R}^d} ds.$$
Da  $\| f(t, x) \|_{\mathbb{R}^d} \leq A(t) + B(t) \|x\|_{\mathbb{R}^d},$    $\Rightarrow V(t) \leq \underbrace{\|y_0\|_{\mathbb{R}^d} + \int_{t_0}^{t} (A(s)}_{{=:\alpha(t)}} + \underbrace{B(s)}_{=: \beta} V(s)) ds.$  

mit **Grönwall-Lemma**  erhalteh wir $0 \leq \|y(t)\|_{\mathbb{R}^d} = V(t) \leq \alpha(t) + \int_{t_0}^{t} \alpha(s) \beta(s) ds.$  

Da $\alpha$ und $\beta$ stetig auf $J \supset [t_0, T^+)$ sind, folgt, dass  
$\|y(t)\|_{\mathbb{R}^d} \leq M, \quad \forall t \in [t_0, T^+).$  
Also ist  $\{ (t, y(t)) \mid t \in [t_0, T^+) \} =: K.$ 

Damit liegt die Lösung in einer **kompakten Menge** $K$,  
und die Fälle (b), (c) aus **Lemma 2.13(iii)** treten nicht auf.  
$\square$

---

## 2.5 Zwei Beispiele zur Bestimmung maximalerExistenzintervalle  

### Idee:  
- Verwenden der *Eindeutigkeit* der Lösung eines **AWP**  
- **Grönwall-Lemma** als Werkzeug  

---

#### Beispiel 2.13
Gegeben sei die Differentialgleichung:  
$$x' = x - \alpha x^2, \quad x(0) = x_0, \quad \alpha > 0.$$
##### Behauptung:  
Die Lösung $x(t)$ existiert auf $[0, \infty)$. und $x(t) \leq 0$ aud $\mathbb{R}$
##### Beweis:
Sei $(T^-, T^+)$ das **maximale Existenzintervall** der Lösung $x(t)$.  

$\underline{1. Schritt:}$  Wir zeigen, dass $x(t) \geq 0$ für alle $t \in (0, T^+)$.  
Falls  $x(t^*) = 0$, für ein $t^* >0$ dann muss $x \equiv 0$ für alle $t > t^*$.  
Das folgt aus der Eindeutigkeit von Lösungen des *AWPs* und der Tatsache, dass $\tilde{x} \equiv 0$
Denn wenn $\tilde{x}(t^*) = 0$, löst dann gilt für die Differentialgleichung:    
$$\tilde{x}' = \tilde{x} - \alpha \tilde{x}^2.$$

$\underline{2.Schritt:}$ $\mathcal{Z.z}$  $T^+ = \infty$,  auf $(0, T^+)$ gilt:  
$$x' = x - \frac{\alpha}{2} x^2(t) \leq x(t).$$  $\Rightarrow 0 \leq x(t) = x(0) + \int_0^t x'(s) ds.\leq x(0)+\in t_{0}^{t} x(s) ds$  

Mit der Grönwall-Ungleichung ergibt sich:  $$x(t) \leq x(0) e^t.$$  Damit ist **Blow-up** nicht möglich, also ist $T^+ = \infty$.  

---

#### Beispiel 2.20  

Betrachten wir das System: 
- $x' = x - \alpha x^2 - x y, \alpha \geq 0$
- $\quad y' = -\beta y + x y. \beta \geq 0$  
mit den Bedingungen:  $x(0) \geq 0, \quad y(0) \geq 0.$  
Das *maximale Existenzintervall* $:= (T^-, T^+)$.  

$1. \underline{Schritt:}$
Falls $x(t) \geq 0$ und $y(t) \geq 0$ für $t \in (0, T^+)$ gilt, dann  
nehmen wir an, dass es ein $t^* > 0$ gibt mit $x(t^*) = 0$.  

Dann gilt:  
$$\begin{pmatrix} x(t) \\ y(t) \end{pmatrix}  
= \begin{pmatrix} 0 \\ y(t^*) e^{-\beta (t - t^*)} \end{pmatrix}.$$  
d.h wenn $x(t^*) = 0$, dann muss die Lösung für alle Zeiten auf der $y$-Achse sein.  
Sei $y(t^*) = 0$. Definiere $\tilde{x}$ als Lösung der Gleichung  
$$\tilde{x}' = \tilde{x} - \alpha \tilde{x}^2, \quad \tilde{x}(t^*) = x(t^*).$$  Dann ist die Lösung  
$$\begin{pmatrix} x(t) \\ y(t) \end{pmatrix}  
= \begin{pmatrix} \tilde{x}(t) \\ 0 \end{pmatrix}, \quad t \in (0, \infty).$$  Das bedeutet: Falls $y(t^*) = 0$, dann muss die Lösung für alle Zeiten auf der $x$-Achse sein.  
Insbesondere gilt:  

- Falls $(x(0), y(0)) \in \mathbb{R}^+ \times \mathbb{R}^+$, dann ist  $(x(t), y(t)) \in \mathbb{R}^+ \times \mathbb{R}^+ \quad \forall t \in (0, T^+).$  
- Falls $(x(0), y(0)) \in \{0\} \times \mathbb{R}^+$, dann ist  $(x(t), y(t)) \in \{0\} \times \mathbb{R}^+ \quad \forall t \in (0, T^+).$  
- Falls $(x(0), y(0)) \in \mathbb{R}^+ \times \{0\}$, dann ist  $(x(t), y(t)) \in \mathbb{R}^+ \times \{0\} \quad \forall t \in (0, T^+).$  

---

$\underline{2. Schritt:}$ Kein Blow-up  
  Die Differentialgleichung für $x$ lautet:  $x' = x - \underbrace{\alpha x^2}_{\leq 0}.$  

Nach Anwendung des **Grönwall-Lemmas** folgt die Abschätzung:  
$$0 \leq x(t) \leq x(0) e^t.$$  Für die Gleichung von $y$:  $y' = -\beta y + x y \leq x y.$  
Wieder durch Anwendung des **Grönwall-Lemmas** erhalten wir:  
$$0 \leq y(t) \leq y(0) e^{\int_0^t x(s) ds}.$$  Da dies für alle $t$ gilt, folgt, dass kein Blow-up auftreten kann, also ist:  
$$T^+ = \infty.$$  
## 2.4 Satz von Peano  

##### Aussage:  
Falls $f$ stetig ist, dann existiert eine Lösung des Anfangswertproblems (AWP):  
$$
y' = f(t, y), \quad y(0) = y_0
$$  für $f \in G'$
##### Erinnerung:  
Falls $y' = \sqrt{|(y)|}$ mit $y(0) = 0$, hat **keine** eindeutige Lösung
##### Bedeutung:  
Der **Banachsche Fixpunktsatz** ist nicht direkt anwendbar zur **Existenz** der Lösung.  
Daher wird eine andere Methode benötigt.

---

#### Beispiel 2.17 (Euler’sche Polygonzugapproximation) 

Das Anfangswertproblem lautet:  
$$y' = f(t, y), \quad y(t_0) = y_0.$$Sei $h > 0$ 
Definiere eine Folge von Zeitpunkten:  
$$t_i := t_0 + i h, \quad i = 0, 1, 2, \dots, -1, -2, \dots$$  Dann kann die Lösung näherungsweise iterativ berechnet werden:  
$$y(t_1) \approx y(t_0) + h y'(t_0).$$  Da $y'(t_0) \approx f(t_0, y(t_0))$, folgt:  $y_1 := y_0 + h f(t_0, y_0) \approx y(t_1).$  

Analog:  
$$y(t_2) \approx y(t_1) + h y'(t_1) = y(t_1) + h f(t_1, y(t_1)).$$  $$ \Rightarrow y_2 := y_1 + h f(t_1, y_1) \approx y(t_2).$$  Die Rekursion der Euler-Methode ist gegeben durch:  
$$
y_{i+1} := y_i + h f(t_i, y_i), \quad i = 0,1,2, \dots
$$  Analog für negative Indizes:  
$$
y_{i-1} := y_i - h f(t_i, y_i), \quad i = 0,-1,-2, \dots
$$  

Dies beschreibt eine **stückweise lineare Interpolation** mit Knotenpunkten $(t_i, y_i)$.  

---
##### Hoffnung: 
Wenn$y_{k} \mapsto y_{t_{0},y_{0}}$ für $h \to 0$, d.h die diskrete Lösung $y_h$ konvergeirt gegen die exakte Lösung $y_{t_0, y_0}$.  

##### Fakten:  
- $y_{h} \to y_{t_{0},y_{0}}$ auf $[t_{0},T]$ für $h \to 0$ falls $f$ *Libschitzstätig im zweiten srgument* ist
%% Daraus folgt Numerisch%%
- Falls $f \in C^1$, dann existiert eine Lösung$TF h' \to 0$ und eine Lösung $y \in C^1$, mit  $y_h \to y$ 

---

#### Beispiel  
Betrachten wir die einfache Differentialgleichung:  
$$y' = y, \quad y(0) = 1.$$  Die Euler-Methode liefert:  $y_{i+1} = y_i + h y_i = (1+h) y_i.$  

Dies führt zur expliziten Darstellung:  $y_i = (1+h)^i y_0.$  

Für kleine $h$, wenn $h \to 0$, konvergiert dies gegen die exakte Lösung:  $e^t y_0.$  
Das bedeutet: Die Methode konvergiert **punktweise** gegen die exakte Lösung.
### Lemma 2.16 (Arzelà-Ascoli)  

##### Voraussetzungen:  
Sei $I \subset \mathbb{R}$ kompakt und $(x_n)_{n=0}^{\infty} \subset C(I; \mathbb{R}^d)$ eine Folge mit:  
1. $(x_n)_{n=0}^{\infty}$ ist **beschränkt**, d.h. $\exists M > 0$, sodass   $|x_n\|_{C(I; \mathbb{R}^d)} \leq M \quad \forall n \in \mathbb{N}.$  
2. $(x_n)_{n=0}^{\infty}$ ist **gleichgradig stetig**, d.h.  
   $\forall \varepsilon > 0 \quad \exists \delta > 0 \quad \forall n, s, t \in I: \quad |s - t| \leq \delta \Rightarrow \| x_n(t) - x_n(s) \|_{\mathbb{R}^d} \leq \varepsilon.$  
##### Behauptung:
Dann existiert eine **Teilfolge** $(x_{n_k})_k$ und eine Funktion $x \in C(I; \mathbb{R}^d)$ mit  
$$\lim_{k \to \infty} \|x - x_{n_k}\|_{C(I; \mathbb{R}^d)} = 0.$$  
---

### Satz 2.18 (Peano)

##### Voraussetzungen:  
Sei $G \subset \mathbb{R}^{d+1}$ ein Gebiet und $f \in C(G; \mathbb{R}^d)$.  

##### Behauptung:
Dann gilt: Für jedes $(t_0, y_0) \in G$ existiert ein $\varepsilon > 0$ und eine Lösung  
$$y \in C^1((t_0 - \varepsilon, t_0 + \varepsilon); \mathbb{R}^d)$$  
des Anfangswertproblems (AWP):  $y' = f(t, y), \quad y(t_0) = y_0.$  

#### Beweis:  

##### Idee:
- Konstruktion der Folge $(y_h)_{h>0}$ ähnlich zur Euler-Approximation in Beispiel 2.17.  
- Zeige, dass die Folge gleichgradig stetig ist und somit **beschränkt**.  
- Anwendung von **Arzelà-Ascoli**, um eine konvergente Teilfolge $(y_{h_n})_n$ mit Grenzwert $y$ zu erhalten.  
- Zeige schließlich, dass   $y = y_0 + \int_{t_0}^{t} f(s, y(s)) ds.$  
  also eine Lösung des AWP ist.
$\underline{0. Schritt:}$
Wahl der Parameter:
Seien $q, h > 0$ so gewählt, dass  
- $R := [t_0 - q, t_0 + q] \times \{z \in \mathbb{R}^d \mid \|z - y_0\|_{\mathbb{R}^d} \leq \delta \} \subset G.$  

Definiere die Menge der Funktionen  
- $M := \{ g \in C(R, \mathbb{R}^d) \}.$  

Wähle  $\varepsilon := \min \left( q, \frac{\delta}{M} \right).$  

---

- $\underline{2. Schritt:}$ Konstruktion der Approximation
Für $N \in \mathbb{N}$ setze  $h_N := \frac{\varepsilon}{N}.$  

Die Approximation wird über die folgende **Rekursion** definiert:  
$$y_{i+1} := y_i + h_N f(t_i, y_i), \quad i = 0, \dots, N.$$ 
Dies ist wohldefiniert, da $f(t_i, y_i)$ in $R$ enthalten ist (Einschränkung auf gültige Werte).  

Analog für rückwärtslaufende Indizes:  
$$y_{i-1} := y_i - h_N f(t_i, y_i), \quad i = 0, -1, -2, \dots$$  
Die Funktion $y_{R, N}$ wird dann als **stückweise lineare Interpolation** definiert.  
Daraus folgt:  $y_{R, N} \in C_1 \left( [t_0 - \varepsilon, t_0 + \varepsilon], \mathbb{R}^d \right).$  

---

![[stück.png]]
Eine **stückweise lineare Approximation** der Lösung, bestehend aus diskreten Punkten $y_i$, die durch gerade Linien verbunden sind.
#### Summendarstellung der Approximation  
Die Approximation $y_N$ auf dem Intervall $[t_i, t_{i+1}]$ ergibt sich rekursiv als:  
$$
y_N \big|_{[t_i, t_{i+1}]} = y_i + (t - t_i) f(t_i, y_i).
$$  Durch wiederholte Anwendung der Rekursion:  $y_{i+1} = y_i + h_N f(t_i, y_i),$  
folgt durch Substitution:  $y_i = y_{i-1} + h_N f(t_{i-1}, y_{i-1}),$  
$$y_{i-1} = y_{i-2} + h_N f(t_{i-2}, y_{i-2}),$$  
$$\vdots$$  
$$y_i = y_0 + h_N \sum_{j=0}^{i-1} f(t_j, y_j) + (t - t_i) f(t_i, y_i).$$  
