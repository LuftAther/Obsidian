## Satz 2.18 (Peano)

##### Voraussetzung: 
Sei $G \subset \mathbb{R}^{d+1}$ ein Gebiet, $f \in C(G; \mathbb{R}^d)$
##### Behauptung:
Dann gilt: Für jedes $(t_0, y_0) \subset G$ existiert ein $\varepsilon > 0$ und eine Lösung $y \in C^1((t_{0}-\epsilon, t_{0} + \epsilon); \mathbb{R}^d)$ von $y':=f(t,y), y(t_{0}) = y_{0}$

---

### Beweis von [[#Satz 2.18 (Peano)]]

**Beweisidee:** Zeige, dass die Funktionen$y_h$, die in Beispiel 2.17 konstruiert werden, gleichgradig stetig sind, wähle konvergente Teilfolge aus mit Grenzfunktion$y$; rechne nach, dass die Integralgleichung 
$$y(t) = y_0 + \int_{t_0}^{t} f(s, y(s)) ds$$
gilt. Seien$a, b > 0$ mit 
$$
\mathfrak{R} := [t_0 - a, t_0 + a] \times \{ y \in \mathbb{R}^d \mid ||y - y_0||_{\mathbb{R}^d} < b \} \subset G.
$$
Sei $M := \sup ||f||_{C(\mathcal{R}; \mathbb{R}^d)}$
##### 1. Schritt
Wähle $\varepsilon \in (0, a]$, so dass $2M \leq b$.  Dann gilt für$N \in \mathbb{N}$und$h_N := \frac{\varepsilon}{N}$$, dass die Rekursion
$$y_{i+1} := y_i + h_N \cdot f(t_i, y_i), \quad i = 0, \dots, N$$
wohldefiniert ist, d.h. $(t_i, y_i) \in \mathcal{R} \subset G$ für $i = 0, \dots, N$.
Hier ist $t_i = t_0 + i h_N$. Analog ist die Rekursion 
$$
y_{i+1} = y_i - h_N f(t_i, y_i)
$$
mit $i = 0, -1, \dots, -(N - 1)$ wohldefiniert.  
$y_{h_N} \in C([t_0 - \varepsilon, t_0 + \varepsilon]; \mathbb{R}^d)$ ist definiert als der stückweise lineare Interpolant.

##### 2. Schritt
Die Folge $(y_{h_N})_{N \in \mathbb{N}}$ ist gleichgradig stetig.  
In der Tat gilt $||y_{h_N}(t) - y_{h_N}(s)||_{\mathbb{R}^d} \leq |s - t| M$

##### 3. Schritt
Das Arzelà-Ascoli-Theorem liefert eine Teilfolge $(y_{h_{N_k}})_{k \in \mathbb{N}}$ und eine Funktion $y \in C([t_0 - \varepsilon, t_0 + \varepsilon]; \mathbb{R}^d)$ mit
$$
\lim_{N \to \infty} \max_{t \in [t_0 - \varepsilon, t_0 + \varepsilon]} ||y_{h_{N_k}}(t) - y(t)||_{\mathbb{R}^d} = 0
$$
##### 4. Schritt
Wir behaupten:
$$
y(t) = y_0 + \int_{t_0}^{t} f(s, y(s)) ds \quad \forall t \in [t_0 - \varepsilon, t_0 + \varepsilon]
$$

Betrachte nur den Fall $t > t_0$.  Sei $g : s \mapsto (s, y(s))$. Betrachte die stückweise konstante Funktion $g_N$ gegeben durch die Bedingung:
$$g_N'(s) = (t_i, y_{h_N}(t_i)) \quad \text{für } s \in [t_i, t_{i+1}], \quad i = 0, \dots, N' - 1$$
Dann gilt:
- $g_N \to g$ punktweise auf $[t_0, t_0 + \varepsilon]$.
- $y_{h_N}(t) = y_0 + \int_{t_0}^{t} f(g_N(s)) ds, \quad t_0 \leq t \leq t_0 + \varepsilon$
- $||f(g_N(s))||_{\mathbb{R}^d} \leq M \quad \forall s \in [t_0, t_0 + \varepsilon], \quad \forall N' \in \mathbb{N}$

Mit der dominanten Konvergenz von Lebesgue folgt:
$$
\lim_{N' \to \infty} \int_{t_0}^{t} f(g_N(s)) ds = \int_{t_0}^{t} f(g(s)) ds
$$

Also folgt aus $y_{h_N}(t) = y_0 + \int_{t_0}^{t} f(g_N(s)) ds$, dass:
$$
y(t) = \lim_{N' \to \infty} y_{h_N}(t) = \lim_{N' \to \infty} \left( y_0 + \int_{t_0}^{t} f(g_N(s)) ds \right) = y_0 + \int_{t_0}^{t} f(g(s)) ds
= y(t) = y_0 + \int_{t_0}^{t} f(s, y(s)) ds$$
 $\square$

---

#### Bemerkung: (Typischer Existenzbeweis)

1) **Konstruiere** eine Folge von Approximationen.
2) **Verwende Kompaktheit**, um eine konvergente Teilfolge auszuwählen.
3) **Zeige**, dass die Grenzfunktion eine Lösung ist.

---

# 3 Lineare Systeme

Betrachte:**(3.1)**  
$y'(t) = A(t) y(t) + l(t)$  mit $A \in C(J, \mathbb{R}^{d \times d})$, $l \in C(J, \mathbb{R}^d)$

---
## 3.1 Homogene Systeme

Falls $l \equiv 0$, d.h. wir betrachten:**(3.2)**  $y' = A y$

---

### Lemma 3.1 (Superpositionsprinzip)

**Voraussetzung:** Betrachte die Menge aller $C^1(J, \mathbb{R}^d)$-Funktionen, die (3.2) lösen.  
**Behauptung:** Diese Menge bildet einen Vektorraum.

#### Beweis von [[#Lemma 3.1 (Superpositionsprinzip)]]
Übung (Übungsbeispiel 1.4.) ■

---
### Satz 3.2

**Voraussetzung:** Sei $\mathcal{L}$ der Vektorraum der Lösungen von (3.2).  
**Behauptung:** Für (beliebiges, festes) $t_0 \in J$ ist die Abbildung:

$\varphi: \mathbb{R}^d \to \mathcal{L}$

$$\underbrace{ y_0 \mapsto y_{t_0, y_0}(\cdot)}_{y' = f(t, y)= Ay \quad; y'(t_0) \longmapsto y_0}$$

ein Vektorraum-Isomorphismus. Insbesondere ist die Dimension von $\mathcal{L}$ gerade $d$.

#### Beweis von [[#Satz 3.2]]
Wir nutzen die Eindeutigkeitsaussage für AWPs um zu zeigen , dass die Abbildung  
$\varphi: y_0 \mapsto y_{t_0, y_0}(\cdot)$  
folgende Eigenschaften erfüllt:
1) Linearität  
2) Injektivität  
3) Surjektivität 

- ##### Ad (1) Linearität
	- Seien $y_1, y_2 \in \mathbb{R}^d$ und $\lambda \in \mathbb{R}$, dann gilt:$\varphi(y_1 + \lambda y_2) = y_{t_0, y_1 + \lambda y_2}$
	- Weiter:$\varphi(y_1) + \lambda \varphi(y_2) = y_{t_0, y_1} + \lambda y_{t_0, y_2}$
	- Da beide Seiten die gleiche Lösung von  $y' = A y$ darstellen und die Lösung eindeutig ist, folgt: $y_1 + \lambda y_2 = y_1 + \lambda y_2$
- ##### Aus dem Buch:
	(i) Seien $y_1, y_2 \in \mathbb{R}^d$, $\lambda \in \mathbb{R}$. Dann gilt:
	Die Funktionen: 
	- $z_1 := \varphi(y_1 + \lambda y_2) = y_{t_0, y_1 + \lambda y_2}$
	- $z_2 := y_{t_0, y_1} + \lambda y_{t_0, y_2} = \varphi(y_1) + \lambda \varphi(y_2)$  
	lösen beide (3.2) mit derselben Anfangsbedingung bei $t = t_0$, nämlich $t_0 + \lambda y_2$.  
	Damit muss $z_1 = z_2$ für alle $t$ gelten.  
	Das heißt: $\varphi(y_1 + \lambda y_2) = \varphi(y_1) + \lambda \varphi(y_2)$

- ##### Ad (2) Injektivität
	- Angenommen, $\varphi(y_0) = 0$, dann folgt:$y_{t_0, y_0}(\cdot) = 0$ 
	- Damit gilt: $0 = y_{t_0, y_0}(t_0) = y_0$
	- Daraus folgt $y_0 = 0$, also ist $\varphi$ injektiv.
- ##### Aus dem Buch:
	Sei $0 = \varphi(y_0)$. Also ist $0 = y_{t_0, y_0}$.  Damit folgt $0 = y_{t_0, y_0}(t_0) = y_0$.  Also ist $\varphi$ *injektiv*.
	
- ##### Ad (3) Surjektivität
	- Sei $z \in \mathcal{L}$ eine Lösung von $z' = A z$.  
	- Setze $y_0 := z(t_0)$.
	- Dann folgt: $\varphi(y_0) = y_{t_0, y_0}(\cdot)$
	- Da die Lösung eindeutig ist, gilt: $y_{t_0, y_0}(\cdot) = z(\cdot)$
	- Also ist $\varphi$ surjektiv.
	- Da $\varphi$ sowohl injektiv als auch surjektiv ist, ist es ein Isomorphismus.
- ##### Aus dem Buch:
	Sei $z \in \mathcal{L}$ eine Lösung von (3.2).  
	Sei $x := z(t_0)$. Dann gilt:$z = x = \varphi(y_0)$
	lösen dasselbe Anfangswertproblem (AWP).  
	Das heißt: $\varphi$ ist *surjektiv*.

Die Funktionen $y^1, \dots, y^d \in \mathcal{L}$ bilden eine Basis, falls sie linear unabhängig sind, d.h.:

$$\sum_{i=1}^{d} a_i y^i = 0 \quad \Rightarrow \quad a_1 = \dots = a_d = 0$$
---

### Lemma 3.3

**Voraussetzung:**  
Seien $y^1, \dots, y^d \in \mathcal{L}$ eine Basis von $\mathcal{L}$.
**Behauptung:**  
Dann gilt: Für jedes $t \in J$ sind die Vektoren $\{y^1(t), \dots, y^d(t)\}$ eine Basis des $\mathbb{R}^d$.

#### Beweis von [[#Lemma 3.3]]

Sei $\bar{t} \in J$ und $a_1, \dots, a_d \in \mathbb{R}$ mit  $\sum_{i=1}^{d} a_i y^i(\bar{t}) = 0$
Dann lösen$y \equiv 0 \quad \text{und} \quad y(t) = \sum_{i=1}^{d} a_i y^i(t)$
das gleiche System $y' = A y$ mit $t_0 = \bar{t}$. Damit folgt
$y \equiv 0 \quad \Rightarrow \quad a_1 = \dots = a_d = 0$
Also sind $\{y^1, \dots, y^d\}$ linear unabhängig.

---
## Definition 3.4

Seien $y^1, \dots, y^d \in \mathcal{L}$ Lösungen von (3.2).  
Wir fassen diese Funktionen in der **Lösungsmatrix**  
$$Y(t) := (y^1(t), \dots, y^d(t))$$zusammen. Falls $y^1, \dots, y^d$ eine Basis bilden, dann heißt $Y$ eine **Fundamentalmatrix**.  
Falls eine Fundamentalmatrix $Y$ die Bedingung  
$$Y(\bar{t}) = I \in \mathbb{R}^{d \times d}$$für ein $\bar{t} \in J$ erfüllt, dann heißt $Y$ eine **Hauptfundamentalmatrix** bezüglich $\bar{t}$.  
Entsprechend heißen die Funktionen $\{y^1, \dots, y^d\}$ ein **Fundamentalsystem** bzw. **Hauptfundamentalsystem**( bezüglich $\bar{t}$).

---

#### Bemerkung 3.5

Sei $Y \in C^1(J; \mathbb{R}^{d \times d})$ eine Fundamentalmatrix. Dann gilt:
1) Nach Lemma 3.3 ist $Y(t)$ für jedes $t \in J$ invertierbar.
2) Für jedes $c \in \mathbb{R}^d$ ist $Y(t) \cdot c$ eine Lösung von (3.2).
3) Jede Lösung $y \in C^1(J, \mathbb{R}^d)$ von (3.2) hat die Form $y(t) = Y(t) \cdot c$  
für ein $c \in \mathbb{R}^d$.  Insbesondere ist die Lösung des AWP  $y' = A(t)y, \quad y(t_0) = y_0$  
gegeben durch: $y_{t_0, y_0}(t) = Y(t) (Y(t_0))^{-1} y_0.$

### 3.1.1 Wronskideterminante

## Definition 3.6

Sei $Y \in C^1(J; \mathbb{R}^{d \times d})$ eine Lösungsmatrix.  
Dann heißt die Funktion $W \in C^1(J; \mathbb{R})$ gegeben durch:
$$W(t) := \det(Y(t))$$  die **Wronskideterminante** von $Y$.

---

### Satz 3.7 (Liouville)

**Voraussetzung:** keine  
**Behauptung:** Die Wronskideterminante erfüllt die ODE  
$$W'(t) = \operatorname{tr} A(t) \cdot W(t) \quad \forall t \in J$$  
wobei die Spur einer Matrix definiert ist als $\operatorname{tr} A(t) = \sum_{i=1}^{d} A_{ii}(t)$.  
Insbesondere ist damit für jedes feste $t_0 \in J$:
$$W(t) = W(t_0) \cdot e^{\int_{t_0}^{t} \operatorname{tr} A(s) ds}$$

---
### Lemma 3.8

**Voraussetzung:** Seien $B, C \in \mathbb{R}^{d \times d}$.
**Behauptung:** Dann gilt

$$\lim_{\varepsilon \to 0} \frac{\det(B + \varepsilon \cdot C \cdot B) - \det B}{\varepsilon} = \operatorname{tr} C \cdot \det B$$

---

## Beweis von [[#Lemma 3.8]]
$$\det(B + \varepsilon C B) - \det B = \det((I + \varepsilon C) B) - \det B$ = (\det(I + \varepsilon C) - 1) \det B$$

**Behauptung** : $\det(I + \varepsilon C) = 1 + \varepsilon \operatorname{tr} C + O(\varepsilon^2), \quad \varepsilon \to 0$
**Beweis**:  $$\lim_{\varepsilon \to 0} \frac{\det(B + \varepsilon C B) - \det B}{\varepsilon} = \operatorname{tr} C \cdot \det B$$

Der Beweis erfolgt durch **Induktion nach $d$**.

## Matrix $C$ und Determinantenentwicklung

Sei die Matrix $C$ gegeben durch:

$$C =
\begin{pmatrix}
c_{11} & c_{12} & \dots & c_{1d} \\
c_{21} & \ddots & & \vdots \\
\vdots & & C' & \\
c_{d1} & & & c_{dd}
\end{pmatrix}, \quad C' \in \mathbb{R}^{(d-1) \times (d-1)}
$$

Dann gilt:

$$\det(I + \varepsilon C) = \det \begin{pmatrix}
1 + \varepsilon c_{11} & \varepsilon c_{12} & \varepsilon c_{13} & \dots & \varepsilon c_{1d} \\
\varepsilon c_{21} & & & & \\
\vdots & & I + \varepsilon C' & & \\
\varepsilon c_{d1} & & & & 
\end{pmatrix}$$

Durch Entwicklung der Determinante erhält man:

$$\det(I + \varepsilon C) = (1 + \varepsilon c_{11}) \det(I + \varepsilon C')$$
$$ - \varepsilon c_{21} \cdot O(\varepsilon) + \varepsilon c_{31} \cdot O(\varepsilon) + \dots$$$$= (1 + \varepsilon c_{11}) \cdot \det(I + \varepsilon C') + O(\varepsilon^2)$$

Durch **Induktion** folgt: 
$(1 + \varepsilon c_{11}) \cdot (1 + \varepsilon \operatorname{tr} C' + O(\varepsilon^2)) + O(\varepsilon^2) = 1 + \varepsilon \operatorname{tr} C + O(\varepsilon^2)$

Damit folgt:

$$\lim_{\varepsilon \to 0} \frac{\det(I + \varepsilon C) - 1}{\varepsilon} = \operatorname{tr} C$$
---
#### Beweis von [[#Satz 3.7 (Liouville)]]

##### 1. Fall: $W(t) \equiv 0$ für ein $\bar{t} \in J$

Dann sind die Spalten von $Y(\bar{t})$ linear abhängig.  
Das bedeutet, es existiert ein $a \in \mathbb{R}^d$ mit $Y(\bar{t}) a = 0$.

Da die Lösung eindeutig ist und $Y(t)$ dieselbe Spaltenstruktur besitzt, folgt:

$$Y(t) a = 0 \quad \forall t \in J$$

Das bedeutet, die Spalten von $Y(t)$ sind linear abhängig für alle $t \in J$, also gilt:

$$W(t) = \det Y(t) \equiv 0$$

---

##### 2. Fall: $W(t) \neq 0$ für alle $t \in J$

Dann existiert die Inverse von $Y(t)$ für alle $t \in J$.  
Für eine kleine Änderung $h$ gilt:

$$W(t + h) - W(t) = \det Y(t + h) - \det Y(t)$$
Durch die Entwicklung:

$\det(Y(t) + h Y'(t) + O(h^2)) - \det Y(t)$

Folgt:$\det(Y(t) + h Y'(t)) + O(h^2) - \det Y(t)$
Da die Determinante eine stetige Funktion ist, folgt, dass $W(t)$ stetig ist.
Wir setzen die vorherige Argumentation fort: $\det(Y(t + h) + h Y'(t)) - \det Y(t) + O(h^2)$
$= \det(Y(t) + h Y'(t) Y(t)^{-1} Y(t)) - \det Y(t) + O(h^2)$
Da $B = Y(t)$ und $C = Y'(t) Y(t)^{-1}= \det(B + h C B) - \det B + O(h^2)$
Wir betrachten:
$$W'(t) = \lim_{h \to 0} \frac{W(t + h) - W(t)}{h}$$
Unter Anwendung der Determinantenregel folgt:

$\operatorname{tr}(Y(t) Y'(t) Y(t)^{-1}) \det Y(t)$Da $Y'(t) = A Y(t)$ gilt:
$= \operatorname{tr}(A Y Y^{-1}) W$$= \operatorname{tr}(A) W$

Damit ist die Behauptung gezeigt. ■

![[Beweis_von_satz_3_7.png| 650]]